

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=&#34;auto&#34;>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/blog/img/favicon.png">
  <link rel="icon" type="image/png" href="/blog/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="rlin">
  <meta name="keywords" content="">
  <title>Kafka - rlin的运维笔记</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/blog/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.4.0/styles/github-gist.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/blog/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"rintime.gitee.io","root":"/blog/","version":"1.8.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"onlypost":false},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null}}};
  </script>
  <script  src="/blog/js/utils.js" ></script>
  <script  src="/blog/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/blog/">&nbsp;<strong>rlin的运维笔记</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/blog/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;</a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" href="javascript:">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/blog/img/default.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Kafka">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2023-02-21 09:21" pubdate>
        2023年2月21日 上午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      30k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      455
       分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Kafka</h1>
            
            <div class="markdown-body">
              <h1 id="一、kafka概述"><a href="#一、kafka概述" class="headerlink" title="一、kafka概述"></a>一、kafka概述</h1><h2 id="1-1-kafka定义"><a href="#1-1-kafka定义" class="headerlink" title="1.1 kafka定义"></a>1.1 kafka定义</h2><p>kafka传统定义：kafka是一个<font color=red>分布式</font>的基于<font color=red>发布/订阅模式</font>的<font color=red>消息队列</font>（Message Queue），主要应用于大数据实时处理领域。</p>
<p>发布/订阅：消息的发布者不会将详细直接发送给特定的订阅者，而是<font color=red>将发布的消息分为不同类别</font>，订阅者<font color=red>只接受感兴趣的消息</font>。</p>
<p>kafka最新定义：kafka是一个开源的<font color=red>分布式事件流平台</font>（Event Streaming Platform），被数千家公司用于高性能<font color=red>数据管道</font>、<font color=red>流分析</font>、<font color=red>数据集成</font>和<font color=red>关键任务应用</font>。</p>
<h2 id="1-2-消息队列"><a href="#1-2-消息队列" class="headerlink" title="1.2 消息队列"></a>1.2 消息队列</h2><p>目前企业中比较常见的消息队列产品主要有 Kafka、ActiveMQ、RabbitMQ、RocketMQ等。</p>
<p>在大数据场景主要采用 Kafka 作为消息队列。在 JavaEE 开发中主要采用 ActiveMQ、RabbitMQ、RocketMQ。</p>
<h3 id="1-2-1-传统消息队列的应用场景"><a href="#1-2-1-传统消息队列的应用场景" class="headerlink" title="1.2.1 传统消息队列的应用场景"></a>1.2.1 传统消息队列的应用场景</h3><p>传统的消息队列的主要应用场景包括：消峰/缓存、解耦、异步通信</p>
<p><strong>消息队列的应用场景——缓冲/消峰</strong></p>
<p>缓冲/消峰：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。</p>
<p><strong>消息队列的应用场景——解耦</strong></p>
<p>解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。</p>
<p><strong>消息队列的应用场景——异步通信</strong></p>
<p>异步通信：允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们。</p>
<h3 id="1-2-2-消息队列的两种模式"><a href="#1-2-2-消息队列的两种模式" class="headerlink" title="1.2.2 消息队列的两种模式"></a>1.2.2 消息队列的两种模式</h3><p>消息队列模式：</p>
<ol>
<li>点对点模式</li>
</ol>
<ul>
<li>消费者主动拉取数据，消息接收后清除消息</li>
</ul>
<ol start="2">
<li>发布/订阅模式</li>
</ol>
<ul>
<li>可以有多个topic主题（浏览、点赞、收藏、评论等）</li>
<li>消费者消费数据之后，不删除数据</li>
<li>每个消费者相互独立，都可以消费到数据</li>
</ul>
<h2 id="1-3-kafka基础结构"><a href="#1-3-kafka基础结构" class="headerlink" title="1.3 kafka基础结构"></a>1.3 kafka基础结构</h2><h3 id="1-3-1-kafka主要的角色"><a href="#1-3-1-kafka主要的角色" class="headerlink" title="1.3.1 kafka主要的角色"></a>1.3.1 kafka主要的角色</h3><p>（1）Producer ：消息生产者，就是向 Kafka broker发消息的客户端。</p>
<p>（2）Consumer ：消息消费者，向 Kafka broker 取消息的客户端。</p>
<p>（3）Consumer Group （CG ）：消费者组，由多个 consumer组成。 消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个 组内 消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即 消费者组是逻辑上的一个订阅者。</p>
<p>（4）Broker ：一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个broker可以容纳多个 topic。</p>
<p>（5）Topic： ：可以理解为一个队列，个 生产者和消费者面向的都是一个 topic。</p>
<p>（6）Partition ：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。</p>
<p>（7）Replica ：副本。一个 topic 的每个分区都有若干个副本，一个 Leader 和若干个Follower。</p>
<p>（8）Leader ：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 Leader。</p>
<p>（9）Follower ：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和Leader 数据的同步。Leader 发生故障时，某个 Follower 会成为新的 Leader。</p>
<h3 id="1-3-2-解释"><a href="#1-3-2-解释" class="headerlink" title="1.3.2 解释"></a>1.3.2 解释</h3><ol>
<li>为方便扩展，并提高吞吐量，一个topic分为多个partition</li>
<li>配合分区的设计，剔除出消费者组的概念，组内每个消费者并行消费</li>
<li>为提高可用性，为每个partition增加若干副本，类似NameNode HA（但是副本有leader和follow之分）</li>
</ol>
<ul>
<li>消费者只会去leader消费数据，不去follow消费</li>
<li>但是leader挂掉后，follow会转正成为leader</li>
</ul>
<ol start="4">
<li>zookeeper存储kafka的什么数据？（kafka2.8.0以后可以配置不采用zookeeper）</li>
</ol>
<ul>
<li>kafka集群中存活节点的数据（kafka服务器节点运行状态）</li>
<li>kafka分区leader相关信息</li>
</ul>
<h1 id="二、kafka安装部署"><a href="#二、kafka安装部署" class="headerlink" title="二、kafka安装部署"></a>二、kafka安装部署</h1><h2 id="2-1-kafka集群规划"><a href="#2-1-kafka集群规划" class="headerlink" title="2.1 kafka集群规划"></a>2.1 kafka集群规划</h2><p>本次实验的kafka的集群按照下面规划的表格进行安装部署：</p>
<table>
<thead>
<tr>
<th>服务器地址</th>
<th>服务器系统</th>
<th>安装的软件</th>
<th>hostname</th>
</tr>
</thead>
<tbody><tr>
<td>10.0.0.7</td>
<td>CentOS7.9</td>
<td>zookeeper、kafka</td>
<td>kafka-node1</td>
</tr>
<tr>
<td>10.0.0.17</td>
<td>CentOS7.9</td>
<td>zookeeper、kafka</td>
<td>kafka-node2</td>
</tr>
<tr>
<td>10.0.0.27</td>
<td>CentOS7.9</td>
<td>zookeeper、kafka</td>
<td>kafka-node3</td>
</tr>
</tbody></table>
<h2 id="2-2-安装前准备"><a href="#2-2-安装前准备" class="headerlink" title="2.2 安装前准备"></a>2.2 安装前准备</h2><h3 id="2-2-1-查看ip地址"><a href="#2-2-1-查看ip地址" class="headerlink" title="2.2.1 查看ip地址"></a>2.2.1 查看ip地址</h3><p>使用以下命令查看ip地址</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ifconfig eth0 | grep inet | awk &#x27;NR==1&#123;print $2&#125;&#x27;<br></code></pre></td></tr></table></figure>

<h3 id="2-2-2查看系统版本"><a href="#2-2-2查看系统版本" class="headerlink" title="2.2.2查看系统版本"></a>2.2.2查看系统版本</h3><p>执行以下命令，查看系统版本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cat /etc/redhat-release <br>CentOS Linux release 7.9.2009 (Core)<br></code></pre></td></tr></table></figure>

<h3 id="2-2-3-修改hostname"><a href="#2-2-3-修改hostname" class="headerlink" title="2.2.3 修改hostname"></a>2.2.3 修改hostname</h3><p>根据规划，修改对应服务器的hostname，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hostnamectl set-hostname kafka-node1<br></code></pre></td></tr></table></figure>

<p>执行命令后退出终端，然后重新登录就可以看到hostname已经被修改了</p>
<h3 id="2-2-4-查看hostname"><a href="#2-2-4-查看hostname" class="headerlink" title="2.2.4 查看hostname"></a>2.2.4 查看hostname</h3><p>执行以下命令查看hostname：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> hostname</span><br>kafka-node1<br></code></pre></td></tr></table></figure>

<h3 id="2-2-5-修改hosts文件"><a href="#2-2-5-修改hosts文件" class="headerlink" title="2.2.5 修改hosts文件"></a>2.2.5 修改hosts文件</h3><p>每台服务都修改osts文件，hosts添加的内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /etc/hosts<br>......<br>10.0.0.7 kafka-node1<br>10.0.0.17 kafka-node2<br>10.0.0.27 kafka-node3<br></code></pre></td></tr></table></figure>

<h3 id="2-2-6-kafka-node1节点生成公钥私钥"><a href="#2-2-6-kafka-node1节点生成公钥私钥" class="headerlink" title="2.2.6 kafka-node1节点生成公钥私钥"></a>2.2.6 kafka-node1节点生成公钥私钥</h3><p>kafka-node1生成公钥私钥。执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh-keygen #输入命令后一直按回车<br>Generating public/private rsa key pair.<br>Enter file in which to save the key (/root/.ssh/id_rsa): <br>Enter passphrase (empty for no passphrase): <br>Enter same passphrase again: <br>Your identification has been saved in /root/.ssh/id_rsa<br>Your public key has been saved in /root/.ssh/id_rsa.pub<br>The key fingerprint is:<br>SHA256:UKulf5OGyaTvFEwqRhEYHgy74WDiJBeteMLpVxdpsv0 root@kafka-node1<br>The key&#x27;s randomart image is:<br>+---[RSA 3072]----+<br>|.o++o.  o        |<br>| ooo.o = .       |<br>|B++.. * =        |<br>|@*o. o @         |<br>|o=  + = S        |<br>| . o . = E .     |<br>|  .   . * =      |<br>|       o o .     |<br>|       .o        |<br>+----[SHA256]-----+<br></code></pre></td></tr></table></figure>

<h3 id="2-2-7-kafka-node1节点将公钥发送给其他服务器"><a href="#2-2-7-kafka-node1节点将公钥发送给其他服务器" class="headerlink" title="2.2.7 kafka-node1节点将公钥发送给其他服务器"></a>2.2.7 kafka-node1节点将公钥发送给其他服务器</h3><p>kafka-node1将公钥发给kafka-node2和kafka-node3，方便再kafka-node1节点控制其他节点。</p>
<p>发送公钥给kafka-node2的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh-copy-id 10.0.0.17<br>/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;<br>The authenticity of host &#x27;10.0.0.17 (10.0.0.17)&#x27; can&#x27;t be established.<br>ECDSA key fingerprint is SHA256:OtC+Vn9Hy9pn4GTmY3iZM1p8YHlskV72H7+CnSBgPpY.<br>ECDSA key fingerprint is MD5:b2:d1:ec:a4:20:c8:db:4d:bd:6c:4c:f0:2b:1a:4f:29.<br>Are you sure you want to continue connecting (yes/no)? yes<br>/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed<br>/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys<br>root@10.0.0.17&#x27;s password: <br><br>Number of key(s) added: 1<br><br>Now try logging into the machine, with:   &quot;ssh &#x27;10.0.0.17&#x27;&quot;<br>and check to make sure that only the key(s) you wanted were added.<br></code></pre></td></tr></table></figure>

<p>发送公钥给kafka-node3的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh-copy-id 10.0.0.27<br>/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;<br>The authenticity of host &#x27;10.0.0.27 (10.0.0.27)&#x27; can&#x27;t be established.<br>ECDSA key fingerprint is SHA256:OtC+Vn9Hy9pn4GTmY3iZM1p8YHlskV72H7+CnSBgPpY.<br>ECDSA key fingerprint is MD5:b2:d1:ec:a4:20:c8:db:4d:bd:6c:4c:f0:2b:1a:4f:29.<br>Are you sure you want to continue connecting (yes/no)? yes<br>/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed<br>/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys<br>root@10.0.0.27&#x27;s password: <br><br>Number of key(s) added: 1<br><br>Now try logging into the machine, with:   &quot;ssh &#x27;10.0.0.27&#x27;&quot;<br>and check to make sure that only the key(s) you wanted were added.<br></code></pre></td></tr></table></figure>

<h3 id="2-2-8-安装zookeeper"><a href="#2-2-8-安装zookeeper" class="headerlink" title="2.2.8 安装zookeeper"></a>2.2.8 安装zookeeper</h3><p>按照规划，我们需要先把zookeeper安装。因为本次实验主要是演示kafa的安装部署，所以zookeeper的部署这里就省略了。</p>
<h2 id="2-3-集群部署kafka"><a href="#2-3-集群部署kafka" class="headerlink" title="2.3 集群部署kafka"></a>2.3 集群部署kafka</h2><h3 id="2-3-1-下载安装包"><a href="#2-3-1-下载安装包" class="headerlink" title="2.3.1 下载安装包"></a>2.3.1 下载安装包</h3><p>创建kafka安装目录，并下载kafka安装包。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">创建安装目录</span><br>[root@kafka-node1 ~]# mkdir /apps<br><span class="hljs-meta">#</span><span class="bash">进入安装目录</span><br>[root@kafka-node1 ~]# cd /apps<br><span class="hljs-meta">#</span><span class="bash">下载kafka安装包</span><br>[root@kafka-node1 apps]# wget https://archive.apache.org/dist/kafka/3.0.0/kafka_2.12-3.0.0.tgz<br></code></pre></td></tr></table></figure>

<h3 id="2-3-2-解压kafka安装包并创建软连接"><a href="#2-3-2-解压kafka安装包并创建软连接" class="headerlink" title="2.3.2 解压kafka安装包并创建软连接"></a>2.3.2 解压kafka安装包并创建软连接</h3><p>解压kafka安装包，并创建软连接。在这里创建软连接是方便以后对kafka的升级换代。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">解压kafka安装包</span><br>[root@kafka-node1 apps]# tar zxvf kafka_2.12-3.0.0.tgz<br><span class="hljs-meta">#</span><span class="bash">对kafka创建软连接</span><br>[root@kafka-node1 apps]# ln -s /apps/kafka_2.12-3.0.0 /apps/kafka<br></code></pre></td></tr></table></figure>

<p>解释：</p>
<p><code>z</code>：</p>
<p><code>x</code>：</p>
<p><code>v</code>：</p>
<p><code>f</code>：</p>
<p><code>-s</code>：</p>
<h3 id="2-3-3-修改kafka配置文件"><a href="#2-3-3-修改kafka配置文件" class="headerlink" title="2.3.3 修改kafka配置文件"></a>2.3.3 修改kafka配置文件</h3><p>修改kafka-node1节点的配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 apps]# cd /apps/kafka/config/<br>[root@kafka-node1 config]# vim server.properties<br><span class="hljs-meta">#</span><span class="bash">broker 的全局唯一编号，不能重复，只能是数字。</span><br>broker.id=1 #修改编号<br><span class="hljs-meta">#</span><span class="bash">处理网络请求的线程数量</span><br>num.network.threads=3<br><span class="hljs-meta">#</span><span class="bash">用来处理磁盘 IO 的线程数量</span><br>num.io.threads=8<br><span class="hljs-meta">#</span><span class="bash">发送套接字的缓冲区大小</span><br>socket.send.buffer.bytes=102400<br><span class="hljs-meta">#</span><span class="bash">接收套接字的缓冲区大小</span><br>socket.receive.buffer.bytes=102400<br><span class="hljs-meta">#</span><span class="bash">请求套接字的缓冲区大小</span><br>socket.request.max.bytes=104857600<br><span class="hljs-meta">#</span><span class="bash">kafka 运行日志(数据)存放的路径，路径不需要提前创建，kafka 自动帮你创建，可以配置多个磁盘路径，路径与路径之间可以用<span class="hljs-string">&quot;，&quot;</span>分隔</span><br>log.dirs=/apps/kafka/data #修改文件路径<br><span class="hljs-meta">#</span><span class="bash">topic 在当前 broker 上的分区个数</span><br>num.partitions=1<br><span class="hljs-meta">#</span><span class="bash">用来恢复和清理 data 下数据的线程数量</span><br>num.recovery.threads.per.data.dir=1<br><span class="hljs-meta">#</span><span class="bash"> 每个 topic 创建时的副本数，默认时 1 个副本</span><br>offsets.topic.replication.factor=1<br>transaction.state.log.replication.factor=1<br>transaction.state.log.min.isr=1<br><span class="hljs-meta">#</span><span class="bash">segment 文件保留的最长时间，超时将被删除</span><br>log.retention.hours=168<br><span class="hljs-meta">#</span><span class="bash">每个 segment 文件的大小，默认最大 1G</span><br>log.segment.bytes=1073741824<br><span class="hljs-meta">#</span><span class="bash"> 检查过期数据的时间，默认 5 分钟检查一次是否数据过期</span><br>log.retention.check.interval.ms=300000<br><span class="hljs-meta">#</span><span class="bash">配置连接 Zookeeper 集群地址（在 zk 根目录下创建/kafka，方便管理）</span><br>zookeeper.connect=kafka-node1:2181,kafka-node2:2181,kafka-node3:2181/kafka #修改zookeeper集群地址<br>zookeeper.connection.timeout.ms=18000<br>group.initial.rebalance.delay.ms=0<br></code></pre></td></tr></table></figure>

<h3 id="2-3-4-将安装包分发给其他节点"><a href="#2-3-4-将安装包分发给其他节点" class="headerlink" title="2.3.4 将安装包分发给其他节点"></a>2.3.4 将安装包分发给其他节点</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">10.0.0.102</span><br>[root@kafka-node1 config]# rsync -av /apps/* kafka-node2:/apps<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash">10.0.0.103</span><br>[root@kafka-node1 config]# rsync -av /apps/* kafka-node3:/apps<br></code></pre></td></tr></table></figure>

<h3 id="2-3-5-其他节点修改配置文件中的“broker-id”"><a href="#2-3-5-其他节点修改配置文件中的“broker-id”" class="headerlink" title="2.3.5 其他节点修改配置文件中的“broker.id”"></a>2.3.5 其他节点修改配置文件中的“broker.id”</h3><p>注：broker.id 不得重复，整个集群中唯一。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">10.0.0.102</span><br>[root@kafka-node2 ~]# cd /apps/kafka/config<br>[root@kafka-node2 config]# vim server.properties<br>......<br>broker.id=2<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash">10.0.0.103</span><br>[root@kafka-node3 ~]# cd /apps/kafka/config<br>[root@kafka-node3 config]# vim server.properties<br>......<br>broker.id=3<br></code></pre></td></tr></table></figure>

<h3 id="2-3-6-设置环境变量"><a href="#2-3-6-设置环境变量" class="headerlink" title="2.3.6 设置环境变量"></a>2.3.6 设置环境变量</h3><p>kafka-node1节点添加环境变量文件，内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# vim /etc/profile.d/kafka.sh<br>export KAFKA_HOME=/apps/kafka<br>export PATH=$PATH:$KAFKA_HOME/bin<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash">刷新一下环境变量</span><br>[root@kafka-node1 ~]# source /etc/profele.d/kafka.sh<br></code></pre></td></tr></table></figure>

<h3 id="2-3-7-将环境变量文件发送其他节点"><a href="#2-3-7-将环境变量文件发送其他节点" class="headerlink" title="2.3.7 将环境变量文件发送其他节点"></a>2.3.7 将环境变量文件发送其他节点</h3><p>将kafka-node1新添加的换将变量文件发送到kafka-node2节点和kafka-node3节点，同时让kafka-node2节点和kafka-node3节点的kafka环境变量也生效，具体执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# scp /etc/profile.d/kafka.sh kafka-node2:/etc/profile.d/<br>[root@kafka-node1 ~]# scp /etc/profile.d/kafka.sh kafka-node3:/etc/profile.d/<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash">其他节点同样刷新一下环境变量</span><br>[root@kafka-node2 ~]# source /etc/profile.d/kafka.sh<br>[root@kafka-node3 ~]# source /etc/profile.d/kafka.sh<br></code></pre></td></tr></table></figure>

<h3 id="2-3-8-启动kafka"><a href="#2-3-8-启动kafka" class="headerlink" title="2.3.8 启动kafka"></a>2.3.8 启动kafka</h3><p>注意：这里需要先安装zookeeper集群，并且启动zookeeper集群。</p>
<p>每台服务器都要执行以下命令启动kafka：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">kafka-node1节点</span><br>[root@kafka-node1 ~]# /apps/kafka/bin/kafka-server-start.sh -daemon /apps/kafka/config/server.properties<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash">kafka-node2节点</span><br>[root@kafka-node2 ~]# /apps/kafka/bin/kafka-server-start.sh -daemon /apps/kafka/config/server.properties<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash">kafka-node3节点</span><br>[root@kafka-node3 ~]# /apps/kafka/bin/kafka-server-start.sh -daemon /apps/kafka/config/server.properties<br></code></pre></td></tr></table></figure>

<p>kafka的监听端口为：9092和34410</p>
<h3 id="2-3-9-关闭kafka"><a href="#2-3-9-关闭kafka" class="headerlink" title="2.3.9 关闭kafka"></a>2.3.9 关闭kafka</h3><p>关闭kafka时可以执行以下命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-server-stop.sh<br></code></pre></td></tr></table></figure>

<p>注意：kafka的关闭可能需要一点时间，给点耐心。</p>
<h3 id="2-3-10-查看kafka状态"><a href="#2-3-10-查看kafka状态" class="headerlink" title="2.3.10 查看kafka状态"></a>2.3.10 查看kafka状态</h3><p>查看kafka启动或关闭状态我们可以执行以下命令：（执行该命令的前提是是否安装了jdk的环境）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">jps<br></code></pre></td></tr></table></figure>

<p>例子1：kafka已经启动时执行<code>jps</code></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# jps<br>4708 Jps<br>4629 Kafka #kafka的PID，这个PID同样可以在data文件中找到。<br>3206 QuorumPeerMain #这个是zookeeper的PID。<br></code></pre></td></tr></table></figure>

<p>例子2：kafka已经关闭是启动<code>jps</code>（我们看不到kafka的pid了）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# jps<br>3206 QuorumPeerMain #这个是zookeeper的PID。<br>4284 Jps<br></code></pre></td></tr></table></figure>

<p>所以这个jps是查看所有java程序的一个好帮手</p>
<h3 id="2-3-11-kafka启动脚本"><a href="#2-3-11-kafka启动脚本" class="headerlink" title="2.3.11 kafka启动脚本"></a>2.3.11 kafka启动脚本</h3><p>为了方便之后对kafka的管理，我们提供了一个对kafka集群管理的脚本，脚本内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">!/bin/bash</span><br>node=(kafka-node1 kafka-node2 kafka-node3)<br>case $1 in<br>	&quot;start&quot;)<br>		for i in &quot;$&#123;node[@]&#125;&quot;<br>		do<br>			echo &quot;--- 启动 $i kafka ---&quot;<br>			ssh $i &quot;/apps/kafka/bin/kafka-server-start.sh -daemon /apps/kafka/config/server.properties&quot;<br>		done<br>		;;<br>	&quot;stop&quot;)<br>                for i in &quot;$&#123;node[@]&#125;&quot;<br>                do<br>                        echo &quot;--- 关闭 $i kafka ---&quot;<br>                        ssh $i &quot;/apps/kafka/bin/kafka-server-stop.sh&quot;<br>                done<br>		;;<br>esac<br></code></pre></td></tr></table></figure>

<p>给该脚本添加权限后就可以通过脚本控制集群的kafka的启动与关闭了</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">添加执行权限</span><br>chmod +x kafka_ctl.sh<br><span class="hljs-meta">#</span><span class="bash">关闭kafka</span><br>./kafka_ctl.sh stop<br><span class="hljs-meta">#</span><span class="bash">启动kafka</span><br>./kafka_Ctl.sh start<br><span class="hljs-meta">#</span><span class="bash">最后我们可以通过jps命令开查看kafka的启动与关闭的状态了。</span><br></code></pre></td></tr></table></figure>

<p>注意：停止 Kafka 集群时，一定要等 Kafka 所有节点进程全部停止后再停止 Zookeeper集群。因为 Zookeeper 集群当中记录着 Kafka 集群相关信息，Zookeeper 集群一旦先停止，Kafka 集群就没有办法再获取停止进程的信息，只能手动杀死 Kafka 进程了。</p>
<h1 id="三、kafka命令行操作"><a href="#三、kafka命令行操作" class="headerlink" title="三、kafka命令行操作"></a>三、kafka命令行操作</h1><h2 id="3-1-主题命令行操作"><a href="#3-1-主题命令行操作" class="headerlink" title="3.1 主题命令行操作"></a>3.1 主题命令行操作</h2><h3 id="3-1-1-查看操作topics命令行参数"><a href="#3-1-1-查看操作topics命令行参数" class="headerlink" title="3.1.1 查看操作topics命令行参数"></a>3.1.1 查看操作topics命令行参数</h3><p>执行以下命令，查看topics命令行参数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh<br>Create, delete, describe, or change a topic.<br>Option                                   Description                            <br>------                                   -----------                            <br>--alter                                  Alter the number of partitions,        <br>                                           replica assignment, and/or           <br>                                           configuration for the topic.         <br>--at-min-isr-partitions                  if set when describing topics, only    <br>                                           show partitions whose isr count is   <br>                                           equal to the configured minimum.     <br>--bootstrap-server &lt;String: server to    REQUIRED: The Kafka server to connect  <br>  connect to&gt;                              to.                                  <br>--command-config &lt;String: command        Property file containing configs to be <br>  config property file&gt;                    passed to Admin Client. This is used <br>                                           only with --bootstrap-server option  <br>                                           for describing and altering broker   <br>                                           configs.                             <br>--config &lt;String: name=value&gt;            A topic configuration override for the <br>                                           topic being created or altered. The  <br>                                           following is a list of valid         <br>                                           configurations:                      <br>                                         	cleanup.policy                        <br>                                         	compression.type                      <br>                                         	delete.retention.ms                   <br>                                         	file.delete.delay.ms                  <br>                                         	flush.messages                        <br>                                         	flush.ms                              <br>                                         	follower.replication.throttled.       <br>                                           replicas                             <br>                                         	index.interval.bytes                  <br>                                         	leader.replication.throttled.replicas <br>                                         	local.retention.bytes                 <br>                                         	local.retention.ms                    <br>                                         	max.compaction.lag.ms                 <br>                                         	max.message.bytes                     <br>                                         	message.downconversion.enable         <br>                                         	message.format.version                <br>                                         	message.timestamp.difference.max.ms   <br>                                         	message.timestamp.type                <br>                                         	min.cleanable.dirty.ratio             <br>                                         	min.compaction.lag.ms                 <br>                                         	min.insync.replicas                   <br>                                         	preallocate                           <br>                                         	remote.storage.enable                 <br>                                         	retention.bytes                       <br>                                         	retention.ms                          <br>                                         	segment.bytes                         <br>                                         	segment.index.bytes                   <br>                                         	segment.jitter.ms                     <br>                                         	segment.ms                            <br>                                         	unclean.leader.election.enable        <br>                                         See the Kafka documentation for full   <br>                                           details on the topic configs. It is  <br>                                           supported only in combination with --<br>                                           create if --bootstrap-server option  <br>                                           is used (the kafka-configs CLI       <br>                                           supports altering topic configs with <br>                                           a --bootstrap-server option).        <br>--create                                 Create a new topic.                    <br>--delete                                 Delete a topic                         <br>--delete-config &lt;String: name&gt;           A topic configuration override to be   <br>                                           removed for an existing topic (see   <br>                                           the list of configurations under the <br>                                           --config option). Not supported with <br>                                           the --bootstrap-server option.       <br>--describe                               List details for the given topics.     <br>--disable-rack-aware                     Disable rack aware replica assignment  <br>--exclude-internal                       exclude internal topics when running   <br>                                           list or describe command. The        <br>                                           internal topics will be listed by    <br>                                           default                              <br>--help                                   Print usage information.               <br>--if-exists                              if set when altering or deleting or    <br>                                           describing topics, the action will   <br>                                           only execute if the topic exists.    <br>--if-not-exists                          if set when creating topics, the       <br>                                           action will only execute if the      <br>                                           topic does not already exist.        <br>--list                                   List all available topics.             <br>--partitions &lt;Integer: # of partitions&gt;  The number of partitions for the topic <br>                                           being created or altered (WARNING:   <br>                                           If partitions are increased for a    <br>                                           topic that has a key, the partition  <br>                                           logic or ordering of the messages    <br>                                           will be affected). If not supplied   <br>                                           for create, defaults to the cluster  <br>                                           default.                             <br>--replica-assignment &lt;String:            A list of manual partition-to-broker   <br>  broker_id_for_part1_replica1 :           assignments for the topic being      <br>  broker_id_for_part1_replica2 ,           created or altered.                  <br>  broker_id_for_part2_replica1 :                                                <br>  broker_id_for_part2_replica2 , ...&gt;                                           <br>--replication-factor &lt;Integer:           The replication factor for each        <br>  replication factor&gt;                      partition in the topic being         <br>                                           created. If not supplied, defaults   <br>                                           to the cluster default.              <br>--topic &lt;String: topic&gt;                  The topic to create, alter, describe   <br>                                           or delete. It also accepts a regular <br>                                           expression, except for --create      <br>                                           option. Put topic name in double     <br>                                           quotes and use the &#x27;\&#x27; prefix to     <br>                                           escape regular expression symbols; e.<br>                                           g. &quot;test\.topic&quot;.                    <br>--topics-with-overrides                  if set when describing topics, only    <br>                                           show topics that have overridden     <br>                                           configs                              <br>--unavailable-partitions                 if set when describing topics, only    <br>                                           show partitions whose leader is not  <br>                                           available                            <br>--under-min-isr-partitions               if set when describing topics, only    <br>                                           show partitions whose isr count is   <br>                                           less than the configured minimum.    <br>--under-replicated-partitions            if set when describing topics, only    <br>                                           show under replicated partitions     <br>--version                                Display Kafka version.<br></code></pre></td></tr></table></figure>

<h3 id="3-1-2-查看当前服务器中的所有-topic"><a href="#3-1-2-查看当前服务器中的所有-topic" class="headerlink" title="3.1.2 查看当前服务器中的所有 topic"></a>3.1.2 查看当前服务器中的所有 topic</h3><p><font color=orange>工作环境使用的命令</font>：因为工作环境的kafka可能会挂掉一个，如果刚好输入的地址是挂掉的一个就看不到数据了，所以我们可以连接的server都写完，命令的内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 kafka-node2:9092 kafka-node3:9092 --list<br></code></pre></td></tr></table></figure>

<p><font color=orange>学习环境使用的命令</font>，命令的内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --list<br></code></pre></td></tr></table></figure>

<h3 id="3-1-3-创建名为first的topic"><a href="#3-1-3-创建名为first的topic" class="headerlink" title="3.1.3 创建名为first的topic"></a>3.1.3 创建名为first的topic</h3><p>创建格式：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server 节点地址:9092 --create --partitions 1 --replication-factor 3 --topic 主题名称<br></code></pre></td></tr></table></figure>

<p>举例：</p>
<p>创建 first topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --create --partitions 1 --replication-factor 3 --topic first<br>Created topic first.<br></code></pre></td></tr></table></figure>

<p>查看主题</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --list first<br>first<br></code></pre></td></tr></table></figure>

<h3 id="3-1-4-查看-first-主题的详情"><a href="#3-1-4-查看-first-主题的详情" class="headerlink" title="3.1.4 查看 first 主题的详情"></a>3.1.4 查看 first 主题的详情</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --describe --topic first<br>Topic: first	TopicId: Q57MrqL5Qda-f3XLayfesg	PartitionCount: 1	ReplicationFactor: 3	Configs: segment.bytes=1073741824<br>	Topic: first	Partition: 0	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1<br></code></pre></td></tr></table></figure>

<p>通过查看主题详情的“Configs: segment.bytes=1073741824”，我们知道kafka底层通过给每一部分的片段的存储为1G，所以不管kafka存储多大的容量，都是通过多个分段来组成的。</p>
<h3 id="3-1-5-first-topic增加分区"><a href="#3-1-5-first-topic增加分区" class="headerlink" title="3.1.5 first topic增加分区"></a>3.1.5 first topic增加分区</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --topic first --alter --partitions 3<br></code></pre></td></tr></table></figure>

<p>注意：主题分区只能增加不能减少、</p>
<p>再次查看first topic详情</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --describe --topic first<br>Topic: first	TopicId: -ly9Cp4hTxeXLY9i_OwEzA	PartitionCount: 3	ReplicationFactor: 3	Configs: segment.bytes=1073741824<br>	Topic: first	Partition: 0	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3<br>	Topic: first	Partition: 1	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1<br>	Topic: first	Partition: 2	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2<br></code></pre></td></tr></table></figure>

<h3 id="3-1-6-删除topic"><a href="#3-1-6-删除topic" class="headerlink" title="3.1.6 删除topic"></a>3.1.6 删除topic</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --topic first --delete<br></code></pre></td></tr></table></figure>

<h2 id="3-2-生产者命令操作"><a href="#3-2-生产者命令操作" class="headerlink" title="3.2 生产者命令操作"></a>3.2 生产者命令操作</h2><h3 id="3-2-1-查看操作生产者命令参数"><a href="#3-2-1-查看操作生产者命令参数" class="headerlink" title="3.2.1 查看操作生产者命令参数"></a>3.2.1 查看操作生产者命令参数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-console-producer.sh<br>Missing required option(s) [bootstrap-server]<br>Option                                   Description                            <br>------                                   -----------                            <br>--batch-size &lt;Integer: size&gt;             Number of messages to send in a single <br>                                           batch if they are not being sent     <br>                                           synchronously. (default: 200)        <br>--bootstrap-server &lt;String: server to    REQUIRED unless --broker-list          <br>  connect to&gt;                              (deprecated) is specified. The server<br>                                           (s) to connect to. The broker list   <br>                                           string in the form HOST1:PORT1,HOST2:<br>                                           PORT2.                               <br>--broker-list &lt;String: broker-list&gt;      DEPRECATED, use --bootstrap-server     <br>                                           instead; ignored if --bootstrap-     <br>                                           server is specified.  The broker     <br>                                           list string in the form HOST1:PORT1, <br>                                           HOST2:PORT2.                         <br>--compression-codec [String:             The compression codec: either &#x27;none&#x27;,  <br>  compression-codec]                       &#x27;gzip&#x27;, &#x27;snappy&#x27;, &#x27;lz4&#x27;, or &#x27;zstd&#x27;.  <br>                                           If specified without value, then it  <br>                                           defaults to &#x27;gzip&#x27;                   <br>--help                                   Print usage information.               <br>--line-reader &lt;String: reader_class&gt;     The class name of the class to use for <br>                                           reading lines from standard in. By   <br>                                           default each line is read as a       <br>                                           separate message. (default: kafka.   <br>                                           tools.                               <br>                                           ConsoleProducer$LineMessageReader)   <br>--max-block-ms &lt;Long: max block on       The max time that the producer will    <br><span class="hljs-meta">  send&gt;</span><span class="bash">                                    block <span class="hljs-keyword">for</span> during a send request</span>      <br>                                           (default: 60000)                     <br>--max-memory-bytes &lt;Long: total memory   The total memory used by the producer  <br>  in bytes&gt;                                to buffer records waiting to be sent <br>                                           to the server. (default: 33554432)   <br>--max-partition-memory-bytes &lt;Long:      The buffer size allocated for a        <br>  memory in bytes per partition&gt;           partition. When records are received <br>                                           which are smaller than this size the <br>                                           producer will attempt to             <br>                                           optimistically group them together   <br>                                           until this size is reached.          <br>                                           (default: 16384)                     <br>--message-send-max-retries &lt;Integer&gt;     Brokers can fail receiving the message <br>                                           for multiple reasons, and being      <br>                                           unavailable transiently is just one  <br>                                           of them. This property specifies the <br>                                           number of retries before the         <br>                                           producer give up and drop this       <br>                                           message. (default: 3)                <br>--metadata-expiry-ms &lt;Long: metadata     The period of time in milliseconds     <br>  expiration interval&gt;                     after which we force a refresh of    <br>                                           metadata even if we haven&#x27;t seen any <br>                                           leadership changes. (default: 300000)<br>--producer-property &lt;String:             A mechanism to pass user-defined       <br><span class="hljs-meta">  producer_prop&gt;</span><span class="bash">                           properties <span class="hljs-keyword">in</span> the form key=value to</span>  <br>                                           the producer.                        <br>--producer.config &lt;String: config file&gt;  Producer config properties file. Note  <br>                                           that [producer-property] takes       <br>                                           precedence over this config.         <br>--property &lt;String: prop&gt;                A mechanism to pass user-defined       <br>                                           properties in the form key=value to  <br>                                           the message reader. This allows      <br>                                           custom configuration for a user-     <br>                                           defined message reader. Default      <br>                                           properties include:                  <br>                                         	parse.key=true|false                  <br>                                         	key.separator=&lt;key.separator&gt;         <br>                                         	ignore.error=true|false               <br>--request-required-acks &lt;String:         The required acks of the producer      <br>  request required acks&gt;                   requests (default: 1)                <br>--request-timeout-ms &lt;Integer: request   The ack timeout of the producer        <br>  timeout ms&gt;                              requests. Value must be non-negative <br>                                           and non-zero (default: 1500)         <br>--retry-backoff-ms &lt;Integer&gt;             Before each retry, the producer        <br>                                           refreshes the metadata of relevant   <br>                                           topics. Since leader election takes  <br>                                           a bit of time, this property         <br>                                           specifies the amount of time that    <br>                                           the producer waits before refreshing <br>                                           the metadata. (default: 100)         <br>--socket-buffer-size &lt;Integer: size&gt;     The size of the tcp RECV size.         <br>                                           (default: 102400)                    <br>--sync                                   If set message send requests to the    <br>                                           brokers are synchronously, one at a  <br>                                           time as they arrive.                 <br>--timeout &lt;Integer: timeout_ms&gt;          If set and the producer is running in  <br>                                           asynchronous mode, this gives the    <br>                                           maximum amount of time a message     <br>                                           will queue awaiting sufficient batch <br>                                           size. The value is given in ms.      <br>                                           (default: 1000)                      <br>--topic &lt;String: topic&gt;                  REQUIRED: The topic id to produce      <br>                                           messages to.                         <br>--version                                Display Kafka version.<br></code></pre></td></tr></table></figure>

<h3 id="3-2-2-生产者发送消息"><a href="#3-2-2-生产者发送消息" class="headerlink" title="3.2.2 生产者发送消息"></a>3.2.2 生产者发送消息</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-console-producer.sh --bootstrap-server kafka-node1:9092 --topic first<br><span class="hljs-meta">&gt;</span><span class="bash">hellow</span><br><span class="hljs-meta">&gt;</span><span class="bash">hi</span><br><span class="hljs-meta">&gt;</span><span class="bash">how are you</span><br><span class="hljs-meta">&gt;</span><span class="bash">how old are you</span><br><span class="hljs-meta">&gt;</span><br></code></pre></td></tr></table></figure>

<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-1.png" srcset="/blog/img/loading.gif"></p>
<h2 id="3-3-消费者命令行操作"><a href="#3-3-消费者命令行操作" class="headerlink" title="3.3 消费者命令行操作"></a>3.3 消费者命令行操作</h2><h3 id="3-3-1-查看操作消费者命令参数"><a href="#3-3-1-查看操作消费者命令参数" class="headerlink" title="3.3.1 查看操作消费者命令参数"></a>3.3.1 查看操作消费者命令参数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node2 ~]# /apps/kafka/bin/kafka-console-consumer.sh<br>This tool helps to read data from Kafka topics and outputs it to standard output.<br>Option                                   Description                            <br>------                                   -----------                            <br>--bootstrap-server &lt;String: server to    REQUIRED: The server(s) to connect to. <br>  connect to&gt;                                                                   <br>--consumer-property &lt;String:             A mechanism to pass user-defined       <br><span class="hljs-meta">  consumer_prop&gt;</span><span class="bash">                           properties <span class="hljs-keyword">in</span> the form key=value to</span>  <br>                                           the consumer.                        <br>--consumer.config &lt;String: config file&gt;  Consumer config properties file. Note  <br>                                           that [consumer-property] takes       <br>                                           precedence over this config.         <br>--enable-systest-events                  Log lifecycle events of the consumer   <br>                                           in addition to logging consumed      <br>                                           messages. (This is specific for      <br>                                           system tests.)                       <br>--formatter &lt;String: class&gt;              The name of a class to use for         <br>                                           formatting kafka messages for        <br>                                           display. (default: kafka.tools.      <br>                                           DefaultMessageFormatter)             <br>--from-beginning                         If the consumer does not already have  <br>                                           an established offset to consume     <br>                                           from, start with the earliest        <br>                                           message present in the log rather    <br>                                           than the latest message.             <br>--group &lt;String: consumer group id&gt;      The consumer group id of the consumer. <br>--help                                   Print usage information.               <br>--include &lt;String: Java regex (String)&gt;  Regular expression specifying list of  <br>                                           topics to include for consumption.   <br>--isolation-level &lt;String&gt;               Set to read_committed in order to      <br>                                           filter out transactional messages    <br>                                           which are not committed. Set to      <br>                                           read_uncommitted to read all         <br>                                           messages. (default: read_uncommitted)<br>--key-deserializer &lt;String:                                                     <br>  deserializer for key&gt;                                                         <br>--max-messages &lt;Integer: num_messages&gt;   The maximum number of messages to      <br>                                           consume before exiting. If not set,  <br>                                           consumption is continual.            <br>--offset &lt;String: consume offset&gt;        The offset to consume from (a non-     <br>                                           negative number), or &#x27;earliest&#x27;      <br>                                           which means from beginning, or       <br>                                           &#x27;latest&#x27; which means from end        <br>                                           (default: latest)                    <br>--partition &lt;Integer: partition&gt;         The partition to consume from.         <br>                                           Consumption starts from the end of   <br>                                           the partition unless &#x27;--offset&#x27; is   <br>                                           specified.                           <br>--property &lt;String: prop&gt;                The properties to initialize the       <br>                                           message formatter. Default           <br>                                           properties include:                  <br>                                          print.timestamp=true|false            <br>                                          print.key=true|false                  <br>                                          print.offset=true|false               <br>                                          print.partition=true|false            <br>                                          print.headers=true|false              <br>                                          print.value=true|false                <br>                                          key.separator=&lt;key.separator&gt;         <br>                                          line.separator=&lt;line.separator&gt;       <br>                                          headers.separator=&lt;line.separator&gt;    <br>                                          null.literal=&lt;null.literal&gt;           <br>                                          key.deserializer=&lt;key.deserializer&gt;   <br>                                          value.deserializer=&lt;value.            <br>                                           deserializer&gt;                        <br>                                          header.deserializer=&lt;header.          <br>                                           deserializer&gt;                        <br>                                         Users can also pass in customized      <br>                                           properties for their formatter; more <br>                                           specifically, users can pass in      <br>                                           properties keyed with &#x27;key.          <br>                                           deserializer.&#x27;, &#x27;value.              <br>                                           deserializer.&#x27; and &#x27;headers.         <br>                                           deserializer.&#x27; prefixes to configure <br>                                           their deserializers.                 <br>--skip-message-on-error                  If there is an error when processing a <br>                                           message, skip it instead of halt.    <br>--timeout-ms &lt;Integer: timeout_ms&gt;       If specified, exit if no message is    <br>                                           available for consumption for the    <br>                                           specified interval.                  <br>--topic &lt;String: topic&gt;                  The topic to consume on.               <br>--value-deserializer &lt;String:                                                   <br>  deserializer for values&gt;                                                      <br>--version                                Display Kafka version.                 <br>--whitelist &lt;String: Java regex          DEPRECATED, use --include instead;     <br><span class="hljs-meta">  (String)&gt;</span><span class="bash">                                ignored <span class="hljs-keyword">if</span> --include specified.</span>      <br>                                           Regular expression specifying list   <br>                                           of topics to include for consumption.<br></code></pre></td></tr></table></figure>

<h3 id="3-3-2-消费消息"><a href="#3-3-2-消费消息" class="headerlink" title="3.3.2 消费消息"></a>3.3.2 消费消息</h3><p>消费者执行以下命令后就会开启开始消费消息，但是此命令消费的消息的时机是从消费者启动后，生产者发送的消息才会被消息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node2 ~]# /apps/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka-node1:9092 --topic first<br>lailailai<br>oooo222<br>233333<br></code></pre></td></tr></table></figure>

<p>生产者发送消息图：</p>
<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-2.png" srcset="/blog/img/loading.gif"></p>
<p>消费者消费消息图：</p>
<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-3.png" srcset="/blog/img/loading.gif"></p>
<p>我们可以从上面的两张截图中看到，消费者只能消费生产者后面发送的几条消息。</p>
<p>消费者如何才能消费历史消息呢？请继续观看。</p>
<h3 id="3-3-3-消费主题的所有历史数据"><a href="#3-3-3-消费主题的所有历史数据" class="headerlink" title="3.3.3 消费主题的所有历史数据"></a>3.3.3 消费主题的所有历史数据</h3><p>在原来消费消息的命令上添加<code>--from-beginning</code>这个选项就可以消费历史消息了 ，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> /apps/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka-node1:9092 --topic first --from-beginning</span><br>how old are you<br>oooo222<br>hellow<br>how are you<br>hi<br>lailailai<br>233333<br></code></pre></td></tr></table></figure>

<p>根据上一小节的截图来看，添加了<code>--from-beginning</code>选项的消费者可以消费历史消息了。</p>
<h1 id="四、kafka生产者"><a href="#四、kafka生产者" class="headerlink" title="四、kafka生产者"></a>四、kafka生产者</h1><h2 id="4-1-生产者消息发送"><a href="#4-1-生产者消息发送" class="headerlink" title="4.1 生产者消息发送"></a>4.1 生产者消息发送</h2><h3 id="4-1-1-发送原理"><a href="#4-1-1-发送原理" class="headerlink" title="4.1.1 发送原理"></a>4.1.1 发送原理</h3><p>在消息发送的过程中，涉及到了 <font color=red>两个线程 ——main 线程和 Sender 线程</font>。在 main 线程中创建了 一个 双端列 队列 RecordAccumulator。main线程将消息发送给 RecordAccumulator，Sender线程不断从 RecordAccumulator 中拉取消息发送到 Kafka Broker。</p>
<h3 id="4-1-2-生产者重要参数列表"><a href="#4-1-2-生产者重要参数列表" class="headerlink" title="4.1.2 生产者重要参数列表"></a>4.1.2 生产者重要参数列表</h3><h2 id="4-2-异步发送-API"><a href="#4-2-异步发送-API" class="headerlink" title="4.2 异步发送 API"></a>4.2 异步发送 API</h2><p>略</p>
<h2 id="4-3-同步发送-API"><a href="#4-3-同步发送-API" class="headerlink" title="4.3 同步发送 API"></a>4.3 同步发送 API</h2><p>略</p>
<h2 id="4-4-生产者分区"><a href="#4-4-生产者分区" class="headerlink" title="4.4 生产者分区"></a>4.4 生产者分区</h2><h3 id="4-4-1-分区好处"><a href="#4-4-1-分区好处" class="headerlink" title="4.4.1 分区好处"></a>4.4.1 分区好处</h3><p>（1） 便于合理使用存储资源，每个Partition在一个Broker上存储，可以把海量的数据按照分区切割成一块一块数据存储在多台Broker上。合理控制分区的任务，可以实现负载均衡的效果。</p>
<p>（2） 提高并行度，生产者可以以分区为单位发送数据；消费者可以以分区为单位进行消费数据。</p>
<h3 id="4-4-2-生产者发我送消息的分区策略"><a href="#4-4-2-生产者发我送消息的分区策略" class="headerlink" title="4.4.2 生产者发我送消息的分区策略"></a>4.4.2 生产者发我送消息的分区策略</h3><p>略</p>
<h3 id="4-4-3-自定义分区器"><a href="#4-4-3-自定义分区器" class="headerlink" title="4.4.3 自定义分区器"></a>4.4.3 自定义分区器</h3><p>如果研发人员可以根据企业需求，自己重新实现分区器。</p>
<p>略</p>
<h2 id="4-5-生产经验——生产者如何提高吞吐量"><a href="#4-5-生产经验——生产者如何提高吞吐量" class="headerlink" title="4.5 生产经验——生产者如何提高吞吐量"></a>4.5 生产经验——生产者如何提高吞吐量</h2><p>略</p>
<h2 id="4-6-生产经验——数据可靠性"><a href="#4-6-生产经验——数据可靠性" class="headerlink" title="4.6 生产经验——数据可靠性"></a>4.6 生产经验——数据可靠性</h2><p>略</p>
<h2 id="4-7-生产经验——数据去重"><a href="#4-7-生产经验——数据去重" class="headerlink" title="4.7 生产经验——数据去重"></a>4.7 生产经验——数据去重</h2><p>略</p>
<h2 id="4-8-生产经验——数据有序"><a href="#4-8-生产经验——数据有序" class="headerlink" title="4.8 生产经验——数据有序"></a>4.8 生产经验——数据有序</h2><p>略</p>
<h2 id="4-9-生产经验——数据乱序"><a href="#4-9-生产经验——数据乱序" class="headerlink" title="4.9 生产经验——数据乱序"></a>4.9 生产经验——数据乱序</h2><p>略</p>
<h1 id="五、kafka-Broker"><a href="#五、kafka-Broker" class="headerlink" title="五、kafka Broker"></a>五、kafka Broker</h1><h2 id="5-1-kafka-broker工作流程"><a href="#5-1-kafka-broker工作流程" class="headerlink" title="5.1 kafka broker工作流程"></a>5.1 kafka broker工作流程</h2><h3 id="5-1-1-zookeeper存储的kafka信息"><a href="#5-1-1-zookeeper存储的kafka信息" class="headerlink" title="5.1.1 zookeeper存储的kafka信息"></a>5.1.1 zookeeper存储的kafka信息</h3><p>（1）启动 Zookeeper 客户端。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node3 ~]# /apps/apache-zookeeper/bin/zkCli.sh<br></code></pre></td></tr></table></figure>

<p>（2）通过 ls命令可以查看 kafka 相关信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[zk: localhost:2181(CONNECTED) 0] ls /kafka<br>[admin, brokers, cluster, config, consumers, controller, controller_epoch, feature, isr_change_notification, latest_producer_id_block, log_dir_event_notification]<br></code></pre></td></tr></table></figure>

<p>这里推荐一个软件插卡zookeeper的信息，软件叫“prettyzoo”</p>
<p><strong>在zookeeper的服务端存储的Kafka相关信息：</strong></p>
<p><code>/kafka/brokers/ids</code>：记录有哪些服务器</p>
<p>举例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[zk: localhost:2181(CONNECTED) 1] ls /kafka/brokers/ids<br>[1, 2, 3] #这里的结果和我们kafka配置文件里的broker.id的数字一一对应<br></code></pre></td></tr></table></figure>

<p><code>/kafka/brokers/topics/first/partitions/0/state</code>：记录谁是leader，有哪些服务器可用</p>
<p>注意：“first”是topics中的一个主题，创建了什么主题就可以查看什么主题</p>
<p>举例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[zk: localhost:2181(CONNECTED) 2] get /kafka/brokers/topics/first/partitions/0/state<br>&#123;&quot;controller_epoch&quot;:1,&quot;leader&quot;:3,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[3,1,2]&#125;<br></code></pre></td></tr></table></figure>

<p><code>/kafka/consumers</code>：0.9版本之前用于保存offset信息，0.9版本之后offset存储在kafka主题中</p>
<p><code>/kafka/controller</code>：辅助选择leader</p>
<h3 id="5-1-2-kafka-Broker总体工作流程"><a href="#5-1-2-kafka-Broker总体工作流程" class="headerlink" title="5.1.2 kafka Broker总体工作流程"></a>5.1.2 kafka Broker总体工作流程</h3><ol>
<li>kafka broker启动后会在zookeeper注册</li>
<li>controller谁先注册，谁说了算</li>
<li>由选举出来的controller监听brokers节点变化</li>
<li>controller决定leader选举</li>
</ol>
<blockquote>
<p>选举规则：在isr中存活为前提，按照AR中排在前面的优先。例如：ar[1,0,2]，isr[1,0,2]时，那么leader就会按照1–&gt;0–&gt;2的顺序轮训。</p>
<p>AR：kafka分区中所有副本的统称</p>
</blockquote>
<ol start="5">
<li>controller将节点信息上传到zookeeper</li>
<li>其他controller从zookeeper同步相关信息</li>
<li>假设broker1中leader挂了</li>
<li>controller监听到节点的变化</li>
<li>获取isr</li>
<li>选举新的leader（在isr中存活为前提，按照ar中排在前面的优先）</li>
<li>更新leader及isr</li>
</ol>
<p><strong>模拟kafka上下线，zookeeper中数据变化</strong></p>
<p>(1）查看/kafka/brokers/ids路径上的节点</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[zk: localhost:2181(CONNECTED) 3] ls /kafka/brokers/ids<br>[1, 2, 3]<br></code></pre></td></tr></table></figure>

<p>（2）查看/kafka/controller路径上的数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[zk: localhost:2181(CONNECTED) 4] get /kafka/controller<br>&#123;&quot;version&quot;:1,&quot;brokerid&quot;:1,&quot;timestamp&quot;:&quot;1677029096948&quot;&#125;<br></code></pre></td></tr></table></figure>

<p>（3）查看/kafka/brokers/topics/first/partitions/0/state 路径上的数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[zk: localhost:2181(CONNECTED) 7] get /kafka/brokers/topics/first/partitions/0/state<br>&#123;&quot;controller_epoch&quot;:5,&quot;leader&quot;:2,&quot;version&quot;:1,&quot;leader_epoch&quot;:0,&quot;isr&quot;:[2,3,1]&#125;<br></code></pre></td></tr></table></figure>

<p>（4）停止 kafka-node3 上的 kafka。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node3 kafka]# /apps/kafka/bin/kafka-server-stop.sh<br></code></pre></td></tr></table></figure>

<p>（5）再次查看/kafka/brokers/ids 路径上的节点。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[zk: localhost:2181(CONNECTED) 8] ls /kafka/brokers/ids<br>[1, 2]<br></code></pre></td></tr></table></figure>

<p>（6）再次查看/kafka/controller 路径上的数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[zk: localhost:2181(CONNECTED) 9] get /kafka/controller<br>&#123;&quot;version&quot;:1,&quot;brokerid&quot;:1,&quot;timestamp&quot;:&quot;1677029096948&quot;&#125;<br></code></pre></td></tr></table></figure>

<p>（7）再次查看/kafka/brokers/topics/first/partitions/0/state 路径上的数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[zk: localhost:2181(CONNECTED) 10] get /kafka/brokers/topics/first/partitions/0/state<br>&#123;&quot;controller_epoch&quot;:5,&quot;leader&quot;:2,&quot;version&quot;:1,&quot;leader_epoch&quot;:1,&quot;isr&quot;:[2,1]&#125;<br></code></pre></td></tr></table></figure>

<p>（8）启动 kafka-node3 上的 kafka。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node3 kafka]# /apps/kafka/bin/kafka-server-start.sh -daemon /apps/kafka/config/server.properties<br></code></pre></td></tr></table></figure>

<p>（9）再次查看/kafka/brokers/ids路径上的节点</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[zk: localhost:2181(CONNECTED) 11] ls /kafka/brokers/ids<br>[1, 2, 3]<br></code></pre></td></tr></table></figure>

<p>（10）再次查看/kafka/controller路径上的数据</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[zk: localhost:2181(CONNECTED) 12] get /kafka/controller<br>&#123;&quot;version&quot;:1,&quot;brokerid&quot;:1,&quot;timestamp&quot;:&quot;1677029096948&quot;&#125;<br></code></pre></td></tr></table></figure>

<p>（11）再次观察/kafka/brokers/topics/first/partitions/0/state 路径上的数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[zk: localhost:2181(CONNECTED) 14] get /kafka/brokers/topics/first/partitions/0/state<br>&#123;&quot;controller_epoch&quot;:5,&quot;leader&quot;:2,&quot;version&quot;:1,&quot;leader_epoch&quot;:1,&quot;isr&quot;:[2,1,3]&#125;<br></code></pre></td></tr></table></figure>

<h3 id="5-1-3-broker-重要参数"><a href="#5-1-3-broker-重要参数" class="headerlink" title="5.1.3 broker 重要参数"></a>5.1.3 broker 重要参数</h3><h2 id="5-2-生产经验——节点服役-退役"><a href="#5-2-生产经验——节点服役-退役" class="headerlink" title="5.2 生产经验——节点服役/退役"></a>5.2 生产经验——节点服役/退役</h2><h3 id="5-2-1-服役新节点"><a href="#5-2-1-服役新节点" class="headerlink" title="5.2.1 服役新节点"></a>5.2.1 服役新节点</h3><h4 id="5-2-1-1-准备新的节点"><a href="#5-2-1-1-准备新的节点" class="headerlink" title="5.2.1.1 准备新的节点"></a>5.2.1.1 准备新的节点</h4><p>准备一台新的服务器，并安装jdk和kafka3.0，这里省略所有的安装过程</p>
<p>新添加服务器的信息：</p>
<table>
<thead>
<tr>
<th>服务器地址</th>
<th>服务器系统</th>
<th>安装的软件</th>
<th>hostname</th>
</tr>
</thead>
<tbody><tr>
<td>10.0.0.37</td>
<td>CentOS7.9</td>
<td>kafka</td>
<td>kafka-node4</td>
</tr>
</tbody></table>
<h4 id="5-2-1-2-kafka-node1修改hosts文件"><a href="#5-2-1-2-kafka-node1修改hosts文件" class="headerlink" title="5.2.1.2 kafka-node1修改hosts文件"></a>5.2.1.2 kafka-node1修改hosts文件</h4><p>kafka-node1节点修改/etc/hosts内容，大概内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /etc/hosts<br>......<br>10.0.0.7 kafka-node1<br>10.0.0.17 kafka-node2<br>10.0.0.27 kafka-node3<br>10.0.0.37 kafka-node4<br></code></pre></td></tr></table></figure>

<h4 id="5-2-1-3-kafka-node1将公钥发送给kafka-node4节点"><a href="#5-2-1-3-kafka-node1将公钥发送给kafka-node4节点" class="headerlink" title="5.2.1.3 kafka-node1将公钥发送给kafka-node4节点"></a>5.2.1.3 kafka-node1将公钥发送给kafka-node4节点</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> ssh-copy-id kafka-node4</span><br>/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;<br>The authenticity of host &#x27;kafka-node4 (10.0.0.37)&#x27; can&#x27;t be established.<br>ECDSA key fingerprint is SHA256:OtC+Vn9Hy9pn4GTmY3iZM1p8YHlskV72H7+CnSBgPpY.<br>ECDSA key fingerprint is MD5:b2:d1:ec:a4:20:c8:db:4d:bd:6c:4c:f0:2b:1a:4f:29.<br>Are you sure you want to continue connecting (yes/no)? yes<br>/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed<br>/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys<br>root@kafka-node4&#x27;s password: <br><br>Number of key(s) added: 1<br><br>Now try logging into the machine, with:   &quot;ssh &#x27;kafka-node4&#x27;&quot;<br>and check to make sure that only the key(s) you wanted were added.<br></code></pre></td></tr></table></figure>

<h4 id="5-2-1-4-kafka-node1节点将hosts文件发送给所有节点"><a href="#5-2-1-4-kafka-node1节点将hosts文件发送给所有节点" class="headerlink" title="5.2.1.4 kafka-node1节点将hosts文件发送给所有节点"></a>5.2.1.4 kafka-node1节点将hosts文件发送给所有节点</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# scp /etc/hosts kafka-node2:/etc/hosts<br><br>[root@kafka-node1 ~]# scp /etc/hosts kafka-node3:/etc/hosts<br><br>[root@kafka-node1 ~]# scp /etc/hosts kafka-node4:/etc/hosts<br></code></pre></td></tr></table></figure>

<h4 id="5-2-1-5-kafka-node4节点修改配置文件"><a href="#5-2-1-5-kafka-node4节点修改配置文件" class="headerlink" title="5.2.1.5 kafka-node4节点修改配置文件"></a>5.2.1.5 kafka-node4节点修改配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node4 apps]# cd /apps/kafka/config/<br>[root@kafka-node4 config]# vim server.properties<br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">############################ Server Basics #############################</span></span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"> The id of the broker. This must be <span class="hljs-built_in">set</span> to a unique <span class="hljs-built_in">integer</span> <span class="hljs-keyword">for</span> each broker.</span><br>broker.id=4<br></code></pre></td></tr></table></figure>

<h4 id="5-2-1-6-启动kafka"><a href="#5-2-1-6-启动kafka" class="headerlink" title="5.2.1.6 启动kafka"></a>5.2.1.6 启动kafka</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node4 kafka]# /apps/kafka/bin/kafka-server-start.sh -daemon /apps/kafka/config/server.properties<br></code></pre></td></tr></table></figure>

<h4 id="5-2-1-7-查看kafka集群topic信息"><a href="#5-2-1-7-查看kafka集群topic信息" class="headerlink" title="5.2.1.7 查看kafka集群topic信息"></a>5.2.1.7 查看kafka集群topic信息</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node4 kafka]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --topic first --describe<br>Topic: first	TopicId: Q57MrqL5Qda-f3XLayfesg	PartitionCount: 3	ReplicationFactor: 3	Configs: segment.bytes=1073741824<br>	Topic: first	Partition: 0	Leader: 2	Replicas: 2,3,1	Isr: 2,1,3<br>	Topic: first	Partition: 1	Leader: 2	Replicas: 3,2,1	Isr: 2,1,3<br>	Topic: first	Partition: 2	Leader: 1	Replicas: 1,3,2	Isr: 1,2,3<br></code></pre></td></tr></table></figure>

<p>此时我们发现topic的数据还是放在旧的kafka节点里，新的节点并没有分到数据。</p>
<h4 id="5-2-1-8-执行数据的负载均衡操作"><a href="#5-2-1-8-执行数据的负载均衡操作" class="headerlink" title="5.2.1.8 执行数据的负载均衡操作"></a>5.2.1.8 执行数据的负载均衡操作</h4><p>（1）创建一个要均衡的主题</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs json">vim topics-to-move.json<br>&#123;<br>    <span class="hljs-attr">&quot;topics&quot;</span>: [<br>        &#123;<span class="hljs-attr">&quot;topic&quot;</span>: <span class="hljs-string">&quot;first&quot;</span>&#125; #first是主题的名称。这里可以写根据需求添加多个主题。<br>    ],<br>    <span class="hljs-attr">&quot;version&quot;</span>: <span class="hljs-number">1</span><br>&#125;<br></code></pre></td></tr></table></figure>

<p>（2）生产一个负载均衡的计划</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">/apps/kafka/bin/kafka-reassign-partitions.sh --bootstrap-server kafka-node1:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;1,2,3,4&quot; --generate<br>Current partition replica assignment<br>&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,3,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[3,2,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,3,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;<br><br>Proposed partition reassignment configuration<br>&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[1,2,3],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3,4],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[3,4,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;<br></code></pre></td></tr></table></figure>

<p>从上面返回的结果来看，从以前的副本是在“1”，“2“，”3“，改变成”1“，”2“，”3“，”4“都有。</p>
<p>（3）创建副本存储计划（所有副本存储在 broker1、broker2、broker3、broker4中）。</p>
<p>内容为上方生成的“Proposed partition reassignment configuration”下的内容</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs json">vim increase-replication-factor.json<br>&#123;<span class="hljs-attr">&quot;version&quot;</span>:<span class="hljs-number">1</span>,<span class="hljs-attr">&quot;partitions&quot;</span>:[&#123;<span class="hljs-attr">&quot;topic&quot;</span>:<span class="hljs-string">&quot;first&quot;</span>,<span class="hljs-attr">&quot;partition&quot;</span>:<span class="hljs-number">0</span>,<span class="hljs-attr">&quot;replicas&quot;</span>:[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],<span class="hljs-attr">&quot;log_dirs&quot;</span>:[<span class="hljs-string">&quot;any&quot;</span>,<span class="hljs-string">&quot;any&quot;</span>,<span class="hljs-string">&quot;any&quot;</span>]&#125;,&#123;<span class="hljs-attr">&quot;topic&quot;</span>:<span class="hljs-string">&quot;first&quot;</span>,<span class="hljs-attr">&quot;partition&quot;</span>:<span class="hljs-number">1</span>,<span class="hljs-attr">&quot;replicas&quot;</span>:[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>],<span class="hljs-attr">&quot;log_dirs&quot;</span>:[<span class="hljs-string">&quot;any&quot;</span>,<span class="hljs-string">&quot;any&quot;</span>,<span class="hljs-string">&quot;any&quot;</span>]&#125;,&#123;<span class="hljs-attr">&quot;topic&quot;</span>:<span class="hljs-string">&quot;first&quot;</span>,<span class="hljs-attr">&quot;partition&quot;</span>:<span class="hljs-number">2</span>,<span class="hljs-attr">&quot;replicas&quot;</span>:[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">1</span>],<span class="hljs-attr">&quot;log_dirs&quot;</span>:[<span class="hljs-string">&quot;any&quot;</span>,<span class="hljs-string">&quot;any&quot;</span>,<span class="hljs-string">&quot;any&quot;</span>]&#125;]&#125;<br></code></pre></td></tr></table></figure>

<p>（4）执行副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">/apps/kafka/bin/kafka-reassign-partitions.sh --bootstrap-server kafka-node1:9092 --reassignment-json-file increase-replication-factor.json --execute<br>Current partition replica assignment<br><br>&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[2,3,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[3,2,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,3,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;<br><br>Save this to use as the --reassignment-json-file option during rollback<br>Successfully started partition reassignments for first-0,first-1,first-2 #出现成功的提示了<br></code></pre></td></tr></table></figure>

<p>（5）验证副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">/apps/kafka/bin/kafka-reassign-partitions.sh --bootstrap-server kafka-node1:9092 --reassignment-json-file increase-replication-factor.json --verify<br>Status of partition reassignment:<br>Reassignment of partition first-0 is complete.<br>Reassignment of partition first-1 is complete.<br>Reassignment of partition first-2 is complete.<br><br>Clearing broker-level throttles on brokers 1,2,3,4<br>Clearing topic-level throttles on topic first<br></code></pre></td></tr></table></figure>

<p>（6）再次查看topic信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">/apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --topic first --describe<br>Topic: first	TopicId: Q57MrqL5Qda-f3XLayfesg	PartitionCount: 3	ReplicationFactor: 3	Configs: segment.bytes=1073741824<br>	Topic: first	Partition: 0	Leader: 2	Replicas: 1,2,3	Isr: 2,1,3<br>	Topic: first	Partition: 1	Leader: 2	Replicas: 2,3,4	Isr: 2,3,4<br>	Topic: first	Partition: 2	Leader: 1	Replicas: 3,4,1	Isr: 1,3,4<br></code></pre></td></tr></table></figure>

<h4 id="5-2-1-9-kafka添加节点总结："><a href="#5-2-1-9-kafka添加节点总结：" class="headerlink" title="5.2.1.9 kafka添加节点总结："></a>5.2.1.9 kafka添加节点总结：</h4><ol>
<li>新旧节点都修改hosts文件，添加新的hosts解析（如果kafka集群不用dns解析可以不加）</li>
<li>安装新的kafka，修改配置文件（主要修改broker.id，其余跟集群节点的一样即可），启动新的kafka</li>
<li>创建主题负载均衡的json文件（告诉kafka哪些主题需要做数据移动）</li>
<li>使用刚创建的文件在kafka里创建负载均衡计划（让kafka根据文件做一个数据移动计划）</li>
<li>使用上一步中返回的第二段结果创建副本存储计划（告诉kafka哪些主题的数据需要放在哪些副本上）</li>
<li>使用kafka执行副本存储计划（让kafka执行主题的数据副本移动）</li>
<li>最后进行验证（可以到zookeeper里的ids进行验证，验证结果看下图）（也就是说：新添加的节点在配置文件上不用修改关于zookeeper的内容也可以登记在zookeeper上）</li>
</ol>
<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-4.png" srcset="/blog/img/loading.gif"></p>
<h3 id="5-2-2-退役旧节点"><a href="#5-2-2-退役旧节点" class="headerlink" title="5.2.2 退役旧节点"></a>5.2.2 退役旧节点</h3><h4 id="5-2-2-1执行负载均衡操作"><a href="#5-2-2-1执行负载均衡操作" class="headerlink" title="5.2.2.1执行负载均衡操作"></a>5.2.2.1执行负载均衡操作</h4><p>（1）创建一个负载均衡主题</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs json">[root@kafka-node1 ~]# vim topics-to-move.json<br>&#123;<br>    <span class="hljs-attr">&quot;topics&quot;</span>: [<br>        &#123;<span class="hljs-attr">&quot;topic&quot;</span>: <span class="hljs-string">&quot;first&quot;</span>&#125; #first是主题的名称。这里可以写根据需求添加多个主题。<br>    ],<br>    <span class="hljs-attr">&quot;version&quot;</span>: <span class="hljs-number">1</span><br>&#125;<br></code></pre></td></tr></table></figure>

<p>（2）创建执行计划（同时也会生成一个副本存储计划的的内容，用于之后的topic副本修改）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-reassign-partitions.sh --bootstrap-server kafka-node1:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;1,2,3&quot; --generate<br>Current partition replica assignment<br>&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[1,2,3],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3,4],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[3,4,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;<br><br>Proposed partition reassignment configuration<br>&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,2,3],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,3,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;<br></code></pre></td></tr></table></figure>

<p>（3）创建副本存存储计划（内容为上方“Proposed partition reassignment configuration”下的内容）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# vim increase-replication-factor.json<br>&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[3,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,2,3],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,3,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;<br></code></pre></td></tr></table></figure>

<p>（4）执行副本存储计划</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-reassign-partitions.sh --bootstrap-server kafka-node1:9092 --reassignment-json-file increase-replication-factor.json --execute<br>Current partition replica assignment<br><br>&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[1,2,3],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[2,3,4],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[3,4,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;]&#125;<br><br>Save this to use as the --reassignment-json-file option during rollback<br>Successfully started partition reassignments for first-0,first-1,first-2<br></code></pre></td></tr></table></figure>

<p>（5）验证副本存储计划</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-reassign-partitions.sh --bootstrap-server kafka-node1:9092 --reassignment-json-file increase-replication-factor.json --verify<br>Status of partition reassignment:<br>Reassignment of partition first-0 is complete.<br>Reassignment of partition first-1 is complete.<br>Reassignment of partition first-2 is complete.<br><br>Clearing broker-level throttles on brokers 1,2,3,4 #虽然“4”节点还在，但是主题的数据副本已经都移动到“1,2,3”节点里了，详细可以看下面的结果<br>Clearing topic-level throttles on topic first<br></code></pre></td></tr></table></figure>

<p>（6）查看topic信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --topic first --describe<br>Topic: first	TopicId: Q57MrqL5Qda-f3XLayfesg	PartitionCount: 3	ReplicationFactor: 3	Configs: segment.bytes=1073741824<br>	Topic: first	Partition: 0	Leader: 2	Replicas: 3,1,2	Isr: 2,1,3<br>	Topic: first	Partition: 1	Leader: 2	Replicas: 1,2,3	Isr: 2,3,1<br>	Topic: first	Partition: 2	Leader: 1	Replicas: 2,3,1	Isr: 1,3,2<br></code></pre></td></tr></table></figure>

<h4 id="5-2-2-2-停止kafka节点"><a href="#5-2-2-2-停止kafka节点" class="headerlink" title="5.2.2.2 停止kafka节点"></a>5.2.2.2 停止kafka节点</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node4 ~]# /apps/kafka/bin/kafka-server-stop.sh<br></code></pre></td></tr></table></figure>

<h4 id="5-2-2-3-退役节点总结"><a href="#5-2-2-3-退役节点总结" class="headerlink" title="5.2.2.3 退役节点总结"></a>5.2.2.3 退役节点总结</h4><ol>
<li>退役节点前，如果该节点有topics数据，要把数据移动到其他节点</li>
<li>在移动完topics数据后才能停止kafka节点</li>
</ol>
<h2 id="5-3-kafka副本"><a href="#5-3-kafka副本" class="headerlink" title="5.3 kafka副本"></a>5.3 kafka副本</h2><h3 id="5-3-1-副本基本信息"><a href="#5-3-1-副本基本信息" class="headerlink" title="5.3.1 副本基本信息"></a>5.3.1 副本基本信息</h3><p>（1）Kafka 副本作用：提高数据可靠性。</p>
<p>（2）Kafka 默认副本 1 个，生产环境一般配置为 2 个，保证数据可靠性；太多副本会<br>增加磁盘存储空间，增加网络上数据传输，降低效率。</p>
<p>（3）Kafka 中副本分为：Leader 和 Follower。Kafka 生产者只会把数据发往 Leader，<br>然后 Follower 找 Leader 进行同步数据。</p>
<p>（4）Kafka 分区中的所有副本统称为 AR（Assigned Repllicas）。</p>
<p>AR = ISR + OSR</p>
<p>ISR：表示和 Leader 保持同步的 Follower 集合。如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。该时间阈值由 replica.lag.time.max.ms 参数设定，默认 30s。Leader 发生故障之后，就会从 ISR中选举新的 Leader。（正常副本）</p>
<p>OSR：表示 Follower 与 Leader副本同步时，延迟过多的副本。（超时副本）</p>
<h3 id="5-3-2-Leader-选举流程"><a href="#5-3-2-Leader-选举流程" class="headerlink" title="5.3.2 Leader 选举流程"></a>5.3.2 Leader 选举流程</h3><h4 id="5-3-2-1-leader选举流程"><a href="#5-3-2-1-leader选举流程" class="headerlink" title="5.3.2.1 leader选举流程"></a>5.3.2.1 leader选举流程</h4><p>Kafka 集群中有一个 broker 的 Controller 会被选举为 Controller Leader，负责<font color=red>管理集群broker的上下线</font>，所有 topic 的<font color=red>分区副本分配</font>和 <font color=red>Leader</font> 选举等工作。</p>
<p>Controller 的信息同步工作是依赖于 Zookeeper的。</p>
<p><font color=orange><code>Leader选举流程</code>：</font></p>
<p>（1）broker启动后在zk中注册</p>
<p>（2）controller谁先注册，谁说了算</p>
<p>（3）由选举出来的controller监听brokers节点变化</p>
<p>（4）controller决定选举</p>
<blockquote>
<p>选举规则：</p>
<p>在isr中存活为前提，按照AR中排在前面的优先。</p>
<p>例如：ar[1,0,2]，isr[1,0,2]，那么leader就会按照1,0,2的顺序轮询</p>
</blockquote>
<p>（5）controller将节点信息上传到zookeeper</p>
<p>（6）其他controller从zookeeper同步相关信息</p>
<p>（7）假设brokers1中leader挂了</p>
<p>（8）controller监听到节点变化</p>
<p>（9）获取isr</p>
<p>（10）选举新的leader（在isr中存活为前提，按照AR中排在前面的优先）</p>
<p>（11）更新leader及isr</p>
<h4 id="5-3-2-2-验证leader选举流程"><a href="#5-3-2-2-验证leader选举流程" class="headerlink" title="5.3.2.2 验证leader选举流程"></a>5.3.2.2 验证leader选举流程</h4><p>此次试验需要准备4个kafka节点</p>
<p>（1）创建一个新的 topic，4个分区，4 个副本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --create --topic zhou --partitions 4 --replication-factor 4<br>Created topic zhou.<br></code></pre></td></tr></table></figure>

<p>（2）查看 Leader分布情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --describe --topic zhou<br>Topic: zhou	TopicId: L-kByjOTQ7KeDYBordZi-Q	PartitionCount: 4	ReplicationFactor: 4	Configs: segment.bytes=1073741824<br>	Topic: zhou	Partition: 0	Leader: 2	Replicas: 2,4,1,3	Isr: 2,4,1,3<br>	Topic: zhou	Partition: 1	Leader: 3	Replicas: 3,1,2,4	Isr: 3,1,2,4<br>	Topic: zhou	Partition: 2	Leader: 4	Replicas: 4,2,3,1	Isr: 4,2,3,1<br>	Topic: zhou	Partition: 3	Leader: 1	Replicas: 1,3,4,2	Isr: 1,3,4,2<br></code></pre></td></tr></table></figure>

<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-5.png" srcset="/blog/img/loading.gif"></p>
<p>（3）停止掉kafka-node4的kafka进程，并查看 Leader分区情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">停止kafka-node4的进程</span><br>[root@kafka-node4 ~]# /apps/kafka/bin/kafka-server-stop.sh<br><span class="hljs-meta">#</span><span class="bash">查看leader分区情况</span><br>[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --describe --topic zhou<br>Topic: zhou	TopicId: L-kByjOTQ7KeDYBordZi-Q	PartitionCount: 4	ReplicationFactor: 4	Configs: segment.bytes=1073741824<br>	Topic: zhou	Partition: 0	Leader: 2	Replicas: 2,4,1,3	Isr: 2,1,3<br>	Topic: zhou	Partition: 1	Leader: 3	Replicas: 3,1,2,4	Isr: 3,1,2<br>	Topic: zhou	Partition: 2	Leader: 2	Replicas: 4,2,3,1	Isr: 2,3,1<br>	Topic: zhou	Partition: 3	Leader: 1	Replicas: 1,3,4,2	Isr: 1,3,2<br></code></pre></td></tr></table></figure>

<p>当我们停掉kafka-node4节点后</p>
<p>本来第3个分区的leader为4的，现在变成2。</p>
<p>isr就剩下1，2，3。</p>
<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-6.png" srcset="/blog/img/loading.gif"></p>
<p>（4）停止掉kafka-node3的kafka进程，并查看 Leader分区情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">停止kafka-node3的进程</span><br>[root@kafka-node3 ~]# /apps/kafka/bin/kafka-server-stop.sh<br><span class="hljs-meta">#</span><span class="bash">查看leader分区情况</span><br>[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --describe --topic zhou<br>Topic: zhou	TopicId: L-kByjOTQ7KeDYBordZi-Q	PartitionCount: 4	ReplicationFactor: 4	Configs: segment.bytes=1073741824<br>	Topic: zhou	Partition: 0	Leader: 2	Replicas: 2,4,1,3	Isr: 2,1<br>	Topic: zhou	Partition: 1	Leader: 1	Replicas: 3,1,2,4	Isr: 1,2<br>	Topic: zhou	Partition: 2	Leader: 2	Replicas: 4,2,3,1	Isr: 2,1<br>	Topic: zhou	Partition: 3	Leader: 1	Replicas: 1,3,4,2	Isr: 1,2<br></code></pre></td></tr></table></figure>

<p>当我们停掉kafka-node3节点后</p>
<p>本来第2个分区的leader为3的，现在变成1。</p>
<p>isr就剩下1，2。</p>
<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-7.png" srcset="/blog/img/loading.gif"></p>
<p>（5）启动kafka-node4的 kafka 进程，并查看 Leader分区情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">停止kafka-node4的进程</span><br>[root@kafka-node4 ~]# /apps/kafka/bin/kafka-server-start.sh -daemon /apps/kafka/config/server.properties<br><span class="hljs-meta">#</span><span class="bash">查看leader分区情况</span><br>[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --describe --topic zhou<br>Topic: zhou	TopicId: L-kByjOTQ7KeDYBordZi-Q	PartitionCount: 4	ReplicationFactor: 4	Configs: segment.bytes=1073741824<br>	Topic: zhou	Partition: 0	Leader: 2	Replicas: 2,4,1,3	Isr: 2,1,4<br>	Topic: zhou	Partition: 1	Leader: 1	Replicas: 3,1,2,4	Isr: 1,2,4<br>	Topic: zhou	Partition: 2	Leader: 2	Replicas: 4,2,3,1	Isr: 2,1,4<br>	Topic: zhou	Partition: 3	Leader: 1	Replicas: 1,3,4,2	Isr: 1,2,4<br></code></pre></td></tr></table></figure>

<p>当我们启动kafka-node4节点后</p>
<p>leader没变，所有isr的顺序再原来的基础上加“4”</p>
<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-8.png" srcset="/blog/img/loading.gif"></p>
<p>（6）启动kafka-node3的 kafka 进程，并查看 Leader分区情况</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">停止kafka-node3的进程</span><br>[root@kafka-node3 ~]# /apps/kafka/bin/kafka-server-start.sh -daemon /apps/kafka/config/server.properties<br><span class="hljs-meta">#</span><span class="bash">查看leader分区情况</span><br>[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --describe --topic zhou<br>Topic: zhou	TopicId: L-kByjOTQ7KeDYBordZi-Q	PartitionCount: 4	ReplicationFactor: 4	Configs: segment.bytes=1073741824<br>	Topic: zhou	Partition: 0	Leader: 2	Replicas: 2,4,1,3	Isr: 2,1,4,3<br>	Topic: zhou	Partition: 1	Leader: 1	Replicas: 3,1,2,4	Isr: 1,2,4,3<br>	Topic: zhou	Partition: 2	Leader: 2	Replicas: 4,2,3,1	Isr: 2,1,4,3<br>	Topic: zhou	Partition: 3	Leader: 1	Replicas: 1,3,4,2	Isr: 1,2,4,3<br></code></pre></td></tr></table></figure>

<p>当我们启动kafka-node3节点后</p>
<p>leader没变，所有isr的顺序再原来的基础上加“3”</p>
<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-9.png" srcset="/blog/img/loading.gif"></p>
<h3 id="5-3-3-Leader-和-和-Follower-故障-处理"><a href="#5-3-3-Leader-和-和-Follower-故障-处理" class="headerlink" title="5.3.3 Leader 和 和 Follower 故障 处理"></a>5.3.3 Leader 和 和 Follower 故障 处理</h3><h4 id="5-3-3-1-Follower故障处理细节"><a href="#5-3-3-1-Follower故障处理细节" class="headerlink" title="5.3.3.1 Follower故障处理细节"></a>5.3.3.1 Follower故障处理细节</h4><p><strong>Follower</strong></p>
<p>（1） Follower发生故障后会被临时踢出ISR</p>
<p>（2） 这个期间Leader和Follower继续接收数据</p>
<p>（3）待该Follower恢复后，Follower会读取本地磁盘记录的<br>上次的HW，并将log文件高于HW的部分截取掉，从HW开始<br>向Leader进行同步。</p>
<p>（4）等该Follower的LEO 大于等于该Partition的HW，即<br>Follower追上Leader之后，就可以重新加入ISR了。</p>
<p>名词解释：</p>
<p><font color=red>LEO（Log End Offset）</font>：每个副本的最后一个offset，LEO其实就是最新的offset + 1。</p>
<p><font color=red>HW（High Watermark）</font>：所有副本中最小的LEO 。</p>
<h4 id="5-3-3-2-Leader故障处理细节"><a href="#5-3-3-2-Leader故障处理细节" class="headerlink" title="5.3.3.2 Leader故障处理细节"></a>5.3.3.2 Leader故障处理细节</h4><p><strong>Leader 故障</strong></p>
<p>（1） Leader发生故障之后，会从ISR中选出一个新的Leader</p>
<p>（2）为保证多个副本之间的数据一致性，其余的Follower会先<br>将各自的log文件高于HW的部分截掉，然后从新的Leader同步<br>数据。</p>
<p><font color=red>注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。</font></p>
<p>名词解释：</p>
<p><font color=red>LEO（Log End Offset）</font>：每个副本的最后一个offset，LEO其实就是最新的offset + 1。</p>
<p><font color=red>HW（High Watermark）</font>：所有副本中最小的LEO 。</p>
<h3 id="5-3-4-分区副本分配"><a href="#5-3-4-分区副本分配" class="headerlink" title="5.3.4 分区副本分配"></a>5.3.4 分区副本分配</h3><p>如果 kafka 服务器只有 4 个节点，那么设置 kafka 的分区数大于服务器台数，在 kafka底层如何分配存储副本呢？</p>
<p><strong>创建 16 分区，3 个副本</strong></p>
<p>（1）创建一个新的 topic，名称为 second。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --create --partitions 16 --replication-factor 3 --topic second<br>Created topic second.<br></code></pre></td></tr></table></figure>

<p>（2）查看分区和副本情况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --describe --topic second<br>Topic: second	TopicId: B1J48IosRum9kKvyJlh5pw	PartitionCount: 16	ReplicationFactor: 3	Configs: segment.bytes=1073741824<br>	Topic: second	Partition: 0	Leader: 2	Replicas: 2,3,4	Isr: 2,3,4 #副本第一列对比第二列错开1位<br>	Topic: second	Partition: 1	Leader: 3	Replicas: 3,4,1	Isr: 3,4,1<br>	Topic: second	Partition: 2	Leader: 4	Replicas: 4,1,2	Isr: 4,1,2<br>	Topic: second	Partition: 3	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3<br>	<br>	Topic: second	Partition: 4	Leader: 2	Replicas: 2,4,1	Isr: 2,4,1 #副本第一列对比第二列错开2位<br>	Topic: second	Partition: 5	Leader: 3	Replicas: 3,1,2	Isr: 3,1,2<br>	Topic: second	Partition: 6	Leader: 4	Replicas: 4,2,3	Isr: 4,2,3<br>	Topic: second	Partition: 7	Leader: 1	Replicas: 1,3,4	Isr: 1,3,4<br>	<br>	Topic: second	Partition: 8	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3 #副本第一列对比第二列错开3位<br>	Topic: second	Partition: 9	Leader: 3	Replicas: 3,2,4	Isr: 3,2,4<br>	Topic: second	Partition: 10	Leader: 4	Replicas: 4,3,1	Isr: 4,3,1<br>	Topic: second	Partition: 11	Leader: 1	Replicas: 1,4,2	Isr: 1,4,2<br>	<br>	Topic: second	Partition: 12	Leader: 2	Replicas: 2,3,4	Isr: 2,3,4 #副本第一列对比第二列错开4位<br>	Topic: second	Partition: 13	Leader: 3	Replicas: 3,4,1	Isr: 3,4,1<br>	Topic: second	Partition: 14	Leader: 4	Replicas: 4,1,2	Isr: 4,1,2<br>	Topic: second	Partition: 15	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3<br></code></pre></td></tr></table></figure>

<p>kafka分区副本分配图</p>
<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-10.png" srcset="/blog/img/loading.gif"></p>
<h3 id="5-3-5-生产经验-——-手动调整分区副本存储"><a href="#5-3-5-生产经验-——-手动调整分区副本存储" class="headerlink" title="5.3.5 生产经验 —— 手动调整分区副本存储"></a>5.3.5 生产经验 —— 手动调整分区副本存储</h3><p>在生产环境中，每台服务器的配置和性能不一致，但是Kafka只会根据自己的代码规则创建对应的分区副本，就会导致个别服务器存储压力较大。所有需要手动调整分区副本的存储。</p>
<h4 id="5-3-5-1-手动调整分区副本存储"><a href="#5-3-5-1-手动调整分区副本存储" class="headerlink" title="5.3.5.1 手动调整分区副本存储"></a>5.3.5.1 手动调整分区副本存储</h4><p>需求：创建一个新的topic，4个分区，两个副本，名称为three。将该topic的所有副本都存储到broker1和broker2两台服务器上。</p>
<p>（1）创建一个新的 topic，名称为 three。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --create --partitions 4 --replication-factor 2 --topic three<br>Created topic three.<br></code></pre></td></tr></table></figure>

<p>（2）查看分区副本存储情况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --describe --topic three <br>Topic: three	TopicId: VpazFuhTQKKciktL-Hlxuw	PartitionCount: 4	ReplicationFactor: 2	Configs: segment.bytes=1073741824<br>	Topic: three	Partition: 0	Leader: 4	Replicas: 4,2	Isr: 4,2<br>	Topic: three	Partition: 1	Leader: 1	Replicas: 1,3	Isr: 1,3<br>	Topic: three	Partition: 2	Leader: 2	Replicas: 2,4	Isr: 2,4<br>	Topic: three	Partition: 3	Leader: 3	Replicas: 3,1	Isr: 3,1<br></code></pre></td></tr></table></figure>

<p>（3）创建副本存储计划（所有副本都指定存储在 broker1、broker2中）。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs json">[root@kafka-node1 ~]# vim increase-replication-factor.json<br>&#123;<br> <span class="hljs-attr">&quot;version&quot;</span>:<span class="hljs-number">1</span>,<br> <span class="hljs-attr">&quot;partitions&quot;</span>:[<br> &#123;<span class="hljs-attr">&quot;topic&quot;</span>:<span class="hljs-string">&quot;three&quot;</span>,<span class="hljs-attr">&quot;partition&quot;</span>:<span class="hljs-number">0</span>,<span class="hljs-attr">&quot;replicas&quot;</span>:[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]&#125;,<br>&#123;<span class="hljs-attr">&quot;topic&quot;</span>:<span class="hljs-string">&quot;three&quot;</span>,<span class="hljs-attr">&quot;partition&quot;</span>:<span class="hljs-number">1</span>,<span class="hljs-attr">&quot;replicas&quot;</span>:[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>]&#125;,<br>&#123;<span class="hljs-attr">&quot;topic&quot;</span>:<span class="hljs-string">&quot;three&quot;</span>,<span class="hljs-attr">&quot;partition&quot;</span>:<span class="hljs-number">2</span>,<span class="hljs-attr">&quot;replicas&quot;</span>:[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]&#125;,<br>&#123;<span class="hljs-attr">&quot;topic&quot;</span>:<span class="hljs-string">&quot;three&quot;</span>,<span class="hljs-attr">&quot;partition&quot;</span>:<span class="hljs-number">3</span>,<span class="hljs-attr">&quot;replicas&quot;</span>:[<span class="hljs-number">2</span>,<span class="hljs-number">1</span>]&#125;]<br>&#125;<br></code></pre></td></tr></table></figure>

<p>（4）执行副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-reassign-partitions.sh --bootstrap-server kafka-node1:9092 --reassignment-json-file increase-replication-factor.json --execute<br>Current partition replica assignment<br><br>&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[4,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,3],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,4],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[3,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;]&#125;]&#125;<br><br>Save this to use as the --reassignment-json-file option during rollback<br>Successfully started partition reassignments for three-0,three-1,three-2,three-3<br></code></pre></td></tr></table></figure>

<p>（5）验证副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-reassign-partitions.sh --bootstrap-server kafka-node1:9092 --reassignment-json-file increase-replication-factor.json --verify<br>Status of partition reassignment:<br>Reassignment of partition three-0 is complete.<br>Reassignment of partition three-1 is complete.<br>Reassignment of partition three-2 is complete.<br>Reassignment of partition three-3 is complete.<br><br>Clearing broker-level throttles on brokers 1,2,3,4<br>Clearing topic-level throttles on topic three<br></code></pre></td></tr></table></figure>

<p>（6）查看分区副本存储情况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --describe --topic three <br>Topic: three	TopicId: VpazFuhTQKKciktL-Hlxuw	PartitionCount: 4	ReplicationFactor: 2	Configs: segment.bytes=1073741824<br>	Topic: three	Partition: 0	Leader: 1	Replicas: 1,2	Isr: 2,1<br>	Topic: three	Partition: 1	Leader: 1	Replicas: 1,2	Isr: 1,2<br>	Topic: three	Partition: 2	Leader: 2	Replicas: 2,1	Isr: 2,1<br>	Topic: three	Partition: 3	Leader: 2	Replicas: 2,1	Isr: 1,2<br></code></pre></td></tr></table></figure>

<h4 id="5-3-5-2-手动调整分区总结"><a href="#5-3-5-2-手动调整分区总结" class="headerlink" title="5.3.5.2 手动调整分区总结"></a>5.3.5.2 手动调整分区总结</h4><ol>
<li>手动调整分区前需要明确该主题需要调整的分区</li>
<li>根据需求创建调整分区的副本存储计划文件</li>
<li>之后使用kafka命令读取并执行该文件</li>
<li>验证主题的分区</li>
</ol>
<h3 id="5-3-6-生产经验——Leader-Partition负载平衡"><a href="#5-3-6-生产经验——Leader-Partition负载平衡" class="headerlink" title="5.3.6 生产经验——Leader Partition负载平衡"></a>5.3.6 生产经验——Leader Partition负载平衡</h3><p>正常情况下，Kafka本身会自动把Leader Partition均匀分散在各个机器上，来保证每台机器的读写吞吐量都是均匀的。但是如果某些broker宕机，会导致Leader Partition过于集中在其他少部分几台broker上，这会导致少数几台broker的读写请求压力过高，其他宕机的broker重启之后都是follower partition，读写请求很低，造成集群负载不均衡。</p>
<p>以下是影响kafka的leader partition负载平衡的参数：</p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>auto.leader.rebalance.enable</td>
<td>默认是 true。 自动 Leader Partition 平衡。生产环境中，leader 重选举的代价比较大，可能会带来性能影响，建议设置为 false 关闭。</td>
</tr>
<tr>
<td>leader.imbalance.per.broker.percentage</td>
<td>默认是 10%。每个 broker 允许的不平衡的 leader的比率。如果每个 broker 超过了这个值，控制器会触发 leader的平衡。</td>
</tr>
<tr>
<td>leader.imbalance.check.interval.seconds</td>
<td>默认值 300 秒。检查 leader 负载是否平衡的间隔时间。</td>
</tr>
</tbody></table>
<p>举例：</p>
<p>假设集群只有一个主题，该主题的存储情况如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">Topic: hello	Partition: 0	Leader: 0	Replicas: 3,0,2,1	Isr: 3,0,2,1 <br>Topic: hello	Partition: 1	Leader: 1	Replicas: 1,2,3,0	Isr: 1,2,3,0<br>Topic: hello	Partition: 2	Leader: 2	Replicas: 0,3,1,2	Isr: 0,3,1,2<br>Topic: hello	Partition: 3	Leader: 3	Replicas: 2,1,0,3	Isr: 2,1,0,3<br></code></pre></td></tr></table></figure>

<p>针对broker0节点，分区2的AR优先副本是0节点，但是0节点缺不是leader节点，所有不平衡数加1，AR副本总数是4。所以broker0节点不平衡率为1/4&gt;10%，需要再平衡。</p>
<p>broker2和broker3节点和broker0不平衡率一样，需要再平衡。broker1的不平衡数为0，不需要再平衡。</p>
<p>在生产环境上，原则上不要经常触发kafka的负载平衡，因为每次平衡都要消耗服务器的性能。</p>
<p>怎么不触发kafka的负载平衡？</p>
<ol>
<li>将<code>auto.leader.rebalance.enable</code>设置为false</li>
<li><code>leader.imbalance.per.broker.percentage</code>设置大一些</li>
</ol>
<h3 id="5-3-7-生产经验——增加副本因子"><a href="#5-3-7-生产经验——增加副本因子" class="headerlink" title="5.3.7 生产经验——增加副本因子"></a>5.3.7 生产经验——增加副本因子</h3><p>在生产环境当中，由于某个主题的重要等级需要提升，我们考虑增加副本。副本数的增加需要先制定计划，然后根据计划执行。</p>
<h4 id="5-3-7-1-使用副本存储计划文件增加副本"><a href="#5-3-7-1-使用副本存储计划文件增加副本" class="headerlink" title="5.3.7.1 使用副本存储计划文件增加副本"></a>5.3.7.1 使用副本存储计划文件增加副本</h4><p>（1）创建 topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --create --partitions 3 --replication-factor 1 --topic four<br>Created topic four.<br></code></pre></td></tr></table></figure>

<p>（2）查看topic信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --describe --topic four<br>Topic: four	TopicId: rf_y1eCBSoW4lmcq35fhIQ	PartitionCount: 3	ReplicationFactor: 1	Configs: segment.bytes=1073741824<br>	Topic: four	Partition: 0	Leader: 4	Replicas: 4	Isr: 4<br>	Topic: four	Partition: 1	Leader: 1	Replicas: 1	Isr: 1<br>	Topic: four	Partition: 2	Leader: 2	Replicas: 2	Isr: 2<br></code></pre></td></tr></table></figure>

<p>（3）创建副本存储计划（所有副本都指定存储在 broker1、broker2、broker3 中）。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs json">vim increase-replication-factor.json<br>&#123;<br>    <span class="hljs-attr">&quot;version&quot;</span>:<span class="hljs-number">1</span>,<br>    <span class="hljs-attr">&quot;partitions&quot;</span>:[<br>        &#123;<span class="hljs-attr">&quot;topic&quot;</span>:<span class="hljs-string">&quot;four&quot;</span>,<span class="hljs-attr">&quot;partition&quot;</span>:<span class="hljs-number">0</span>,<span class="hljs-attr">&quot;replicas&quot;</span>:[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]&#125;,<br>        &#123;<span class="hljs-attr">&quot;topic&quot;</span>:<span class="hljs-string">&quot;four&quot;</span>,<span class="hljs-attr">&quot;partition&quot;</span>:<span class="hljs-number">1</span>,<span class="hljs-attr">&quot;replicas&quot;</span>:[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]&#125;,<br>        &#123;<span class="hljs-attr">&quot;topic&quot;</span>:<span class="hljs-string">&quot;four&quot;</span>,<span class="hljs-attr">&quot;partition&quot;</span>:<span class="hljs-number">2</span>,<span class="hljs-attr">&quot;replicas&quot;</span>:[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>]&#125;]<br>&#125;<br></code></pre></td></tr></table></figure>

<p>(4）执行副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-reassign-partitions.sh --bootstrap-server kafka-node1:9092 --reassignment-json-file increase-replication-factor.json --execute<br>Current partition replica assignment<br><br>&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[4],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2],&quot;log_dirs&quot;:[&quot;any&quot;]&#125;]&#125;<br><br>Save this to use as the --reassignment-json-file option during rollback<br>Successfully started partition reassignments for four-0,four-1,four-2<br></code></pre></td></tr></table></figure>

<p>（5）验证副本存储计划。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-reassign-partitions.sh --bootstrap-server kafka-node1:9092 --reassignment-json-file increase-replication-factor.json --verify<br>Status of partition reassignment:<br>Reassignment of partition four-0 is complete.<br>Reassignment of partition four-1 is complete.<br>Reassignment of partition four-2 is complete.<br><br>Clearing broker-level throttles on brokers 1,2,3,4<br>Clearing topic-level throttles on topic four<br></code></pre></td></tr></table></figure>

<p>（6）查看分区副本存储情况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --describe --topic four<br>Topic: four	TopicId: rf_y1eCBSoW4lmcq35fhIQ	PartitionCount: 3	ReplicationFactor: 3	Configs: segment.bytes=1073741824<br>	Topic: four	Partition: 0	Leader: 1	Replicas: 1,2,3	Isr: 2,3,1<br>	Topic: four	Partition: 1	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3<br>	Topic: four	Partition: 2	Leader: 2	Replicas: 1,2,3	Isr: 2,3,1<br></code></pre></td></tr></table></figure>

<h4 id="5-7-3-2-直接使用命令增加副本"><a href="#5-7-3-2-直接使用命令增加副本" class="headerlink" title="5.7.3.2 直接使用命令增加副本"></a>5.7.3.2 直接使用命令增加副本</h4><p>（1）创建topic</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --create --partitions 3 --replication-factor 1 --topic test1<br>Created topic test1.<br></code></pre></td></tr></table></figure>

<p>（2）直接通过命令修改副本</p>
<p>在执行命令后，结果出现了错误。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --alter --topic test1 --partitions 3 --replication-factor 2<br>Option &quot;[replication-factor]&quot; can&#x27;t be used with option &quot;[alter]&quot;<br>Option                                   Description                            <br>------                                   -----------                            <br>--alter                                  Alter the number of partitions,        <br>                                           replica assignment, and/or           <br>                                           configuration for the topic.         <br>--at-min-isr-partitions                  if set when describing topics, only    <br>                                           show partitions whose isr count is   <br>                                           equal to the configured minimum.     <br>--bootstrap-server &lt;String: server to    REQUIRED: The Kafka server to connect  <br>  connect to&gt;                              to.                                  <br>--command-config &lt;String: command        Property file containing configs to be <br>  config property file&gt;                    passed to Admin Client. This is used <br>                                           only with --bootstrap-server option  <br>                                           for describing and altering broker   <br>                                           configs.                             <br>--config &lt;String: name=value&gt;            A topic configuration override for the <br>                                           topic being created or altered. The  <br>                                           following is a list of valid         <br>                                           configurations:                      <br>                                         	cleanup.policy                        <br>                                         	compression.type                      <br>                                         	delete.retention.ms                   <br>                                         	file.delete.delay.ms                  <br>                                         	flush.messages                        <br>                                         	flush.ms                              <br>                                         	follower.replication.throttled.       <br>                                           replicas                             <br>                                         	index.interval.bytes                  <br>                                         	leader.replication.throttled.replicas <br>                                         	local.retention.bytes                 <br>                                         	local.retention.ms                    <br>                                         	max.compaction.lag.ms                 <br>                                         	max.message.bytes                     <br>                                         	message.downconversion.enable         <br>                                         	message.format.version                <br>                                         	message.timestamp.difference.max.ms   <br>                                         	message.timestamp.type                <br>                                         	min.cleanable.dirty.ratio             <br>                                         	min.compaction.lag.ms                 <br>                                         	min.insync.replicas                   <br>                                         	preallocate                           <br>                                         	remote.storage.enable                 <br>                                         	retention.bytes                       <br>                                         	retention.ms                          <br>                                         	segment.bytes                         <br>                                         	segment.index.bytes                   <br>                                         	segment.jitter.ms                     <br>                                         	segment.ms                            <br>                                         	unclean.leader.election.enable        <br>                                         See the Kafka documentation for full   <br>                                           details on the topic configs. It is  <br>                                           supported only in combination with --<br>                                           create if --bootstrap-server option  <br>                                           is used (the kafka-configs CLI       <br>                                           supports altering topic configs with <br>                                           a --bootstrap-server option).        <br>--create                                 Create a new topic.                    <br>--delete                                 Delete a topic                         <br>--delete-config &lt;String: name&gt;           A topic configuration override to be   <br>                                           removed for an existing topic (see   <br>                                           the list of configurations under the <br>                                           --config option). Not supported with <br>                                           the --bootstrap-server option.       <br>--describe                               List details for the given topics.     <br>--disable-rack-aware                     Disable rack aware replica assignment  <br>--exclude-internal                       exclude internal topics when running   <br>                                           list or describe command. The        <br>                                           internal topics will be listed by    <br>                                           default                              <br>--help                                   Print usage information.               <br>--if-exists                              if set when altering or deleting or    <br>                                           describing topics, the action will   <br>                                           only execute if the topic exists.    <br>--if-not-exists                          if set when creating topics, the       <br>                                           action will only execute if the      <br>                                           topic does not already exist.        <br>--list                                   List all available topics.             <br>--partitions &lt;Integer: # of partitions&gt;  The number of partitions for the topic <br>                                           being created or altered (WARNING:   <br>                                           If partitions are increased for a    <br>                                           topic that has a key, the partition  <br>                                           logic or ordering of the messages    <br>                                           will be affected). If not supplied   <br>                                           for create, defaults to the cluster  <br>                                           default.                             <br>--replica-assignment &lt;String:            A list of manual partition-to-broker   <br>  broker_id_for_part1_replica1 :           assignments for the topic being      <br>  broker_id_for_part1_replica2 ,           created or altered.                  <br>  broker_id_for_part2_replica1 :                                                <br>  broker_id_for_part2_replica2 , ...&gt;                                           <br>--replication-factor &lt;Integer:           The replication factor for each        <br>  replication factor&gt;                      partition in the topic being         <br>                                           created. If not supplied, defaults   <br>                                           to the cluster default.              <br>--topic &lt;String: topic&gt;                  The topic to create, alter, describe   <br>                                           or delete. It also accepts a regular <br>                                           expression, except for --create      <br>                                           option. Put topic name in double     <br>                                           quotes and use the &#x27;\&#x27; prefix to     <br>                                           escape regular expression symbols; e.<br>                                           g. &quot;test\.topic&quot;.                    <br>--topics-with-overrides                  if set when describing topics, only    <br>                                           show topics that have overridden     <br>                                           configs                              <br>--unavailable-partitions                 if set when describing topics, only    <br>                                           show partitions whose leader is not  <br>                                           available                            <br>--under-min-isr-partitions               if set when describing topics, only    <br>                                           show partitions whose isr count is   <br>                                           less than the configured minimum.    <br>--under-replicated-partitions            if set when describing topics, only    <br>                                           show under replicated partitions     <br>--version                                Display Kafka version. <br></code></pre></td></tr></table></figure>

<p><strong>通过以上实验得知：不能单纯的通过选项修改副本，只能通过读取文件的方式修改副本。</strong></p>
<h2 id="5-4-文件存储"><a href="#5-4-文件存储" class="headerlink" title="5.4 文件存储"></a>5.4 文件存储</h2><h3 id="5-4-1-文件存储机制"><a href="#5-4-1-文件存储机制" class="headerlink" title="5.4.1 文件存储机制"></a>5.4.1 文件存储机制</h3><h4 id="5-4-1-1-topic数据的存储机制"><a href="#5-4-1-1-topic数据的存储机制" class="headerlink" title="5.4.1.1 topic数据的存储机制"></a>5.4.1.1 topic数据的存储机制</h4><p>Topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断 追加到该log文件末端，为防止log文件过大导致数据定位效率低下，Kafka采取了 分片和 索引机制，将每个partition分为多个segment。每个segment包括：“.index”文件、“.log”文件和.timeindex等文件。这些文件位于一个文件夹下，该<br>文件夹的命名规则为：topic名称+分区序号，例如：first-0。</p>
<h4 id="5-4-1-2-Topic-数据到底存储-在什么位置？"><a href="#5-4-1-2-Topic-数据到底存储-在什么位置？" class="headerlink" title="5.4.1.2 Topic 数据到底存储 在什么位置？"></a>5.4.1.2 Topic 数据到底存储 在什么位置？</h4><p>（1）启动生产者，并发送消息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-console-producer.sh --bootstrap-server kafka-node1:9092 --topic first<br><span class="hljs-meta">&gt;</span><span class="bash">hellow</span><br><span class="hljs-meta">&gt;</span><span class="bash">hi</span><br><span class="hljs-meta">&gt;</span><span class="bash">world</span><br><span class="hljs-meta">&gt;</span><br></code></pre></td></tr></table></figure>

<p>（2）查看 kafka-node1数据目录上的first的topic文件夹。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# cd /apps/kafka/data<br>[root@kafka-node1 data]# ll | grep first<br>drwxr-xr-x 2 root root 167 Feb 24 10:28 first-0<br>drwxr-xr-x 2 root root 167 Feb 23 19:10 first-1<br>drwxr-xr-x 2 root root 167 Feb 24 10:28 first-2<br></code></pre></td></tr></table></figure>

<p>（3）查看first-0文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# cd /apps/kafka/data/first-0<br>[root@kafka-node1 first-0]# ll<br>total 12<br>-rw-r--r-- 1 root root 10485760 Feb 23 14:21 00000000000000000000.index #偏移量所以文件<br>-rw-r--r-- 1 root root      223 Feb 24 10:28 00000000000000000000.log #日志文件<br>-rw-r--r-- 1 root root 10485756 Feb 23 14:21 00000000000000000000.timeindex #时间戳索引文件<br>-rw-r--r-- 1 root root       12 Feb 24 10:28 leader-epoch-checkpoint<br>-rw-r--r-- 1 root root       43 Feb 23 14:21 partition.metadata<br><span class="hljs-meta">#</span><span class="bash">注意：index和<span class="hljs-built_in">log</span>文件以当前segment的第一条消息offset命令</span><br></code></pre></td></tr></table></figure>

<p>（4）查看log日志，发现是乱码</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 first-0]# cat 00000000000000000000.log <br><span class="hljs-meta">&gt;</span><span class="bash">򂱨|þ3c|þ3cÿÿÿÿÿÿÿÿÿÿÿÿÿÿ</span> <br>                        hellowC£8Jh|þ_|þ_ÿÿÿÿÿÿÿÿÿÿÿÿÿÿ&quot;how are you:o§.@8*@8*ÿÿÿÿÿÿÿÿÿÿÿÿÿÿhi<br></code></pre></td></tr></table></figure>

<p>（5）通过工具查看index信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 first-0]# /apps/kafka/bin/kafka-run-class.sh kafka.tools.DumpLogSegments --files 00000000000000000000.index<br>Dumping 00000000000000000000.index<br>offset: 0 position: 0<br></code></pre></td></tr></table></figure>

<p>（6）通过工具查看log信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 first-0]# /apps/kafka/bin/kafka-run-class.sh kafka.tools.DumpLogSegments --files 00000000000000000000.log <br>Dumping 00000000000000000000.log<br>Starting offset: 0<br>baseOffset: 0 lastOffset: 0 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 0 CreateTime: 1677134279523 size: 74 magic: 2 compresscodec: none crc: 4123128424 isvalid: true<br>baseOffset: 1 lastOffset: 1 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 0 isTransactional: false isControl: false position: 74 CreateTime: 1677134290838 size: 79 magic: 2 compresscodec: none crc: 2738375272 isvalid: true<br>baseOffset: 2 lastOffset: 2 count: 1 baseSequence: -1 lastSequence: -1 producerId: -1 producerEpoch: -1 partitionLeaderEpoch: 2 isTransactional: false isControl: false position: 153 CreateTime: 1677205714986 size: 70 magic: 2 compresscodec: none crc: 2523899694 isvalid: true<br></code></pre></td></tr></table></figure>

<h4 id="5-4-1-3-index和log文件解读"><a href="#5-4-1-3-index和log文件解读" class="headerlink" title="5.4.1.3 index和log文件解读"></a>5.4.1.3 index和log文件解读</h4><p><strong>index文件解读</strong></p>
<p>index文件其实就是一个索引，记录了哪条消息在log文件中的位置，查找消息的时候先从index获取位置，然后就可以定位到消息在log文件具体哪个地方。</p>
<p>index采用了稀疏索引的方式去存储，不是每来一条消息就记录一个索引，而是当消息大于某个值的时候，就会记录一次索引，默认4KB。</p>
<p>稀疏存储也就选取一些消息的offset以及positioni进行存储，因为把对应片段的所有消息所以都存储，那么必然会占用大量的内存。</p>
<p><strong>log文件解读</strong></p>
<p>log文件是存储数据的文件，同时里面还存储了数据的索引、偏移量、时间等信息</p>
<h4 id="5-4-1-4-topic数据查找过程"><a href="#5-4-1-4-topic数据查找过程" class="headerlink" title="5.4.1.4 topic数据查找过程"></a>5.4.1.4 topic数据查找过程</h4><ol>
<li>根据index文件的目标offset定位segment文件</li>
<li>找到小于等于目标offser的最大offser对应的索引项</li>
<li>定位到log文件</li>
<li>象限便利找到目标Reccord</li>
</ol>
<p>注意点：</p>
<blockquote>
<ol>
<li>index为稀疏索引，大约每往log文件写入4kb数据，会往index文件写入一条索引。参数log.index.interval.bytes默认4kb。</li>
<li>Index文件中保存的offset为相对offset，这样能确保offset的值所占空间不会过大，因此能将offset的值控制在固定大小</li>
</ol>
</blockquote>
<h4 id="5-4-2-5-日志存储参数配置"><a href="#5-4-2-5-日志存储参数配置" class="headerlink" title="5.4.2.5 日志存储参数配置"></a>5.4.2.5 日志存储参数配置</h4><table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>log.segment.bytes</td>
<td>Kafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分成块的大小，默认值 1G。</td>
</tr>
<tr>
<td>log.index.interval.bytes</td>
<td>默认 4kb，kafka 里面每当写入了 4kb 大小的日志（.log），然后就往 index 文件里面记录一个索引。 稀疏索引。</td>
</tr>
</tbody></table>
<h3 id="5-4-2-文件清理策略"><a href="#5-4-2-文件清理策略" class="headerlink" title="5.4.2 文件清理策略"></a>5.4.2 文件清理策略</h3><h4 id="5-4-2-1-文件清理时间"><a href="#5-4-2-1-文件清理时间" class="headerlink" title="5.4.2.1 文件清理时间"></a>5.4.2.1 文件清理时间</h4><p>Kafka 中默认的日志保存时间为 7 天，可以通过调整如下参数修改保存时间。</p>
<ul>
<li>log.retention.hours：小时，默认 7天。</li>
<li>log.retention.minutes：分钟。</li>
<li>log.retention.ms：毫秒。</li>
<li>log.retention.check.interval.ms：负责设置检查周期，默认 5 分钟。</li>
</ul>
<p>优先级：<code>log.retention.ms</code>&gt;<code>log.retention.minutes</code>&gt;<code>log.retention.hours</code></p>
<p>那么日志一旦超过了设置的时间，怎么处理呢？</p>
<p>Kafka 中提供的日志清理策略有 delete 和 compact 两种。</p>
<h4 id="5-4-2-2-delete日志清除策略"><a href="#5-4-2-2-delete日志清除策略" class="headerlink" title="5.4.2.2 delete日志清除策略"></a>5.4.2.2 delete日志清除策略</h4><p><code>log.cleanup.policy = delete</code>： 所有数据启用删除策略</p>
<p>（1）基于时间：默认打开。以 segment 中所有记录中的最大时间戳作为该文件时间戳。<br>（2）基于大小：默认关闭。超过设置的所有日志总大小，删除最早的 segment。（在生产环境中是不打开的）<br><code>log.retention.bytes</code>，默认等于-1，表示无穷大。</p>
<p>思考：如果一个 segment中有一部分数据过期，一部分没有过期，怎么处理？</p>
<p>答案：最后一条消息的时间戳过期了才会被删除。</p>
<h4 id="5-4-2-3-compact日志压缩策略"><a href="#5-4-2-3-compact日志压缩策略" class="headerlink" title="5.4.2.3 compact日志压缩策略"></a>5.4.2.3 compact日志压缩策略</h4><p>compact日志压缩：对于相同key的不同value值，只保留最后一个版本。</p>
<p><code>log.cleanup.policy = compact</code>：所有数据启用压缩策略</p>
<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-11.png" srcset="/blog/img/loading.gif"></p>
<p>压缩后的offset可能是不连续的，比如上图中没有6，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，实际上会拿到offset为7的消息，并从这个位置开始消费。</p>
<p>这种策略只适合特殊场景，比如消息的key是用户ID，value是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。</p>
<p>使用时常出现什么问题：</p>
<ol>
<li><p>在刚完成集群部署之后，测试生产者和消费者是否能正常工作。在输入生产者命令的时候，将topic主题名称多输入了“–”，在消费者里输入的是正确的topic的名称。结果是导致生产者无论输入什么消息，消费者都接收不到。</p>
<p>解决办法：后来查看执行生产者消费的命令的时候才知道多输入了一些符号，后面再测试的时候修改会正确的topic的名称。之后我就写了一个脚本用来对生产者和消费者的测试，避免了输入错误的问题</p>
</li>
</ol>
<h2 id="5-5-高效读写数据"><a href="#5-5-高效读写数据" class="headerlink" title="5.5 高效读写数据"></a>5.5 高效读写数据</h2><h3 id="5-5-1-kafka如何实现高效读写数据？"><a href="#5-5-1-kafka如何实现高效读写数据？" class="headerlink" title="5.5.1 kafka如何实现高效读写数据？"></a>5.5.1 kafka如何实现高效读写数据？</h3><ol>
<li>Kafka 本身是分布式集群，可以采用分区技术，并 本身是分布式集群，可以采用分区技术，并行度高 度高</li>
<li>读数据采用稀疏索引，读数据采用稀疏索引，可以快速定位要消费的数据 快速定位要消费的数据</li>
<li>顺序写磁盘</li>
</ol>
<blockquote>
<p>Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。 官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。</p>
</blockquote>
<ol start="4">
<li> 页缓存 + 零拷贝技术</li>
</ol>
<h3 id="5-5-2-页缓存-零拷贝技术简介"><a href="#5-5-2-页缓存-零拷贝技术简介" class="headerlink" title="5.5.2 页缓存 + 零拷贝技术简介"></a>5.5.2 页缓存 + 零拷贝技术简介</h3><p>零拷贝：Kafka的数据加工处理操作交由Kafka生产者和Kafka消费者处理。Kafka Broker应用层不关心存储的数据，所以就不用走应用层，传输效率高。</p>
<p>PageCache 页缓存：Kafka重度依赖底层操作系统提供的PageCache功能。当上层有写操作时，操作系统只是将数据写入PageCache。当读操作发生时，先从PageCache中查找，如果找不到，再去磁盘中读取。实际上PageCache是把尽可能多的空闲内存都当做了磁盘缓存来使用。</p>
<p>参数说明</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>log.flush.interval.messages</td>
<td>强制页缓存刷写到磁盘的条数，默认是 long 的最大值，9223372036854775807。一般不建议修改，交给系统自己管理。</td>
</tr>
<tr>
<td>log.flush.interval.ms</td>
<td>每隔多久，刷数据到磁盘，默认是 null。一般不建议修改，交给系统自己管理。</td>
</tr>
</tbody></table>
<h1 id="六、kafka消费者"><a href="#六、kafka消费者" class="headerlink" title="六、kafka消费者"></a>六、kafka消费者</h1><h2 id="6-1-kafka消费方式"><a href="#6-1-kafka消费方式" class="headerlink" title="6.1 kafka消费方式"></a>6.1 kafka消费方式</h2><ul>
<li>pull（拉）模式：consumer采用从broker中主动拉取数据。Kafka 采用这种方式。（类似自助餐里的想吃多少就拿多少）</li>
<li>push（推）模式：Kafka没有采用这种方式，因为由broker<br>决定消息发送速率，很难适应所有消费者的消费速率。例如推送的速度是50m/s，Consumer1、Consumer2消费的数据小于50m/s就来不及处理消息。</li>
</ul>
<blockquote>
<p>pull模式不足之处是，如果Kafka没有数据，消费者可能会陷入循环中，一直返回空数据。</p>
</blockquote>
<h2 id="6-2-Kafka-消费者"><a href="#6-2-Kafka-消费者" class="headerlink" title="6.2 Kafka 消费者"></a>6.2 Kafka 消费者</h2><h3 id="6-2-1-消费者总体工作流程"><a href="#6-2-1-消费者总体工作流程" class="headerlink" title="6.2.1 消费者总体工作流程"></a>6.2.1 消费者总体工作流程</h3><p>略</p>
<h3 id="6-2-2-消费者组原理"><a href="#6-2-2-消费者组原理" class="headerlink" title="6.2.2 消费者组原理"></a>6.2.2 消费者组原理</h3><p>略</p>
<h3 id="6-2-3-消费着重要参数"><a href="#6-2-3-消费着重要参数" class="headerlink" title="6.2.3 消费着重要参数"></a>6.2.3 消费着重要参数</h3><h2 id="6-3-消费者API"><a href="#6-3-消费者API" class="headerlink" title="6.3 消费者API"></a>6.3 消费者API</h2><h3 id="6-3-1-独立消费者案例（订阅主题）"><a href="#6-3-1-独立消费者案例（订阅主题）" class="headerlink" title="6.3.1 独立消费者案例（订阅主题）"></a>6.3.1 独立消费者案例（订阅主题）</h3><p>略</p>
<h3 id="6-3-2-独立消费者案例"><a href="#6-3-2-独立消费者案例" class="headerlink" title="6.3.2 独立消费者案例"></a>6.3.2 独立消费者案例</h3><p>略</p>
<h3 id="6-3-3-消费者组案例"><a href="#6-3-3-消费者组案例" class="headerlink" title="6.3.3 消费者组案例"></a>6.3.3 消费者组案例</h3><p>略</p>
<h2 id="6-4-生产经验——分区的分配以及再平衡"><a href="#6-4-生产经验——分区的分配以及再平衡" class="headerlink" title="6.4 生产经验——分区的分配以及再平衡"></a>6.4 生产经验——分区的分配以及再平衡</h2><ol>
<li>一个consumer group中有多个consumer组成，一个 topic有多个partition组成，现在的问题是，到底由哪个consumer来消费哪个<br>partition的数据。</li>
<li>Kafka有四种主流的分区分配策略： Range、RoundRobin、Sticky、CooperativeSticky。可以通过配置参数partition.assignment.strategy，修改分区的分配策略。默认策略是Range + CooperativeSticky。Kafka可以同时使用多个分区分配策略。</li>
</ol>
<p><strong>消费者组的初始化流程：</strong></p>
<ol>
<li>每个consumer都发送JoinGroup请求</li>
<li>选出一个consumer作为leader</li>
<li>把要消费的topic情况发送给leader 消费者</li>
<li>leader会负责制定消费方案</li>
<li>把消费方案发给coordinator</li>
<li>Coordinator就把消费方案下发给各个consumer</li>
<li>每个消费者都会和coordinator保持心跳（默认3s），一旦超时<br>（session.timeout.ms=45s），该消费者会被移除，并触发再平衡；或者消费者处理消息的过长（max.poll.interval.ms5分钟），也会触发再平衡</li>
</ol>
<p>参数说明：</p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>heartbeat.interval.ms</td>
<td>Kafka 消费者和 coordinator 之间的心跳时间，默认 3s。该条目的值必须小于 session.timeout.ms，也不应该高于session.timeout.ms 的 1/3。</td>
</tr>
<tr>
<td>session.timeout.ms</td>
<td>Kafka 消费者和 coordinator 之间连接超时时间，默认 45s。超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td>max.poll.interval.ms</td>
<td>消费者处理消息的最大时长，默认是 5 分钟。超过该值，该消费者被移除，消费者组执行再平衡。</td>
</tr>
<tr>
<td>partition.assignment.strategy</td>
<td>消 费 者 分 区 分 配 策 略 ， 默 认 策 略 是 Range +CooperativeSticky。Kafka 可以同时使用多个分区分配策略。可 以 选 择 的 策 略 包 括 ： Range 、 RoundRobin 、 Sticky 、CooperativeSticky</td>
</tr>
</tbody></table>
<h3 id="6-4-1-Range以及再平衡"><a href="#6-4-1-Range以及再平衡" class="headerlink" title="6.4.1 Range以及再平衡"></a>6.4.1 Range以及再平衡</h3><h4 id="6-4-1-1-Range分区策略原理"><a href="#6-4-1-1-Range分区策略原理" class="headerlink" title="6.4.1.1 Range分区策略原理"></a>6.4.1.1 Range分区策略原理</h4><p>Range 是对每个 topic 而言的。</p>
<ol>
<li>首先对同一个 topic 里面的分区按照序号进行排序，并<br>对消费者按照字母顺序进行排序。</li>
<li>假如现在有 7 个分区，3 个消费者，排序后的分区将会<br>是0,1,2,3,4,5,6；消费者排序完之后将会是C0,C1,C2。</li>
<li>通过 partitions数/consumer数 来决定每个消费者应该<br>消费几个分区。如果除不尽，那么前面几个消费者将会多<br>消费 1 个分区。</li>
<li>例如，7/3 = 2 余 1 ，除不尽，那么 消费者 C0 便会多<br>消费 1 个分区。 8/3=2余2，除不尽，那么C0和C1分别多<br>消费一个。</li>
</ol>
<p>注意：如果只是针对 1 个 topic 而言，C0消费者多消费1<br>个分区影响不是很大。但是如果有N 多个 topic，那么针对每<br>个 topic，消费者 C0都将多消费 1 个分区，topic越多，C0消<br>费的分区会比其他消费者明显多消费 N 个分区。</p>
<p>容易产生数据倾斜！</p>
<h4 id="6-4-1-2-Range分区分配策略案例"><a href="#6-4-1-2-Range分区分配策略案例" class="headerlink" title="6.4.1.2 Range分区分配策略案例"></a>6.4.1.2 Range分区分配策略案例</h4><p>（1）修改主题first为7个分区</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs she"><br></code></pre></td></tr></table></figure>

<p>注意：分区数可以增加，但是不能减少。</p>
<p>（2）</p>
<h4 id="6-4-1-3-Range分区再平衡案例"><a href="#6-4-1-3-Range分区再平衡案例" class="headerlink" title="6.4.1.3 Range分区再平衡案例"></a>6.4.1.3 Range分区再平衡案例</h4><p>（1）停止掉 0 号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。</p>
<p>​        1 号消费者：消费到 3、4号分区数据。</p>
<p>​        2 号消费者：消费到 5、6号分区数据。</p>
<p>​        0 号消费者的任务会整体被分配到 1 号消费者或者 2 号消费者。</p>
<p>说明：0号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。</p>
<p>（2）再次重新发送消息观看结果（45s 以后）。</p>
<p>​        1 号消费者：消费到 0、1、2、3 号分区数据。</p>
<p>​        2 号消费者：消费到 4、5、6号分区数据。</p>
<p>说明：消费者 0 已经被踢出消费者组，所以重新按照 range 方式分配。</p>
<h3 id="6-4-2-RoundRobin-以及再平衡"><a href="#6-4-2-RoundRobin-以及再平衡" class="headerlink" title="6.4.2 RoundRobin 以及再平衡"></a>6.4.2 RoundRobin 以及再平衡</h3><h4 id="6-4-2-1-RoundRobin-分区策略原理"><a href="#6-4-2-1-RoundRobin-分区策略原理" class="headerlink" title="6.4.2.1 RoundRobin 分区策略原理"></a>6.4.2.1 RoundRobin 分区策略原理</h4><p>RoundRobin 针对集群中所有Topic而言。</p>
<p>RoundRobin 轮询分区策略，是把所有的 partition 和所有的consumer 都列出来，然后按照 hashcode 进行排序，最后通过轮询算法来分配 partition 给到各个消费者。</p>
<h4 id="6-4-2-2-RoundRobin-分区分配策略案例"><a href="#6-4-2-2-RoundRobin-分区分配策略案例" class="headerlink" title="6.4.2.2 RoundRobin 分区分配策略案例"></a>6.4.2.2 RoundRobin 分区分配策略案例</h4><p>略</p>
<h4 id="6-4-2-3-RoundRobin-分区分配再平衡案例"><a href="#6-4-2-3-RoundRobin-分区分配再平衡案例" class="headerlink" title="6.4.2.3 RoundRobin 分区分配再平衡案例"></a>6.4.2.3 RoundRobin 分区分配再平衡案例</h4><p>（1）停止掉 0 号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。</p>
<p>​        1 号消费者：消费到 2、5号分区数据</p>
<p>​        2 号消费者：消费到 4、1号分区数据</p>
<p>​        0 号消费者的任务会按照 RoundRobin 的方式，把数据轮询分成 0 、6 和 3 号分区数据，分别由 1号消费者或者 2号消费者消费。</p>
<p>说明：0号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。</p>
<p>（2）再次重新发送消息观看结果（45s 以后）。</p>
<p>​        1 号消费者：消费到 0、2、4、6 号分区数据</p>
<p>​        2 号消费者：消费到 1、3、5号分区数据</p>
<p>说明：消费者 0 已经被踢出消费者组，所以重新按照 RoundRobin 方式分配。</p>
<h3 id="6-4-3-Sticky-以及再平衡"><a href="#6-4-3-Sticky-以及再平衡" class="headerlink" title="6.4.3 Sticky 以及再平衡"></a>6.4.3 Sticky 以及再平衡</h3><p>粘性分区定义：可以理解为分配的结果带有“粘性的”。即在执行一次新的分配之前，考虑上一次分配的结果，尽量少的调整分配的变动，可以节省大量的开销。</p>
<p>粘性分区是 Kafka 从 0.11.x 版本开始引入这种分配策略，首先会尽量均衡的放置分区到消费者上面，在出现同一消费者组内消费者出现问题的时候，会尽量保持原有分配的分区不变化。</p>
<h4 id="6-4-3-1-需求"><a href="#6-4-3-1-需求" class="headerlink" title="6.4.3.1 需求"></a>6.4.3.1 需求</h4><p>设置主题为 first，7个分区；准备 3 个消费者，采用粘性分区策略，并进行消费，观察消费分配情况。然后再停止其中一个消费者，再次观察消费分配情况。</p>
<h4 id="6-4-3-2-步骤"><a href="#6-4-3-2-步骤" class="headerlink" title="6.4.3.2 步骤"></a>6.4.3.2 步骤</h4><p>略</p>
<h4 id="6-4-3-3-Sticky-分区分配再平衡案例"><a href="#6-4-3-3-Sticky-分区分配再平衡案例" class="headerlink" title="6.4.3.3 Sticky 分区分配再平衡案例"></a>6.4.3.3 Sticky 分区分配再平衡案例</h4><p>（1）停止掉 0 号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。</p>
<p>​        1 号消费者：消费到 2、5、3号分区数据。</p>
<p>​        2 号消费者：消费到 4、6号分区数据。</p>
<p>​        0 号消费者的任务会按照粘性规则，尽可能均衡的随机分成 0 和 1 号分区数据，分别由 1号消费者或者 2号消费者消费。</p>
<p>说明：0号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。</p>
<p>（2）再次重新发送消息观看结果（45s 以后）。</p>
<p>​        1 号消费者：消费到 2、3、5号分区数据。</p>
<p>​        2 号消费者：消费到 0、1、4、6 号分区数据。</p>
<p>说明：消费者 0 已经被踢出消费者组，所以重新按照粘性方式分配。</p>
<h2 id="6-5-offset-位移"><a href="#6-5-offset-位移" class="headerlink" title="6.5 offset 位移"></a>6.5 offset 位移</h2><h3 id="6-5-1-offset-的默认维护位置"><a href="#6-5-1-offset-的默认维护位置" class="headerlink" title="6.5.1 offset 的默认维护位置"></a>6.5.1 offset 的默认维护位置</h3><p>__consumer_offsets 主题里面采用 key 和 value 的方式存储数据。key 是 group.id+topic+分区号，value 就是当前 offset 的值。每隔一段时间，kafka 内部会对这个 topic 进行compact，也就是每个 group.id+topic+分区号就保留最新数据。</p>
<p>从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets</p>
<p>Kafka0.9版本之前，consumer默认将offset保存在Zookeeper中</p>
<p>面试题：</p>
<blockquote>
<p>消费者offset维护再哪个位置？</p>
<p>再kafka0.9版本之前，消费的offset维护再zookeeper当中。</p>
<p>kafia0.9版本之后维护在系统主机之中。</p>
</blockquote>
<h4 id="6-5-1-1-消费offset案例"><a href="#6-5-1-1-消费offset案例" class="headerlink" title="6.5.1.1 消费offset案例"></a>6.5.1.1 消费offset案例</h4><p>__consumer_offsets 为 Kafka 中的 topic，那就可以通过消费者进行消费。</p>
<p>（1）修改kakfa配置</p>
<p>在配置文件 config/consumer.properties 中添加配置 <code>exclude.internal.topics=false</code>，默认是 true，表示不能消费系统主题。为了查看该系统主题数据，所以该参数修改为 false。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# cd /apps/kafka/config/<br>[root@kafka-node1 config]# vim consumer.properties<br>exclude.internal.topics=false<br><br>[root@kafka-node2 ~]# cd /apps/kafka/config/<br>[root@kafka-node1 config]# vim consumer.properties<br>exclude.internal.topics=false<br><br>[root@kafka-node3 ~]# cd /apps/kafka/config/<br>[root@kafka-node1 config]# vim consumer.properties<br>exclude.internal.topics=false<br></code></pre></td></tr></table></figure>

<p>注意：修改完成后不用重启。</p>
<p>（2）创建一个新的topic，名叫“test2”</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --create --topic test2 --partitions 2 --replication-factor 2<br>Created topic test2.<br></code></pre></td></tr></table></figure>

<p>（3）启动生产者往“test2“生产数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-console-producer.sh --topic test2 --bootstrap-server kafka-node1:9092<br></code></pre></td></tr></table></figure>

<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-12.png" srcset="/blog/img/loading.gif"></p>
<p>（4）启动消费者消费“test2“数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node2 ~]# /apps/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka-node1:9092 --topic test2 --group test<br></code></pre></td></tr></table></figure>

<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-13.png" srcset="/blog/img/loading.gif"></p>
<p>注意：指定消费者组名称，更好观察数据存储位置（key 是 group.id+topic+分区号）。</p>
<p>（5）查看消费者消费主题__consumer_offsets。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node3 config]# /apps/kafka/bin/kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server kafka-node1:9092 --consumer.config /apps/kafka/config/consumer.properties --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot; --from-beginning | grep test2<br></code></pre></td></tr></table></figure>

<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-14.png" srcset="/blog/img/loading.gif"></p>
<h3 id="6-5-2-自动提交-offset"><a href="#6-5-2-自动提交-offset" class="headerlink" title="6.5.2 自动提交 offset"></a>6.5.2 自动提交 offset</h3><p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。<br>5s<br>自动提交offset的相关参数：</p>
<ul>
<li>enable.auto.commit ：是否开启自动提交offset功能，默认是true</li>
<li>auto.commit.interval.ms ：自动提交offset的时间间隔，默认是5s</li>
</ul>
<p>参数说明：</p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>enable.auto.commit</td>
<td>默认值为 true，消费者会自动周期性地向服务器提交偏移量。</td>
</tr>
<tr>
<td>auto.commit.interval.ms</td>
<td>如果设置了 enable.auto.commit 的值为 true， 则该值定义了消费者偏移量向 Kafka 提交的频率，默认 5s。</td>
</tr>
</tbody></table>
<h4 id="6-5-2-1-消费者自动提交offset"><a href="#6-5-2-1-消费者自动提交offset" class="headerlink" title="6.5.2.1 消费者自动提交offset"></a>6.5.2.1 消费者自动提交offset</h4><p>略</p>
<h3 id="6-5-3-手动提交offset"><a href="#6-5-3-手动提交offset" class="headerlink" title="6.5.3 手动提交offset"></a>6.5.3 手动提交offset</h3><p>虽然自动提交offset十分简单便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因此Kafka还提供了手动提交offset的API。</p>
<p>手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将本次提交的一批数据最高的偏移量提交；不同点是，同步提交阻塞当前线程，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而异步提交则没有失败重试机制，故有可能提交失败。</p>
<p>手动提交参数说明：</p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>commitSync（同步提交）</td>
<td>必须等待offset提交完毕，再去消费下一批数据。</td>
</tr>
<tr>
<td>commitAsync（异步提交）</td>
<td>发送完提交offset请求后，就开始消费下一批数据了。</td>
</tr>
</tbody></table>
<p>手动提交过程：</p>
<ol>
<li>消费者组不断拉取数据</li>
<li>consummer手动提交offset</li>
</ol>
<h4 id="6-5-3-1-同步提交offset"><a href="#6-5-3-1-同步提交offset" class="headerlink" title="6.5.3.1 同步提交offset"></a>6.5.3.1 同步提交offset</h4><p>由于同步提交 offset 有失败重试机制，故更加可靠，但是由于一直等待提交结果，提交的效率比较低。</p>
<h4 id="6-5-3-2-异步提交offset"><a href="#6-5-3-2-异步提交offset" class="headerlink" title="6.5.3.2 异步提交offset"></a>6.5.3.2 异步提交offset</h4><p>虽然同步提交 offset 更可靠一些，但是由于其会阻塞当前线程，直到提交成功。因此吞吐量会受到很大的影响。因此更多的情况下，会选用异步提交 offset的方式。</p>
<h3 id="6-5-4-指定offset消费"><a href="#6-5-4-指定offset消费" class="headerlink" title="6.5.4 指定offset消费"></a>6.5.4 指定offset消费</h3><p>auto.offset.reset = earliest | latest | none 默认是 latest。</p>
<p>当 Kafka 中没有初始偏移量（消费者组第一次消费）或服务器上不再存在当前偏移量时（例如该数据已被删除），该怎么办？</p>
<p>（1）earliest：自动将偏移量重置为最早的偏移量，<code>--from-eginning</code>。<br>（2）latest（默认值）：自动将偏移量重置为最新偏移量。</p>
<p>（3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常。</p>
<p>（4）任意指定 offset 位移开始消费</p>
<h3 id="6-5-5-指定时间消费"><a href="#6-5-5-指定时间消费" class="headerlink" title="6.5.5 指定时间消费"></a>6.5.5 指定时间消费</h3><p>略</p>
<h3 id="6-5-6-漏消费和重复消费"><a href="#6-5-6-漏消费和重复消费" class="headerlink" title="6.5.6 漏消费和重复消费"></a>6.5.6 漏消费和重复消费</h3><p>重复消费：已经消费了数据，但是 offset没提交。</p>
<p>漏消费：先提交 offset后消费，有可能会造成数据的漏消费。</p>
<h4 id="6-5-6-1-重复消费"><a href="#6-5-6-1-重复消费" class="headerlink" title="6.5.6.1 重复消费"></a>6.5.6.1 重复消费</h4><p>重复消费是自动提交offset引起的。</p>
<p>引起重复消费过程是：</p>
<ol>
<li>consummer每5s提交offset</li>
<li>如果提交offset后的2s，consumer挂了</li>
<li>再次启动consummer，则从上一次提交的offset处继续消费，导致重复消费</li>
</ol>
<h4 id="6-5-6-2-漏消费"><a href="#6-5-6-2-漏消费" class="headerlink" title="6.5.6.2 漏消费"></a>6.5.6.2 漏消费</h4><p>设置offset为手动提交，当offset被提交时，数据还在内存中未落盘，此时刚好消费者线程被kill掉，那么offset已经提交，但是数据未处理，导致这部分内存中的数据丢失。</p>
<p>引起漏消费过程是：</p>
<ol>
<li>提交offset</li>
<li>消费者消费的数据还在内存中，消费者挂掉，导致漏消费</li>
</ol>
<p>思考：怎么能做到既不漏消费也不重复消费呢？详看消费者事务。</p>
<h2 id="6-6-生产经验——消费者事务"><a href="#6-6-生产经验——消费者事务" class="headerlink" title="6.6 生产经验——消费者事务"></a>6.6 生产经验——消费者事务</h2><p>如果想完成Consumer端的精准一次性消费，那么需要Kafka消费端将消费过程和提交offset过程做原子绑定。此时我们需要将Kafka的offset保存到支持事务的自定义介质（比如MySQL）。这部分知识会在后续项目部分涉及。</p>
<h2 id="6-7-生产经验——数据积压（消费者如何提高吞吐量）"><a href="#6-7-生产经验——数据积压（消费者如何提高吞吐量）" class="headerlink" title="6.7 生产经验——数据积压（消费者如何提高吞吐量）"></a>6.7 生产经验——数据积压（消费者如何提高吞吐量）</h2><p>消费者如何提高吞吐量？</p>
<ol>
<li><p>如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数 = 分区数。（两者缺一不可）</p>
</li>
<li><p>如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间 &lt; 生产速度），使处理的数据小于生产的数据，也会造成数据积压。</p>
</li>
</ol>
<p>参数说明：</p>
<table>
<thead>
<tr>
<th>参数名称</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>fetch.max.bytes</td>
<td>默认 Default: 52428800（50 m）。消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值（50m）仍然可以拉取回来这批数据，因此，这不是一个绝对最大值。一批次的大小受 message.max.bytes （brokerconfig）or max.message.bytes （topic config）影响。</td>
</tr>
<tr>
<td>max.poll.records</td>
<td>一次 poll拉取数据返回消息的最大条数，默认是 500 条</td>
</tr>
</tbody></table>
<h1 id="七、kafka-eagle监控"><a href="#七、kafka-eagle监控" class="headerlink" title="七、kafka-eagle监控"></a>七、kafka-eagle监控</h1><p>Kafka-Eagle框架可以监控 Kafka 集群的整体运行情况，在生产环境中经常使用。</p>
<p>现在kafka-eagle改名为kafka-efak了</p>
<p>下面就来部署kafka-eagle。</p>
<h2 id="7-1-准备MySQL环境"><a href="#7-1-准备MySQL环境" class="headerlink" title="7.1 准备MySQL环境"></a>7.1 准备MySQL环境</h2><p>Kafka-Eagle 的安装依赖于 MySQL，MySQL 主要用来存储可视化展示的数据。</p>
<h3 id="7-1-1-卸载MySQL"><a href="#7-1-1-卸载MySQL" class="headerlink" title="7.1.1 卸载MySQL"></a>7.1.1 卸载MySQL</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# rpm -qa | grep -i mysql &amp;&amp; rpm -qa | grep -i mariadb<br></code></pre></td></tr></table></figure>

<h3 id="7-1-2-安装MySQL仓库"><a href="#7-1-2-安装MySQL仓库" class="headerlink" title="7.1.2 安装MySQL仓库"></a>7.1.2 安装MySQL仓库</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# rpm -ih --force --nodeps https://dev.mysql.com/get/mysql80-community-release-el7-7.noarch.rpm<br>warning: /var/tmp/rpm-tmp.DoCwlB: Header V4 RSA/SHA256 Signature, key ID 3a79bd29: NOKEY<br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">################################ [100%]</span></span><br>Updating / installing...<br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">################################ [100%]</span></span><br></code></pre></td></tr></table></figure>

<h3 id="7-1-3-修改MySQL仓库配置"><a href="#7-1-3-修改MySQL仓库配置" class="headerlink" title="7.1.3 修改MySQL仓库配置"></a>7.1.3 修改MySQL仓库配置</h3><p>修改MySQL仓库配置，将安装MySQL8改为安装MySQL5.7</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# vim /etc/yum.repos.d/mysql-community.repo<br><span class="hljs-meta">#</span><span class="bash"> Enable to use MySQL 5.7</span><br>[mysql57-community]<br>name=MySQL 5.7 Community Server<br>baseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/7/$basearch<br>enabled=1 #修改为1<br>gpgcheck=1<br>gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql-2022<br>       file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql<br><br>[mysql80-community]<br>name=MySQL 8.0 Community Server<br>baseurl=http://repo.mysql.com/yum/mysql-8.0-community/el/7/$basearch<br>enabled=0 #修改为0<br>gpgcheck=1<br>gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql-2022<br>       file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql<br></code></pre></td></tr></table></figure>

<h3 id="7-1-4-安装仓库的gpgkey"><a href="#7-1-4-安装仓库的gpgkey" class="headerlink" title="7.1.4 安装仓库的gpgkey"></a>7.1.4 安装仓库的gpgkey</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# rpm --import http://repo.mysql.com/RPM-GPG-KEY-mysql-2022<br>[root@kafka-node1 ~]# rpm --import http://repo.mysql.com/RPM-GPG-KEY-mysql<br></code></pre></td></tr></table></figure>

<h3 id="7-1-5-安装MySQL"><a href="#7-1-5-安装MySQL" class="headerlink" title="7.1.5 安装MySQL"></a>7.1.5 安装MySQL</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# yum install -y mysql-community-server<br></code></pre></td></tr></table></figure>

<h3 id="7-1-6-启动MySQL并设置开机启动"><a href="#7-1-6-启动MySQL并设置开机启动" class="headerlink" title="7.1.6 启动MySQL并设置开机启动"></a>7.1.6 启动MySQL并设置开机启动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# systemctl start mysqld<br>[root@kafka-node1 ~]# systemctl enable mysqld<br></code></pre></td></tr></table></figure>

<h3 id="7-1-7-查看MySQL密码"><a href="#7-1-7-查看MySQL密码" class="headerlink" title="7.1.7 查看MySQL密码"></a>7.1.7 查看MySQL密码</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# cat /var/log/mysqld.log | grep &quot;A temporary password&quot;<br>2023-02-24T07:12:54.955433Z 1 [Note] A temporary password is generated for root@localhost: XHhNw(h2!Ck)<br></code></pre></td></tr></table></figure>

<h3 id="7-1-8-登录MySQL"><a href="#7-1-8-登录MySQL" class="headerlink" title="7.1.8 登录MySQL"></a>7.1.8 登录MySQL</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# mysql -uroot -p<br>Enter password: <br>Welcome to the MySQL monitor.  Commands end with ; or \g.<br>Your MySQL connection id is 2<br>Server version: 5.7.41<br><br>Copyright (c) 2000, 2023, Oracle and/or its affiliates.<br><br>Oracle is a registered trademark of Oracle Corporation and/or its<br>affiliates. Other names may be trademarks of their respective<br>owners.<br><br>Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.<br></code></pre></td></tr></table></figure>

<h3 id="7-1-9-修改密码"><a href="#7-1-9-修改密码" class="headerlink" title="7.1.9 修改密码"></a>7.1.9 修改密码</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs mysql">mysql&gt; set password=password(&#x27;Admin123456@&#x27;);<br>Query OK, 0 rows affected, 1 warning (0.01 sec)<br></code></pre></td></tr></table></figure>

<h3 id="7-1-10-修改root用户的host"><a href="#7-1-10-修改root用户的host" class="headerlink" title="7.1.10 修改root用户的host"></a>7.1.10 修改root用户的host</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs mysql">mysql&gt; UPDATE mysql.user SET HOST=&#x27;%&#x27; WHERE USER=&#x27;root&#x27;;<br>Query OK, 1 row affected (0.00 sec)<br>Rows matched: 1  Changed: 1  Warnings: 0<br><br>mysql&gt; FLUSH PRIVILEGES;<br>Query OK, 0 rows affected (0.02 sec)<br><br>mysql&gt; select user,host from mysql.user;<br>+---------------+-----------+<br>| user          | host      |<br>+---------------+-----------+<br>| root          | %         |<br>| mysql.session | localhost |<br>| mysql.sys     | localhost |<br>+---------------+-----------+<br>3 rows in set (0.00 sec)<br></code></pre></td></tr></table></figure>

<h2 id="7-2-kafka环境准备"><a href="#7-2-kafka环境准备" class="headerlink" title="7.2 kafka环境准备"></a>7.2 kafka环境准备</h2><h3 id="7-2-1-关闭kafka集群"><a href="#7-2-1-关闭kafka集群" class="headerlink" title="7.2.1 关闭kafka集群"></a>7.2.1 关闭kafka集群</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# ./kafka_ctl.sh stop<br>--- 关闭 kafka-node1 kafka ---<br>--- 关闭 kafka-node2 kafka ---<br>--- 关闭 kafka-node3 kafka ---<br></code></pre></td></tr></table></figure>

<h3 id="7-2-2-修改kafka启动命令的参数"><a href="#7-2-2-修改kafka启动命令的参数" class="headerlink" title="7.2.2 修改kafka启动命令的参数"></a>7.2.2 修改kafka启动命令的参数</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# vim /apps/kafka/bin/kafka-server-start.sh<br><span class="hljs-meta">#</span><span class="bash">将如下参数</span><br>if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then<br>    export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;<br>fi<br><span class="hljs-meta">#</span><span class="bash">改为</span><br>if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then<br>    export KAFKA_HEAP_OPTS=&quot;-server -Xms2G -Xmx2G -XX:PermSize=128m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:ParallelGCThreads=8 -XX:ConcGCThreads=5 -XX:InitiatingHeapOccupancyPercent=70&quot;<br>    export JMX_PORT=&quot;9999&quot;<br>    #export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;<br>fi<br></code></pre></td></tr></table></figure>

<p>注意：修改之后在启动 Kafka 之前要分发之其他节点</p>
<h2 id="7-3-kafka-eagle安装"><a href="#7-3-kafka-eagle安装" class="headerlink" title="7.3 kafka-eagle安装"></a>7.3 kafka-eagle安装</h2><p>kafka-eagle官方网站：</p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs http">https://www.kafka-eagle.org/<br></code></pre></td></tr></table></figure>

<h3 id="7-3-1-下载kafka-eagle"><a href="#7-3-1-下载kafka-eagle" class="headerlink" title="7.3.1 下载kafka-eagle"></a>7.3.1 下载kafka-eagle</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# mkdir -p /apps<br>[root@kafka-node1 ~]# wget https://github.com/smartloli/kafka-eagle-bin/archive/v2.0.8.tar.gz<br></code></pre></td></tr></table></figure>

<h3 id="7-3-2-解压kafka-eagle并创建软连接"><a href="#7-3-2-解压kafka-eagle并创建软连接" class="headerlink" title="7.3.2 解压kafka-eagle并创建软连接"></a>7.3.2 解压kafka-eagle并创建软连接</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# tar zxf v2.0.8.tar.gz<br>[root@kafka-node1 ~]# cd kafka-eagle-bin-2.0.8/<br>[root@kafka-node1 kafka-eagle-bin-2.0.8]# tar xf efak-web-2.0.8-bin.tar.gz -C /apps/<br>[root@kafka-node1 kafka-eagle-bin-2.0.8]# ln -s /apps/efak-web-2.0.8 /apps/kafka-efak<br></code></pre></td></tr></table></figure>

<h3 id="7-3-3-修改kafka-eagle配置文件"><a href="#7-3-3-修改kafka-eagle配置文件" class="headerlink" title="7.3.3 修改kafka-eagle配置文件"></a>7.3.3 修改kafka-eagle配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 kafka-eagle-bin-2.0.8]# cd /apps/kafka-efak/conf/<br>[root@kafka-node1 conf]# vim system-config.properties<br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> multi zookeeper &amp; kafka cluster list</span><br><span class="hljs-meta">#</span><span class="bash"> Settings prefixed with <span class="hljs-string">&#x27;kafka.eagle.&#x27;</span> will be deprecated, use <span class="hljs-string">&#x27;efak.&#x27;</span> instead</span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>efak.zk.cluster.alias=cluster1,cluster2<br>cluster1.zk.list=kafka-node1:2181,kafka-node2:2181,kafka-node3:2181/kafka #修改为kafka节点信息<br><span class="hljs-meta">#</span><span class="bash">cluster2.zk.list=xdn10:2181,xdn11:2181,xdn12:2181 <span class="hljs-comment">#注释这里</span></span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> zookeeper <span class="hljs-built_in">enable</span> acl</span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>cluster1.zk.acl.enable=false<br>cluster1.zk.acl.schema=digest<br>cluster1.zk.acl.username=test<br>cluster1.zk.acl.password=test123<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> broker size online list</span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>cluster1.efak.broker.size=20<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> zk client thread <span class="hljs-built_in">limit</span></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>kafka.zk.limit.size=32<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> EFAK webui port</span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>efak.webui.port=8048<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> kafka jmx acl and ssl authenticate</span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>cluster1.efak.jmx.acl=false<br>cluster1.efak.jmx.user=keadmin<br>cluster1.efak.jmx.password=keadmin123<br>cluster1.efak.jmx.ssl=false<br>cluster1.efak.jmx.truststore.location=/data/ssl/certificates/kafka.truststore<br>cluster1.efak.jmx.truststore.password=ke123456<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> kafka offset storage</span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>cluster1.efak.offset.storage=kafka<br><span class="hljs-meta">#</span><span class="bash">cluster2.efak.offset.storage=zk <span class="hljs-comment">#注释这里</span></span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> kafka jmx uri</span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>cluster1.efak.jmx.uri=service:jmx:rmi:///jndi/rmi://%s/jmxrmi<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> kafka metrics, 15 days by default</span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>efak.metrics.charts=true<br>efak.metrics.retain=15<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> kafka sql topic records max</span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>efak.sql.topic.records.max=5000<br>efak.sql.topic.preview.records.max=10<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> delete kafka topic token</span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>efak.topic.token=keadmin<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> kafka sasl authenticate</span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>cluster1.efak.sasl.enable=false<br>cluster1.efak.sasl.protocol=SASL_PLAINTEXT<br>cluster1.efak.sasl.mechanism=SCRAM-SHA-256<br>cluster1.efak.sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username=&quot;kafka&quot; password=&quot;kafka-eagle&quot;;<br>cluster1.efak.sasl.client.id=<br>cluster1.efak.blacklist.topics=<br>cluster1.efak.sasl.cgroup.enable=false<br>cluster1.efak.sasl.cgroup.topics=<br>cluster2.efak.sasl.enable=false<br>cluster2.efak.sasl.protocol=SASL_PLAINTEXT<br>cluster2.efak.sasl.mechanism=PLAIN<br>cluster2.efak.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=&quot;kafka&quot; password=&quot;kafka-eagle&quot;;<br>cluster2.efak.sasl.client.id=<br>cluster2.efak.blacklist.topics=<br>cluster2.efak.sasl.cgroup.enable=false<br>cluster2.efak.sasl.cgroup.topics=<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> kafka ssl authenticate</span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>cluster3.efak.ssl.enable=false<br>cluster3.efak.ssl.protocol=SSL<br>cluster3.efak.ssl.truststore.location=<br>cluster3.efak.ssl.truststore.password=<br>cluster3.efak.ssl.keystore.location=<br>cluster3.efak.ssl.keystore.password=<br>cluster3.efak.ssl.key.password=<br>cluster3.efak.ssl.endpoint.identification.algorithm=https<br>cluster3.efak.blacklist.topics=<br>cluster3.efak.ssl.cgroup.enable=false<br>cluster3.efak.ssl.cgroup.topics=<br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> kafka sqlite jdbc driver address			<span class="hljs-comment">#这部分的内容注释</span></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash">efak.driver=org.sqlite.JDBC</span><br><span class="hljs-meta">#</span><span class="bash">efak.url=jdbc:sqlite:/hadoop/kafka-eagle/db/ke.db</span><br><span class="hljs-meta">#</span><span class="bash">efak.username=root</span><br><span class="hljs-meta">#</span><span class="bash">efak.password=www.kafka-eagle.org</span><br><span class="hljs-meta"></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br><span class="hljs-meta">#</span><span class="bash"> kafka mysql jdbc driver address		<span class="hljs-comment">#这部分的内容接触注释并按需要修改</span></span><br><span class="hljs-meta">#</span><span class="bash"><span class="hljs-comment">#####################################</span></span><br>efak.driver=com.mysql.cj.jdbc.Driver<br>efak.url=jdbc:mysql://127.0.0.1:3306/ke?useUnicode=true&amp;characterEncoding=UTF-8&amp;zeroDateTimeBehavior=convertToNull #注意这里的地址<br>efak.username=root<br>efak.password=Admin123456@<br></code></pre></td></tr></table></figure>

<h3 id="7-3-4-添加环境变量"><a href="#7-3-4-添加环境变量" class="headerlink" title="7.3.4 添加环境变量"></a>7.3.4 添加环境变量</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 conf]# vim /etc/profile.d/eagle.sh<br>export KE_HOME=/apps/kafka-efak<br>export PATH=$PATH:$KE_HOME/bin<br>[root@kafka-node1 conf]# source /etc/profile.d/eagle.sh<br></code></pre></td></tr></table></figure>

<h3 id="7-3-5-启动kafka-eagle"><a href="#7-3-5-启动kafka-eagle" class="headerlink" title="7.3.5 启动kafka-eagle"></a>7.3.5 启动kafka-eagle</h3><p>注意：启动kafka-eagle前先启动zookeeper和kafka</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">之前并没有关闭zookeeper，所以这里只启动kafka</span><br>[root@kafka-node1 ~]# ./kafka_ctl.sh start<br>[root@kafka-node1 ~]# ke.sh start<br>[2023-02-24 15:46:55] INFO: Starting  EFAK( Eagle For Apache Kafka ) environment check ...<br>......<br>[2023-02-24 15:46:59] INFO: Port Progress: [##################################################] | 100%<br>[2023-02-24 15:47:02] INFO: Config Progress: [##################################################] | 100%<br>[2023-02-24 15:47:05] INFO: Startup Progress: [##################################################] | 100%<br>[2023-02-24 15:46:55] INFO: Status Code[0]<br>[2023-02-24 15:46:55] INFO: [Job done!]<br>Welcome to<br>    ______    ______    ___     __ __<br>   / ____/   / ____/   /   |   / //_/<br>  / __/     / /_      / /| |  / ,&lt;   <br> / /___    / __/     / ___ | / /| |  <br>/_____/   /_/       /_/  |_|/_/ |_|  <br>( Eagle For Apache Kafka® )<br><br>Version 2.0.8 -- Copyright 2016-2021<br>*******************************************************************<br>* EFAK Service has started success.<br>* Welcome, Now you can visit &#x27;http://10.0.0.7:8048&#x27;<br>* Account:admin ,Password:123456<br>*******************************************************************<br>* &lt;Usage&gt; ke.sh [start|status|stop|restart|stats] &lt;/Usage&gt;<br>* &lt;Usage&gt; https://www.kafka-eagle.org/ &lt;/Usage&gt;<br>*******************************************************************<br></code></pre></td></tr></table></figure>

<p>说明：停止kafka-eagle，执行以下命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# ke.sh stop<br></code></pre></td></tr></table></figure>

<h2 id="7-4-kafka-eagle页面操作"><a href="#7-4-kafka-eagle页面操作" class="headerlink" title="7.4 kafka-eagle页面操作"></a>7.4 kafka-eagle页面操作</h2><p>登录页面查看监控数据。</p>
<p>浏览器访问：（访问地址在启动后的信息有显示给我们了）</p>
<figure class="highlight http"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs http">http://10.0.0.7:8048<br></code></pre></td></tr></table></figure>

<p>登录kafka-eagle</p>
<p>account：admin</p>
<p>passowrd：123456</p>
<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-15.png" srcset="/blog/img/loading.gif"></p>
<p>Dashboard页面：</p>
<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-16.png" srcset="/blog/img/loading.gif"></p>
<p>BScreen页面</p>
<p>点击左侧导航栏的BScreen就可以进入</p>
<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-18.png" srcset="/blog/img/loading.gif"></p>
<p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-17.png" srcset="/blog/img/loading.gif"></p>
<h1 id="八、kafka-kraft模式"><a href="#八、kafka-kraft模式" class="headerlink" title="八、kafka-kraft模式"></a>八、kafka-kraft模式</h1><h2 id="8-1-kafka-kraft模式简介"><a href="#8-1-kafka-kraft模式简介" class="headerlink" title="8.1 kafka-kraft模式简介"></a>8.1 kafka-kraft模式简介</h2><p><img src="C:\Users\kinci\Desktop\blog-picture\kafka\kafka-19.png" srcset="/blog/img/loading.gif"></p>
<p>左图为 Kafka 现有架构，元数据在 zookeeper 中，运行时动态选举 controller，由controller进行 Kafka 集群管理。右图为 kraft模式架构（实验性），不再依赖 zookeeper集群，而是用三台 controller节点代替 zookeeper，元数据保存在 controller中，由 controller 直接进行 Kafka集群管理。</p>
<p>这样做的好处有以下几个：</p>
<ul>
<li>Kafka 不再依赖外部框架，而是能够独立运行；</li>
<li>controller管理集群时，不再需要从 zookeeper中先读取数据，集群性能上升；</li>
<li>由于不依赖 zookeeper，集群扩展时不再受到 zookeeper读写能力限制；</li>
<li>controller 不再动态选举，而是由配置文件规定。这样我们可以有针对性的加强controller 节点的配置，而不是像以前一样对随机 controller 节点的高负载束手无策。</li>
</ul>
<h2 id="8-2-kafka-kraft集群部署"><a href="#8-2-kafka-kraft集群部署" class="headerlink" title="8.2 kafka-kraft集群部署"></a>8.2 kafka-kraft集群部署</h2><p>本次实验需要部署jdk1.8环境和kafka3.0，详细的规划如下：</p>
<table>
<thead>
<tr>
<th>服务器地址</th>
<th>服务器系统</th>
<th>hostname</th>
<th>安装的软件</th>
</tr>
</thead>
<tbody><tr>
<td>10.0.0.7</td>
<td>CentOS7.9</td>
<td>kafka-node1</td>
<td>jdk、kafka</td>
</tr>
<tr>
<td>10.0.0.17</td>
<td>CentOS7.9</td>
<td>kafka-node2</td>
<td>jdk、kafka</td>
</tr>
<tr>
<td>10.0.0.27</td>
<td>CetnOS7.9</td>
<td>kafka-node3</td>
<td>jdk、kafka</td>
</tr>
</tbody></table>
<p>查看服务器地址，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@CentOS7 ~]# ifconfig eth0 | grep inet | awk &#x27;NR==1&#123;print $2&#125;&#x27;<br>10.0.0.7<br></code></pre></td></tr></table></figure>

<p>查看服务器系统，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@CentOS7 ~]# cat /etc/redhat-release <br>CentOS Linux release 7.9.2009 (Core)<br></code></pre></td></tr></table></figure>

<p>修改hostname，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@CentOS7 ~]# hostnamectl set-hostname kafka-node1 #执行命令后退出终端重新连接终端<br></code></pre></td></tr></table></figure>

<p>查看hostname，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# hostname<br>kafka-node1<br></code></pre></td></tr></table></figure>

<p>所有节点都可以根据规划修改hostname，服务器地址。</p>
<h3 id="8-2-1-安装前准备"><a href="#8-2-1-安装前准备" class="headerlink" title="8.2.1 安装前准备"></a>8.2.1 安装前准备</h3><h4 id="8-2-1-1-kafka-node1生成公钥私钥"><a href="#8-2-1-1-kafka-node1生成公钥私钥" class="headerlink" title="8.2.1.1 kafka-node1生成公钥私钥"></a>8.2.1.1 kafka-node1生成公钥私钥</h4><p>kafka-node1节点生成公钥私钥，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# ssh-keygen  #执行命令后根据继续多次的按回车键<br>Generating public/private rsa key pair.<br>Enter file in which to save the key (/root/.ssh/id_rsa): <br>Created directory &#x27;/root/.ssh&#x27;.<br>Enter passphrase (empty for no passphrase): <br>Enter same passphrase again: <br>Your identification has been saved in /root/.ssh/id_rsa.<br>Your public key has been saved in /root/.ssh/id_rsa.pub.<br>The key fingerprint is:<br>SHA256:OBcruzxd9b5xQ8Osty/bK/WF4UwTjD8NwvDhDhNQvJM root@kafka-node1<br>The key&#x27;s randomart image is:<br>+---[RSA 2048]----+<br>|        .++o.o   |<br>|          .++.+  |<br>|        . oooo + |<br>|       . oE+. O .|<br>|      + S  o.= X |<br>|       =  .   B.o|<br>|      .. .   ooo+|<br>|     ....    .+++|<br>|      o.      +*=|<br>+----[SHA256]-----+<br></code></pre></td></tr></table></figure>

<h4 id="8-2-1-2-将公钥发送到其他节点"><a href="#8-2-1-2-将公钥发送到其他节点" class="headerlink" title="8.2.1.2 将公钥发送到其他节点"></a>8.2.1.2 将公钥发送到其他节点</h4><p>为了之后方便控制其他节点，将kafka-node1的公钥发送到其他节点（包括发送给kafka-node1节点自己）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# ssh-copy-id kafka-node1<br>[root@kafka-node1 ~]# ssh-copy-id kafka-node2<br>[root@kafka-node1 ~]# ssh-copy-id kafka-node3<br></code></pre></td></tr></table></figure>

<h4 id="8-2-1-3-kafka-node1修改hosts文件"><a href="#8-2-1-3-kafka-node1修改hosts文件" class="headerlink" title="8.2.1.3 kafka-node1修改hosts文件"></a>8.2.1.3 kafka-node1修改hosts文件</h4><p>修改<code>/etc/hosts</code>问津，修改的内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# vim /etc/hosts<br>......<br><span class="hljs-meta">#</span><span class="bash">添加以下内容</span><br>10.0.0.7 kafka-node1<br>10.0.0.17 kafka-node2<br>10.0.0.27 kafka-node3<br></code></pre></td></tr></table></figure>

<h4 id="8-2-1-4-将hosts文件发送到其他节点"><a href="#8-2-1-4-将hosts文件发送到其他节点" class="headerlink" title="8.2.1.4 将hosts文件发送到其他节点"></a>8.2.1.4 将hosts文件发送到其他节点</h4><p>将host文件发送到其他节点，并进行检查，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">发送hosts文件命令</span><br>[root@kafka-node1 ~]# scp /etc/hosts kafka-node2:/etc/hosts<br>[root@kafka-node1 ~]# scp /etc/hosts kafka-node3:/etc/hosts<br><span class="hljs-meta">#</span><span class="bash">检查hosts文件命令</span><br>[root@kafka-node1 ~]# ssh kafka-node2 &quot;cat /etc/hosts&quot;<br>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4<br>::1         localhost localhost.localdomain localhost6 localhost6.localdomain6<br>10.0.0.7 kafka-node1<br>10.0.0.17 kafka-node2<br>10.0.0.27 kafka-node3<br>[root@kafka-node1 ~]# ssh kafka-node3 &quot;cat /etc/hosts&quot;<br>127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4<br>::1         localhost localhost.localdomain localhost6 localhost6.localdomain6<br>10.0.0.7 kafka-node1<br>10.0.0.17 kafka-node2<br>10.0.0.27 kafka-node3<br></code></pre></td></tr></table></figure>

<h3 id="8-2-2-部署jdk环境"><a href="#8-2-2-部署jdk环境" class="headerlink" title="8.2.2 部署jdk环境"></a>8.2.2 部署jdk环境</h3><h4 id="8-2-2-1-kafka-node1下载jdk安装包"><a href="#8-2-2-1-kafka-node1下载jdk安装包" class="headerlink" title="8.2.2.1 kafka-node1下载jdk安装包"></a>8.2.2.1 kafka-node1下载jdk安装包</h4><p>安装jdk环境，命令执行的内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">创建安装目录</span><br>[root@kafka-node1 ~]# mkdir -p /apps<br><span class="hljs-meta">#</span><span class="bash">进入安装目录</span><br>[root@kafka-node1 ~]# cd /apps<br><span class="hljs-meta">#</span><span class="bash">下载jdk1.8，下载过程略。jdk安装包可以到oracle官网上去下载。</span><br></code></pre></td></tr></table></figure>

<h4 id="8-2-2-2-kafka-node1部署jdk安装包"><a href="#8-2-2-2-kafka-node1部署jdk安装包" class="headerlink" title="8.2.2.2 kafka-node1部署jdk安装包"></a>8.2.2.2 kafka-node1部署jdk安装包</h4><p>解压jdk安装包，并做软连接。执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">解压安装包</span><br>[root@kafka-node1 apps]# tar zxvf jdk-8u212-linux-x64.tar.gz<br><span class="hljs-meta">#</span><span class="bash">做软连接</span><br>[root@kafka-node1 apps]# ln -s /apps/jdk1.8.0_212 /apps/jdk<br></code></pre></td></tr></table></figure>

<h4 id="8-2-2-3-kafka-node1添加jdk环境变量"><a href="#8-2-2-3-kafka-node1添加jdk环境变量" class="headerlink" title="8.2.2.3 kafka-node1添加jdk环境变量"></a>8.2.2.3 kafka-node1添加jdk环境变量</h4><p>添加的jdk环境变量的内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 apps]# vim /etc/profile.d/jdk.sh<br>export JAVA_HOME=/apps/jdk #jdk在哪里解压或做软连接到哪里的，路径就改到相应的地方的/bin目录下即可。<br>export TOMCAT_HOME=/apps/tomcat<br>export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$TOMCAT_HOME/bin:$PATH<br>export CLASSPATH=.$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib:$JAVA_HOME/lib/tools.jar<br></code></pre></td></tr></table></figure>

<h4 id="8-2-2-4-kafka-node1将部署完成的jdk文件发送到其他节点"><a href="#8-2-2-4-kafka-node1将部署完成的jdk文件发送到其他节点" class="headerlink" title="8.2.2.4 kafka-node1将部署完成的jdk文件发送到其他节点"></a>8.2.2.4 kafka-node1将部署完成的jdk文件发送到其他节点</h4><p>kafka-node1节点已经部署好jdk，现在只需要将jdk文件和环境变量发送到其他节点即可，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">发送jdk文件</span><br>[root@kafka-node1 apps]# rsync -av /apps/* kafka-node2:/apps<br>[root@kafka-node1 apps]# rsync -av /apps/* kafka-node3:/apps<br><br>[root@kafka-node1 ~]# source /etc/profile.d/jdk.sh#发送环境变量文件<br>[root@kafka-node1 apps]# scp /etc/profile.d/jdk.sh kafka-node2:/etc/profile.d/<br>[root@kafka-node1 apps]# scp /etc/profile.d/jdk.sh kafka-node3:/etc/profile.d/<br></code></pre></td></tr></table></figure>

<h4 id="8-2-2-5-让所有节点的jdk环境变量生效"><a href="#8-2-2-5-让所有节点的jdk环境变量生效" class="headerlink" title="8.2.2.5 让所有节点的jdk环境变量生效"></a>8.2.2.5 让所有节点的jdk环境变量生效</h4><p>每个节点都执行命令，让jdk的环境变量文件生效，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# source /etc/profile.d/jdk.sh<br>[root@kafka-node2 ~]# source /etc/profile.d/jdk.sh<br>[root@kafka-node3 ~]# source /etc/profile.d/jdk.sh<br></code></pre></td></tr></table></figure>

<h3 id="8-2-3-部署kafka集群"><a href="#8-2-3-部署kafka集群" class="headerlink" title="8.2.3 部署kafka集群"></a>8.2.3 部署kafka集群</h3><h4 id="8-2-3-1-kafka-node1节点下载kafka安装包"><a href="#8-2-3-1-kafka-node1节点下载kafka安装包" class="headerlink" title="8.2.3.1 kafka-node1节点下载kafka安装包"></a>8.2.3.1 kafka-node1节点下载kafka安装包</h4><p>进入安装目录，下载kafka安装包，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">进入安装目录</span><br>[root@kafka-node1 ~]# cd /apps<br><span class="hljs-meta">#</span><span class="bash">下载kafka安装包</span><br>[root@kafka-node1 apps]# wget https://archive.apache.org/dist/kafka/3.0.0/kafka_2.12-3.0.0.tgz<br></code></pre></td></tr></table></figure>

<h4 id="8-2-3-2-kafka-node1节点解压安装包"><a href="#8-2-3-2-kafka-node1节点解压安装包" class="headerlink" title="8.2.3.2 kafka-node1节点解压安装包"></a>8.2.3.2 kafka-node1节点解压安装包</h4><p>解压kafka安装包，并做软连接，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 apps]# tar xf kafka_2.12-3.0.0.tgz<br>[root@kafka-node1 apps]# ln -s /apps/kafka_2.12-3.0.0 /apps/kafka<br></code></pre></td></tr></table></figure>

<h4 id="8-2-3-3-kafka-node1节点修改配置文件"><a href="#8-2-3-3-kafka-node1节点修改配置文件" class="headerlink" title="8.2.3.3 kafka-node1节点修改配置文件"></a>8.2.3.3 kafka-node1节点修改配置文件</h4><p>修改kafka配置文件，主要修改“kafka角色”、“节点ID”，“controller列表”、“broker对外监听的地址”和“kafka数据存储目录”这几项，修改的内容可以参考下面：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 apps]# cd /apps/kafka/config/kraft/<br>[root@kafka-node1 kraft]# vim server.properties<br><span class="hljs-meta">#</span><span class="bash">kafka的角色（controller相当于主机、broker节点相当于从机，主机类似zookeeper功能），在controller足够的情况下增加kafka节点只需要增加broker即可。</span><br>process.roles=broker,controller #根据需求修改<br><span class="hljs-meta">#</span><span class="bash">节点ID，ID号码唯一。</span><br>node.id=1 #根据需求修改<br><span class="hljs-meta">#</span><span class="bash">Controller列表。</span><br><span class="hljs-meta">#</span><span class="bash">格式为：节点ID@kafka节点IP:kafka controller端口。</span><br><span class="hljs-meta">#</span><span class="bash">多个controller节点就在后面加逗号后继续填写节点即可。</span><br>controller.quorum.voters=1@kafka-node1:9093,2@kafka-node2:9093,3@kafka-node3:9093 #根据集群需求修改<br><span class="hljs-meta">#</span><span class="bash">不同服务器绑定的端口</span><br>listeners=PLAINTEXT://:9092,CONTROLLER://:9093<br><span class="hljs-meta">#</span><span class="bash">broker服务协议别名</span><br>inter.broker.listener.name=PLAINTEXT<br><span class="hljs-meta">#</span><span class="bash">broker对外监听的地址</span><br>advertised.listeners=PLAINTEXT://kafka-node1:9092<br><span class="hljs-meta">#</span><span class="bash">controller服务协议别名</span><br>controller.listener.names=CONTROLLER<br><span class="hljs-meta">#</span><span class="bash">协议别名到安全协议的映射</span><br>listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL<br><span class="hljs-meta">#</span><span class="bash">kafka 数据存储目录</span><br>log.dirs=/apps/kafka/data #根据需求修改<br></code></pre></td></tr></table></figure>

<h4 id="8-2-3-4-kafka-node1将修改过的配置文件分发到其他节点"><a href="#8-2-3-4-kafka-node1将修改过的配置文件分发到其他节点" class="headerlink" title="8.2.3.4 kafka-node1将修改过的配置文件分发到其他节点"></a>8.2.3.4 kafka-node1将修改过的配置文件分发到其他节点</h4><p>将修改过的配置文件发送给其他节点，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 kraft]# rsync -av /apps/* kafka-node2:/apps<br>[root@kafka-node1 kraft]# rsync -av /apps/* kafka-node3:/apps<br></code></pre></td></tr></table></figure>

<h4 id="8-2-3-5-其他节点修改kafka节点"><a href="#8-2-3-5-其他节点修改kafka节点" class="headerlink" title="8.2.3.5 其他节点修改kafka节点"></a>8.2.3.5 其他节点修改kafka节点</h4><p>kafka-node2修改配置文件，修改的内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node2 ~]# cd /apps/kafka/config/kraft/<br>[root@kafka-node2 kraft]# vim server.properties<br><span class="hljs-meta">#</span><span class="bash">主要修改以下内容：</span><br><span class="hljs-meta">#</span><span class="bash">节点ID</span><br>node.id=2<br><span class="hljs-meta">#</span><span class="bash">kafka对外监听的地址</span><br>advertised.listeners=PLAINTEXT://kafka-node2:9092<br></code></pre></td></tr></table></figure>

<p>kafka-node3节点修改配置文件，修改的内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node3 ~]# cd /apps/kafka/config/kraft/<br>[root@kafka-node3 kraft]# vim server.properties<br><span class="hljs-meta">#</span><span class="bash">主要修改以下内容：</span><br><span class="hljs-meta">#</span><span class="bash">节点ID</span><br>node.id=3<br><span class="hljs-meta">#</span><span class="bash">kafka对外监听的地址</span><br>advertised.listeners=PLAINTEXT://kafka-node3:9092<br></code></pre></td></tr></table></figure>

<h2 id="8-3-初始化集群数据目录"><a href="#8-3-初始化集群数据目录" class="headerlink" title="8.3 初始化集群数据目录"></a>8.3 初始化集群数据目录</h2><h3 id="8-3-1-生成存储目录唯一ID"><a href="#8-3-1-生成存储目录唯一ID" class="headerlink" title="8.3.1 生成存储目录唯一ID"></a>8.3.1 生成存储目录唯一ID</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# cd /apps/kafka/bin/<br>[root@kafka-node1 bin]# ./kafka-storage.sh random-uuid<br>_ZncMjnWTKyA8PzWYu3WqQ<br></code></pre></td></tr></table></figure>

<h3 id="8-3-2-三个节点都用该ID格式化存储目录"><a href="#8-3-2-三个节点都用该ID格式化存储目录" class="headerlink" title="8.3.2 三个节点都用该ID格式化存储目录"></a>8.3.2 三个节点都用该ID格式化存储目录</h3><p>kafka-node1格式化存储目录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 bin]# ./kafka-storage.sh format -t _ZncMjnWTKyA8PzWYu3WqQ -c /apps/kafka/config/kraft/server.properties <br>Formatting /apps/kafka/data<br></code></pre></td></tr></table></figure>

<p>kafka-node2格式化存储目录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node2 kraft]# cd /apps/kafka/bin/<br>[root@kafka-node2 bin]# ./kafka-storage.sh format -t _ZncMjnWTKyA8PzWYu3WqQ -c /apps/kafka/config/kraft/server.properties<br>Formatting /apps/kafka/data<br></code></pre></td></tr></table></figure>

<p>kafka-node3格式化存储目录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node3 kraft]# cd /apps/kafka/bin/<br>[root@kafka-node3 bin]# ./kafka-storage.sh format -t _ZncMjnWTKyA8PzWYu3WqQ -c /apps/kafka/config/kraft/server.properties<br>Formatting /apps/kafka/data<br></code></pre></td></tr></table></figure>

<h2 id="8-4-启动kafka集群"><a href="#8-4-启动kafka集群" class="headerlink" title="8.4 启动kafka集群"></a>8.4 启动kafka集群</h2><p>kafka-node1节点启动kafka。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 bin]# ./kafka-server-start.sh -daemon /apps/kafka/config/kraft/server.properties<br></code></pre></td></tr></table></figure>

<p>kafka-node2节点启动kafka。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node2 bin]# ./kafka-server-start.sh -daemon /apps/kafka/config/kraft/server.properties<br></code></pre></td></tr></table></figure>

<p>kafka-node3节点启动kafka。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node3 bin]# ./kafka-server-start.sh -daemon /apps/kafka/config/kraft/server.properties<br></code></pre></td></tr></table></figure>

<h2 id="8-5-各节点查看kafa启动情况"><a href="#8-5-各节点查看kafa启动情况" class="headerlink" title="8.5 各节点查看kafa启动情况"></a>8.5 各节点查看kafa启动情况</h2><p>kafka-node1节点查看jps：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 bin]# jps <br>4310 Jps<br>4222 Kafka<br></code></pre></td></tr></table></figure>

<p>kafka-node2节点查看jpd：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node2 bin]# jps<br>3256 Jps<br>3166 Kafka<br></code></pre></td></tr></table></figure>

<p>kafka-node3节点查看jps：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node3 bin]# jps<br>3112 Kafka<br>3199 Jps<br></code></pre></td></tr></table></figure>

<h2 id="8-6-测试kafka"><a href="#8-6-测试kafka" class="headerlink" title="8.6 测试kafka"></a>8.6 测试kafka</h2><h3 id="8-6-1-kafka集群创建topic"><a href="#8-6-1-kafka集群创建topic" class="headerlink" title="8.6.1 kafka集群创建topic"></a>8.6.1 kafka集群创建topic</h3><p>在kafka集群创建一个有两分区和两副本的topic，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --create --partitions 2 --replication-factor 2 --topic first<br>Created topic first.<br></code></pre></td></tr></table></figure>

<h3 id="8-6-2-kafka集群创建生产者并生产消息"><a href="#8-6-2-kafka集群创建生产者并生产消息" class="headerlink" title="8.6.2 kafka集群创建生产者并生产消息"></a>8.6.2 kafka集群创建生产者并生产消息</h3><p>kafka-node1节点创建生产者，并生产消息，执行的内容如下： </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# /apps/kafka/bin/kafka-console-producer.sh --bootstrap-server kafka-node1:9092 --topic first<br><span class="hljs-meta">&gt;</span><span class="bash">hello</span><br><span class="hljs-meta">&gt;</span><span class="bash">world</span><br><span class="hljs-meta">&gt;</span><span class="bash">hello,world</span><br><span class="hljs-meta">&gt;</span><span class="bash">world,hello</span><br><span class="hljs-meta">&gt;</span><span class="bash">world</span><br></code></pre></td></tr></table></figure>

<h3 id="8-6-3-kafka集群创建消费者并消费消息"><a href="#8-6-3-kafka集群创建消费者并消费消息" class="headerlink" title="8.6.3 kafka集群创建消费者并消费消息"></a>8.6.3 kafka集群创建消费者并消费消息</h3><p>kafka-node2节点创建消费者，并消费消息，执行的内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node2 bin]# /apps/kafka/bin/kafka-console-consumer.sh --bootstrap-server kafka-node1:9092 --topic first<br>hello<br>world<br>hello,world<br>world,hello<br>world<br></code></pre></td></tr></table></figure>

<p>注意：建议在测试的时候生产者和消费者先创建后，生产者在生产消息。</p>
<p>经过测试，kraft模式下的kafka集群创建完成。</p>
<h2 id="8-7-kafka创建kraft模式总结"><a href="#8-7-kafka创建kraft模式总结" class="headerlink" title="8.7 kafka创建kraft模式总结"></a>8.7 kafka创建kraft模式总结</h2><ol>
<li>不需要再安装zookeeper，只需要部署jdk环境即可。</li>
<li>在配置文件方面，使用的是<code>../config/kraft</code>里面的配置文件。</li>
<li>主要配置文件中的角色，kfka节点ID，controller列表，broker对外监听的地址和kafka数据存储目录这几项。</li>
<li>在启动kafka集群前有初始化数据目录的一系列操作。</li>
<li>初始化完成后就可以启动kafka集群，并进行测试。</li>
</ol>
<h1 id="九、kafka硬件配置选择"><a href="#九、kafka硬件配置选择" class="headerlink" title="九、kafka硬件配置选择"></a>九、kafka硬件配置选择</h1><h2 id="9-1-场景说明"><a href="#9-1-场景说明" class="headerlink" title="9.1 场景说明"></a>9.1 场景说明</h2><p>以下条件为假设，下面小节都会使用以下假设内容：</p>
<ul>
<li>100 万日活，每人每天 100 条日志，每天总共的日志条数是 100 万 * 100 条 = 1 亿条。</li>
<li>1 亿/24 小时/60 分/60 秒 = 1150 条/每秒钟。</li>
<li>每条日志大小：0.5k - 2k（取 1k）。</li>
<li>1150 条/每秒钟 * 1k ≈ 1m/s 。</li>
<li>高峰期每秒钟：1150 条 * 20 倍 = 23000 条。</li>
<li>高峰期每秒多少数据量：20MB/s。</li>
</ul>
<h2 id="9-2-kafka部署服务器台数选择"><a href="#9-2-kafka部署服务器台数选择" class="headerlink" title="9.2 kafka部署服务器台数选择"></a>9.2 kafka部署服务器台数选择</h2><p>服务器台数= 2 * （生产者峰值生产速率 * 副本 / 100） + 1</p>
<p>​                    = 2 * （20m/s * 2 / 100） + 1</p>
<p>​                    ≈ 3 台</p>
<p>注意：中间除不进的需要加1。</p>
<h2 id="9-3-磁盘选择"><a href="#9-3-磁盘选择" class="headerlink" title="9.3 磁盘选择"></a>9.3 磁盘选择</h2><p>kafka 底层主要是<font color=red>顺序写</font>，固态硬盘和机械硬盘的顺序写速度差不多。</p>
<p><font color=red>建议选择普通的机械硬盘。</font></p>
<p>每天总数据量：1 亿条 * 1k ≈ 100g</p>
<p>100g * 副本 2 * 保存时间 3 天 / 0.7 ≈ 1T</p>
<p>建议三台服务器硬盘总大小，大于等于 1T。</p>
<h3 id="9-4-内存选择"><a href="#9-4-内存选择" class="headerlink" title="9.4 内存选择"></a>9.4 内存选择</h3><p>kafka  内存 = 堆内存（kafka 内部配置） + 页缓存（服务器内存）</p>
<ul>
<li><p>Kafka 堆内存建议每个节点：10g ~ 15g</p>
</li>
<li><p>页缓存  ：（分区数 * 1g * 25%）/ 节点数。</p>
<blockquote>
<p>例如 10 个分区，页缓存大小=（10 * 1g * 25%）/ 3 ≈ 1g</p>
</blockquote>
</li>
</ul>
<p>所以：一台服务器 10g + 1g </p>
<p>注意：页缓存：页缓存是 Linux 系统服务器的内存。我们只需要保证 1 个 segment（1g）中25%的数据在内存中就好。</p>
<h3 id="9-4-1-修改kafka使用内存"><a href="#9-4-1-修改kafka使用内存" class="headerlink" title="9.4.1 修改kafka使用内存"></a>9.4.1 修改kafka使用内存</h3><p>将kafka使用内存修改为10G。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 ~]# cd /apps/kafka/bin/<br>[root@kafka-node1 bin]# vim kafka-server-start.sh<br>if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then<br>    export KAFKA_HEAP_OPTS=&quot;-Xmx10G -Xms10G&quot;<br>fi<br></code></pre></td></tr></table></figure>

<h3 id="9-4-2-查看kafka进程号"><a href="#9-4-2-查看kafka进程号" class="headerlink" title="9.4.2 查看kafka进程号"></a>9.4.2 查看kafka进程号</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 bin]# jps<br>5691 Kafka<br>7179 Jps<br></code></pre></td></tr></table></figure>

<h3 id="9-4-3-根据kafka进程号，查看kafka的GC情况"><a href="#9-4-3-根据kafka进程号，查看kafka的GC情况" class="headerlink" title="9.4.3 根据kafka进程号，查看kafka的GC情况"></a>9.4.3 根据kafka进程号，查看kafka的GC情况</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 bin]# jstat -gc 5691 1s 10<br> S0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT     GCT   <br> 0.0   4096.0  0.0   4096.0 51200.0  17408.0   993280.0   141824.0  43776.0 40318.9 5632.0 5134.2     16    0.323   0      0.000    0.323<br> 0.0   4096.0  0.0   4096.0 51200.0  17408.0   993280.0   141824.0  43776.0 40318.9 5632.0 5134.2     16    0.323   0      0.000    0.323<br> 0.0   4096.0  0.0   4096.0 51200.0  17408.0   993280.0   141824.0  43776.0 40318.9 5632.0 5134.2     16    0.323   0      0.000    0.323<br> 0.0   4096.0  0.0   4096.0 51200.0  17408.0   993280.0   141824.0  43776.0 40318.9 5632.0 5134.2     16    0.323   0      0.000    0.323<br> 0.0   4096.0  0.0   4096.0 51200.0  17408.0   993280.0   141824.0  43776.0 40318.9 5632.0 5134.2     16    0.323   0      0.000    0.323<br> 0.0   4096.0  0.0   4096.0 51200.0  17408.0   993280.0   141824.0  43776.0 40318.9 5632.0 5134.2     16    0.323   0      0.000    0.323<br> 0.0   4096.0  0.0   4096.0 51200.0  17408.0   993280.0   141824.0  43776.0 40318.9 5632.0 5134.2     16    0.323   0      0.000    0.323<br> 0.0   4096.0  0.0   4096.0 51200.0  17408.0   993280.0   141824.0  43776.0 40318.9 5632.0 5134.2     16    0.323   0      0.000    0.323<br> 0.0   4096.0  0.0   4096.0 51200.0  17408.0   993280.0   141824.0  43776.0 40318.9 5632.0 5134.2     16    0.323   0      0.000    0.323<br> 0.0   4096.0  0.0   4096.0 51200.0  17408.0   993280.0   141824.0  43776.0 40318.9 5632.0 5134.2     16    0.323   0      0.000    0.323<br></code></pre></td></tr></table></figure>

<p>参数说明：</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>参数说明</th>
</tr>
</thead>
<tbody><tr>
<td>S0C</td>
<td>第一个幸存区的大小</td>
</tr>
<tr>
<td>S1C</td>
<td>第二个幸存区的大小</td>
</tr>
<tr>
<td>S0U</td>
<td>第一个幸存区的使用大小</td>
</tr>
<tr>
<td>EC</td>
<td>伊甸园区的大小</td>
</tr>
<tr>
<td>EU</td>
<td>伊甸园区的使用大小</td>
</tr>
<tr>
<td>OC</td>
<td>老年代大小</td>
</tr>
<tr>
<td>OU</td>
<td>老年代使用大小</td>
</tr>
<tr>
<td>MC</td>
<td>方法区大小</td>
</tr>
<tr>
<td>MU</td>
<td>方法区使用大小</td>
</tr>
<tr>
<td>CCSC</td>
<td>压缩类空间大小</td>
</tr>
<tr>
<td>CCSU</td>
<td>压缩类空间使用大小</td>
</tr>
<tr>
<td>YGC</td>
<td>年轻代垃圾回收次数（主要关注点）</td>
</tr>
<tr>
<td>YGCT</td>
<td>年轻代垃圾回收消耗时间</td>
</tr>
<tr>
<td>FGC</td>
<td>老年代垃圾回收次数</td>
</tr>
<tr>
<td>FGCT</td>
<td>老年代垃圾回收消耗时间</td>
</tr>
<tr>
<td>GCT</td>
<td>垃圾回收消耗总时间</td>
</tr>
</tbody></table>
<h3 id="9-4-4-根据kafka进程号，查看kafka的堆内存"><a href="#9-4-4-根据kafka进程号，查看kafka的堆内存" class="headerlink" title="9.4.4 根据kafka进程号，查看kafka的堆内存"></a>9.4.4 根据kafka进程号，查看kafka的堆内存</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-node1 bin]# jmap -heap 5691<br>Attaching to process ID 5691, please wait...<br>Debugger attached successfully.<br>Server compiler detected.<br>JVM version is 25.212-b10<br><br>using thread-local object allocation.<br>Garbage-First (G1) GC with 2 thread(s)<br><br>Heap Configuration:<br>   MinHeapFreeRatio         = 40<br>   MaxHeapFreeRatio         = 70<br>   MaxHeapSize              = 1073741824 (1024.0MB)<br>   NewSize                  = 1363144 (1.2999954223632812MB)<br>   MaxNewSize               = 643825664 (614.0MB)<br>   OldSize                  = 5452592 (5.1999969482421875MB)<br>   NewRatio                 = 2<br>   SurvivorRatio            = 8<br>   MetaspaceSize            = 21807104 (20.796875MB)<br>   CompressedClassSpaceSize = 1073741824 (1024.0MB)<br>   MaxMetaspaceSize         = 17592186044415 MB<br>   G1HeapRegionSize         = 1048576 (1.0MB)<br><br>Heap Usage:<br>G1 Heap:<br>   regions  = 1024<br>   capacity = 1073741824 (1024.0MB)<br>   used     = 196608008 (187.50000762939453MB)<br>   free     = 877133816 (836.4999923706055MB)<br>   18.31054762005806% used #关注点<br>G1 Young Generation:<br>Eden Space:<br>   regions  = 45<br>   capacity = 52428800 (50.0MB)<br>   used     = 47185920 (45.0MB)<br>   free     = 5242880 (5.0MB)<br>   90.0% used<br>Survivor Space:<br>   regions  = 4<br>   capacity = 4194304 (4.0MB)<br>   used     = 4194304 (4.0MB)<br>   free     = 0 (0.0MB)<br>   100.0% used<br>G1 Old Generation:<br>   regions  = 140<br>   capacity = 1017118720 (970.0MB)<br>   used     = 145227784 (138.50000762939453MB)<br>   free     = 871890936 (831.4999923706055MB)<br>   14.278351301999436% used #关注点<br><br>10223 interned Strings occupying 1168168 bytes.<br></code></pre></td></tr></table></figure>

<h2 id="9-4-CPU选择"><a href="#9-4-CPU选择" class="headerlink" title="9.4 CPU选择"></a>9.4 CPU选择</h2><p>假设有32cpu core 将32cpu分成两份（按3:1分），一份是24cpu core（用于kafka）另一份是8cpu core（用于其他）</p>
<p>num.io.threads（建议占cpu core（kafka用的cpu core）的50%） = 12</p>
<p>num.replica.fetchers（建议占剩下的cpu core的50%的1/3） = 4</p>
<p>num.network.threads（建议占cpu core的50%的2/3） = 8</p>
<p>建议用32cpu core</p>
<h2 id="9-5-网络选择"><a href="#9-5-网络选择" class="headerlink" title="9.5 网络选择"></a>9.5 网络选择</h2><p>网络带宽 = 峰值吞吐量 ≈ 20MB/s 选择千兆网卡即可。</p>
<p>100Mbps 单位是 bit；10M/s 单位是 byte ; 1byte = 8bit，100Mbps/8 = 12.5M/s。</p>
<p>一般百兆的网卡（100Mbps ）、千兆的网卡（1000Mbps）、万兆的网卡（10000Mbps）。</p>
<h2 id="9-6-kafka硬件配置总结"><a href="#9-6-kafka硬件配置总结" class="headerlink" title="9.6 kafka硬件配置总结"></a>9.6 kafka硬件配置总结</h2><ol>
<li>选择kafka配置前需要和开发确定每日消息的数量，消息大小，消息保存时间（包括高峰期的）</li>
<li>根据开发给的信息配置kafka的节点数量，每台节点的配置等等</li>
</ol>
<h1 id="十、kafka总体调优"><a href="#十、kafka总体调优" class="headerlink" title="十、kafka总体调优"></a>十、kafka总体调优</h1><p>注意：kafka在调整配置文件的参数时有些参数在官网上看到的是“read-only”、“per-broker”和“cluster-wide”，这三种种类。以下是中文解释：</p>
<blockquote>
<p>read-only：只有节点重新启动才能读取修改的参数</p>
<p>per-broker：针对单个节点修改就会生效</p>
<p>cluster-wide：针对整个集群节点，全部修改后就会生效</p>
</blockquote>
<h2 id="10-1-如何提升吞吐量"><a href="#10-1-如何提升吞吐量" class="headerlink" title="10.1 如何提升吞吐量"></a>10.1 如何提升吞吐量</h2><h3 id="10-1-1-提升生产吞吐量"><a href="#10-1-1-提升生产吞吐量" class="headerlink" title="10.1.1 提升生产吞吐量"></a>10.1.1 提升生产吞吐量</h3><ul>
<li>buffer.memory：发送消息的缓冲区大小，默认值是 32m，可以增加到 64m。</li>
<li>batch.size：默认是 16k。如果 batch 设置太小，会导致频繁网络请求，吞吐量下降；如果 batch 太大，会导致一条消息需要等待很久才能被发送出去，增加网络延时。</li>
<li>linger.ms，这个值默认是 0，意思就是消息必须立即被发送。一般设置一个 5-100毫秒。如果 linger.ms 设置的太小，会导致频繁网络请求，吞吐量下降；如果 linger.ms 太长，会导致一条消息需要等待很久才能被发送出去，增加网络延时。</li>
<li>compression.type：默认是 none，不压缩，但是也可以使用 lz4 压缩，效率还是不错的，压缩之后可以减小数据量，提升吞吐量，但是会加大 producer 端的 CPU 开销。</li>
</ul>
<h3 id="10-1-2-增加分区"><a href="#10-1-2-增加分区" class="headerlink" title="10.1.2 增加分区"></a>10.1.2 增加分区</h3><h3 id="10-1-3-消费者提高吞吐量"><a href="#10-1-3-消费者提高吞吐量" class="headerlink" title="10.1.3 消费者提高吞吐量"></a>10.1.3 消费者提高吞吐量</h3><ul>
<li>调整 fetch.max.bytes 大小，默认是 50m。</li>
<li>调整 max.poll.records 大小，默认是 500 条。</li>
</ul>
<h3 id="10-1-4-增加下游消费者处理能力"><a href="#10-1-4-增加下游消费者处理能力" class="headerlink" title="10.1.4 增加下游消费者处理能力"></a>10.1.4 增加下游消费者处理能力</h3><h2 id="10-2-数据精准一次"><a href="#10-2-数据精准一次" class="headerlink" title="10.2 数据精准一次"></a>10.2 数据精准一次</h2><h3 id="10-2-1-生产者角度"><a href="#10-2-1-生产者角度" class="headerlink" title="10.2.1 生产者角度"></a>10.2.1 生产者角度</h3><ul>
<li>acks 设置为 -1（acks=-1）</li>
<li>幂等性（enable.idempotence = true） + 事务</li>
</ul>
<h3 id="10-2-2-broker-服务端"><a href="#10-2-2-broker-服务端" class="headerlink" title="10.2.2 broker 服务端"></a>10.2.2 broker 服务端</h3><ul>
<li>分区副本大于等于 2 （–replication-factor 2）。</li>
<li>ISR 里应答的最小副本数量大于等于 2 （min.insync.replicas = 2）。</li>
</ul>
<h3 id="10-2-3-消费者"><a href="#10-2-3-消费者" class="headerlink" title="10.2.3 消费者"></a>10.2.3 消费者</h3><ul>
<li>事务 + 手动提交 offset （enable.auto.commit = false）。</li>
<li>消费者输出的目的地必须支持事务（MySQL、Kafka）。</li>
</ul>
<h2 id="10-3-合理设置分区数"><a href="#10-3-合理设置分区数" class="headerlink" title="10.3 合理设置分区数"></a>10.3 合理设置分区数</h2><ol>
<li>创建一个只有 1 个分区的 topic。</li>
<li>测试这个 topic 的 producer 吞吐量和 consumer 吞吐量。</li>
<li>假设他们的值分别是 Tp 和 Tc，单位可以是 MB/s。</li>
<li>然后假设总的目标吞吐量是 Tt，那么分区数 = Tt / min（Tp，Tc）。</li>
</ol>
<p>例如：producer 吞吐量 = 20m/s；consumer 吞吐量 = 50m/s，期望吞吐量 100m/s；</p>
<p>分区数 = 100 / 20 = 5 分区</p>
<p>分区数一般设置为：3-10 个</p>
<p>分区数不是越多越好，也不是越少越好，需要搭建完集群，进行压测，再灵活调整分区个数。</p>
<h2 id="10-4-单条日志大于1m"><a href="#10-4-单条日志大于1m" class="headerlink" title="10.4 单条日志大于1m"></a>10.4 单条日志大于1m</h2><ul>
<li>“message.max.bytes”，根据消息大小设置该参数的大小</li>
<li>“max.request.size”，根据消息大小设置该参数的大小</li>
</ul>
<h2 id="10-5-kafka服务器挂了"><a href="#10-5-kafka服务器挂了" class="headerlink" title="10.5 kafka服务器挂了"></a>10.5 kafka服务器挂了</h2><p>在生产环境中，如果某个 Kafka 节点挂掉。</p>
<p>正常处理办法：</p>
<p>（1）先尝试重新启动一下，如果能启动正常，那直接解决。</p>
<p>（2）如果重启不行，先查看kafka日志，根据日志解决问题。</p>
<p>（3）考虑增加内存、增加 CPU、网络带宽。</p>
<p>（4）如果将 kafka 整个节点误删除，如果副本数大于等于 2，可以按照服役新节点的方式重新服役一个新节点，并执行负载均衡。</p>
<h1 id="十一-kafka压力测试"><a href="#十一-kafka压力测试" class="headerlink" title="十一 kafka压力测试"></a>十一 kafka压力测试</h1><p>使用kafka自带脚本，对kafka集群进行压测。以下测试只是用作举例，没有多少可行性。生产环境需要在生产服务器上进行压力测试。</p>
<ul>
<li>生产者压测脚本是：kafka-producer-perf-test.sh</li>
<li>消费者压测脚本是：kafka-consumer-perf-test.sh</li>
</ul>
<p>在进行压测之前，为了不对kafka集群造成影响，我们用另外用一台新服务器安装kafka。然后通过该服务器连接上kafka集群来进行测试。</p>
<p>新服务器规划如下：</p>
<table>
<thead>
<tr>
<th>服务器地址</th>
<th>服务器系统</th>
<th>hostname</th>
<th>安装的软件</th>
</tr>
</thead>
<tbody><tr>
<td>10.0.0.37</td>
<td>CentOS7.9</td>
<td>kafka-test</td>
<td>jdk、kafka</td>
</tr>
</tbody></table>
<p>新服务器部署kafka过程就不多说了，不会部署kafka的可以看之前的部署教程。</p>
<h2 id="11-1-kafka-生产者压力测试"><a href="#11-1-kafka-生产者压力测试" class="headerlink" title="11.1 kafka 生产者压力测试"></a>11.1 kafka 生产者压力测试</h2><h3 id="11-1-1-生产者压力测试准备"><a href="#11-1-1-生产者压力测试准备" class="headerlink" title="11.1.1 生产者压力测试准备"></a>11.1.1 生产者压力测试准备</h3><p>创建一个叫“test”的topic，设置为3个分区，3个副本，执行的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# /apps/kafka/bin/kafka-topics.sh --bootstrap-server kafka-node1:9092 --create --replication-factor 3 --partitions 3 --topic test<br>Created topic test.<br></code></pre></td></tr></table></figure>

<h3 id="11-1-2-生产者进行压力测试"><a href="#11-1-2-生产者进行压力测试" class="headerlink" title="11.1.2 生产者进行压力测试"></a>11.1.2 生产者进行压力测试</h3><h4 id="11-1-2-1-第一次压力测试"><a href="#11-1-2-1-第一次压力测试" class="headerlink" title="11.1.2.1 第一次压力测试"></a>11.1.2.1 第一次压力测试</h4><p>条件：batch.size=16384 linger.ms=0</p>
<p>测试的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# /apps/kafka/bin/kafka-producer-perf-test.sh --topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=kafka-node1:9092,kafka-node2:9092,kafka-node3:9092 batch.size=16384 linger.ms=0<br></code></pre></td></tr></table></figure>

<p>参数说明：</p>
<p><code>record-size</code>：是一条信息有多大，单位是字节，本次测试设置为 1k。</p>
<p><code>num-records</code>：是总共发送多少条信息，本次测试设置为 100 万条。</p>
<p><code>throughput</code>：是每秒多少条信息，设成-1，表示不限流，尽可能快的生产数据，可测出生产者最大吞吐量。本次实验设置为每秒钟 1 万条。</p>
<p><code>producer-props</code>：后面可以配置生产者相关参数，batch.size 配置为 16k。</p>
<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">36376 records sent, 7275.2 records/sec (7.10 MB/sec), 1073.8 ms avg latency, 1526.0 ms max latency.<br>58455 records sent, 11691.0 records/sec (11.42 MB/sec), 1000.3 ms avg latency, 1580.0 ms max latency.<br>54201 records sent, 10820.7 records/sec (10.57 MB/sec), 487.3 ms avg latency, 1019.0 ms max latency.<br>......<br>49990 records sent, 9998.0 records/sec (9.76 MB/sec), 4.3 ms avg latency, 33.0 ms max latency.<br>50030 records sent, 10006.0 records/sec (9.77 MB/sec), 4.7 ms avg latency, 62.0 ms max latency.<br>46810 records sent, 9073.5 records/sec (8.86 MB/sec), 4.6 ms avg latency, 406.0 ms max latency.<br>1000000 records sent, 9905.795881 records/sec (9.67 MB/sec), 149.33 ms avg latency, 1580.00 ms max latency, 4 ms 50th, 1050 ms 95th, 1390 ms 99th, 1561 ms 99.9th.<br></code></pre></td></tr></table></figure>

<p>我们可以从结果上看出，条件为：“batch.size=16384 linger.ms=0”的生产者速度为：9.67 MB/sec</p>
<h4 id="11-1-2-2-第二次压力测试"><a href="#11-1-2-2-第二次压力测试" class="headerlink" title="11.1.2.2 第二次压力测试"></a>11.1.2.2 第二次压力测试</h4><p>条件：batch.size=32768 linger.ms=0</p>
<p>测试的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# /apps/kafka/bin/kafka-producer-perf-test.sh --topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=kafka-node1:9092,kafka-node2:9092,kafka-node3:9092 batch.size=32768 linger.ms=0<br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell">49852 records sent, 9966.4 records/sec (9.73 MB/sec), 54.5 ms avg latency, 554.0 ms max latency.<br>50140 records sent, 10026.0 records/sec (9.79 MB/sec), 8.0 ms avg latency, 87.0 ms max latency.<br>50030 records sent, 10006.0 records/sec (9.77 MB/sec), 3.9 ms avg latency, 34.0 ms max latency.<br>......<br>49999 records sent, 9997.8 records/sec (9.76 MB/sec), 13.3 ms avg latency, 240.0 ms max latency.<br>49980 records sent, 9996.0 records/sec (9.76 MB/sec), 3.2 ms avg latency, 78.0 ms max latency.<br>1000000 records sent, 9998.100361 records/sec (9.76 MB/sec), 6.83 ms avg latency, 554.00 ms max latency, 2 ms 50th, 13 ms 95th, 160 ms 99th, 324 ms 99.9th.<br></code></pre></td></tr></table></figure>

<p>我们可以从结果上看出，条件为：“batch.size=32768 linger.ms=0”的生产者速度为：9.76 MB/sec</p>
<h4 id="11-1-2-3-第三次压力测试"><a href="#11-1-2-3-第三次压力测试" class="headerlink" title="11.1.2.3 第三次压力测试"></a>11.1.2.3 第三次压力测试</h4><p>条件：batch.size=4096 linger.ms=0</p>
<p>测试的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# /apps/kafka/bin/kafka-producer-perf-test.sh --topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=kafka-node1:9092,kafka-node2:9092,kafka-node3:9092 batch.size=4096 linger.ms=0<br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs shell">15394 records sent, 3078.8 records/sec (3.01 MB/sec), 2006.1 ms avg latency, 3464.0 ms max latency.<br>19179 records sent, 3835.8 records/sec (3.75 MB/sec), 5117.3 ms avg latency, 6545.0 ms max latency.<br>21273 records sent, 4254.6 records/sec (4.15 MB/sec), 6018.4 ms avg latency, 6712.0 ms max latency.<br>......<br>24240 records sent, 4848.0 records/sec (4.73 MB/sec), 5029.1 ms avg latency, 9244.0 ms max latency.<br>22746 records sent, 4549.2 records/sec (4.44 MB/sec), 5177.2 ms avg latency, 9678.0 ms max latency.<br>22638 records sent, 4527.6 records/sec (4.42 MB/sec), 5260.2 ms avg latency, 10274.0 ms max latency.<br>17244 records sent, 3448.1 records/sec (3.37 MB/sec), 6374.3 ms avg latency, 10401.0 ms max latency.<br>6034 records sent, 1205.8 records/sec (1.18 MB/sec), 10247.1 ms avg latency, 11804.0 ms max latency.<br>1000000 records sent, 4374.568011 records/sec (4.27 MB/sec), 5366.30 ms avg latency, 13126.00 ms max latency, 5424 ms 50th, 9555 ms 95th, 10256 ms 99th, 12771 ms 99.9th.<br></code></pre></td></tr></table></figure>

<p>我们可以从结果上看出，条件为：“batch.size=4096 linger.ms=0”的生产者速度为：4.27 MB/sec</p>
<h4 id="11-1-2-4-第四次压力测试"><a href="#11-1-2-4-第四次压力测试" class="headerlink" title="11.1.2.4 第四次压力测试"></a>11.1.2.4 第四次压力测试</h4><p>条件：batch.size=4096 linger.ms=50</p>
<p>测试的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# /apps/kafka/bin/kafka-producer-perf-test.sh --topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=kafka-node1:9092,kafka-node2:9092,kafka-node3:9092 batch.size=4096 linger.ms=50<br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">16009 records sent, 3201.8 records/sec (3.13 MB/sec), 2042.6 ms avg latency, 3463.0 ms max latency.<br>20049 records sent, 4009.8 records/sec (3.92 MB/sec), 4978.9 ms avg latency, 6550.0 ms max latency.<br>21852 records sent, 4340.9 records/sec (4.24 MB/sec), 5729.3 ms avg latency, 6580.0 ms max latency.<br>......<br>19641 records sent, 3928.2 records/sec (3.84 MB/sec), 5734.0 ms avg latency, 10605.0 ms max latency.<br>20205 records sent, 4041.0 records/sec (3.95 MB/sec), 6100.0 ms avg latency, 10755.0 ms max latency.<br>8586 records sent, 1716.5 records/sec (1.68 MB/sec), 8879.2 ms avg latency, 11642.0 ms max latency.<br>1000000 records sent, 4172.334087 records/sec (4.07 MB/sec), 5651.01 ms avg latency, 12407.00 ms max latency, 6086 ms 50th, 9892 ms 95th, 11060 ms 99th, 12140 ms 99.9th.<br></code></pre></td></tr></table></figure>

<p>我们可以从结果上看出，条件为：“batch.size=4096 linger.ms=50”的生产者速度为：4.07 MB/sec</p>
<h4 id="11-1-2-5-第五次压力测试"><a href="#11-1-2-5-第五次压力测试" class="headerlink" title="11.1.2.5 第五次压力测试"></a>11.1.2.5 第五次压力测试</h4><p>条件：batch.size=4096 linger.ms=50 compression.type=snappy</p>
<p>测试的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# /apps/kafka/bin/kafka-producer-perf-test.sh --topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=kafka-node1:9092,kafka-node2:9092,kafka-node3:9092 batch.size=4096 linger.ms=50 compression.type=snappy<br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">19425 records sent, 3885.0 records/sec (3.79 MB/sec), 6844.2 ms avg latency, 7642.0 ms max latency.<br>20022 records sent, 4004.4 records/sec (3.91 MB/sec), 6206.4 ms avg latency, 6873.0 ms max latency.<br>22254 records sent, 4450.8 records/sec (4.35 MB/sec), 5813.4 ms avg latency, 6633.0 ms max latency.<br>......<br>21711 records sent, 4342.2 records/sec (4.24 MB/sec), 5010.4 ms avg latency, 8307.0 ms max latency.<br>21646 records sent, 4327.5 records/sec (4.23 MB/sec), 5628.6 ms avg latency, 8736.0 ms max latency.<br>11481 records sent, 2295.7 records/sec (2.24 MB/sec), 8325.3 ms avg latency, 9531.0 ms max latency.<br>1000000 records sent, 4090.782649 records/sec (3.99 MB/sec), 5811.20 ms avg latency, 11024.00 ms max latency, 6771 ms 50th, 8491 ms 95th, 8905 ms 99th, 10716 ms 99.9th.<br></code></pre></td></tr></table></figure>

<p>我们可以从结果上看出，条件为：“batch.size=4096 linger.ms=50 compression.type=snappy”的生产者速度为：3.99 MB/sec</p>
<h4 id="11-1-2-6-第六次压力测试"><a href="#11-1-2-6-第六次压力测试" class="headerlink" title="11.1.2.6 第六次压力测试"></a>11.1.2.6 第六次压力测试</h4><p>条件：batch.size=4096 linger.ms=50 compression.type=zstd</p>
<p>测试的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# /apps/kafka/bin/kafka-producer-perf-test.sh --topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=kafka-node1:9092,kafka-node2:9092,kafka-node3:9092 batch.size=4096 linger.ms=50 compression.type=zstd<br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">14792 records sent, 2951.3 records/sec (2.88 MB/sec), 2375.6 ms avg latency, 3568.0 ms max latency.<br>29865 records sent, 5965.8 records/sec (5.83 MB/sec), 4573.1 ms avg latency, 5751.0 ms max latency.<br>32255 records sent, 6449.7 records/sec (6.30 MB/sec), 6307.7 ms avg latency, 6888.0 ms max latency.<br>......<br>33910 records sent, 6782.0 records/sec (6.62 MB/sec), 5605.2 ms avg latency, 8267.0 ms max latency.<br>38185 records sent, 7618.7 records/sec (7.44 MB/sec), 5911.3 ms avg latency, 8561.0 ms max latency.<br>38930 records sent, 7786.0 records/sec (7.60 MB/sec), 5238.8 ms avg latency, 7895.0 ms max latency.<br>1000000 records sent, 6665.511311 records/sec (6.51 MB/sec), 5800.06 ms avg latency, 8662.00 ms max latency, 6017 ms 50th, 7794 ms 95th, 8286 ms 99th, 8539 ms 99.9th.<br></code></pre></td></tr></table></figure>

<p>我们可以从结果上看出，条件为：“batch.size=4096 linger.ms=50 compression.type=zstd”的生产者速度为：</p>
<h4 id="11-1-2-7-第七次压力测试"><a href="#11-1-2-7-第七次压力测试" class="headerlink" title="11.1.2.7 第七次压力测试"></a>11.1.2.7 第七次压力测试</h4><p>条件：batch.size=4096 linger.ms=50 compression.type=gzip</p>
<p>测试的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# /apps/kafka/bin/kafka-producer-perf-test.sh --topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=kafka-node1:9092,kafka-node2:9092,kafka-node3:9092 batch.size=4096 linger.ms=50 compression.type=gzip<br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">21057 records sent, 4211.4 records/sec (4.11 MB/sec), 2132.9 ms avg latency, 3009.0 ms max latency.<br>29470 records sent, 5885.8 records/sec (5.75 MB/sec), 3928.8 ms avg latency, 5099.0 ms max latency.<br>32810 records sent, 6562.0 records/sec (6.41 MB/sec), 5744.0 ms avg latency, 6762.0 ms max latency.<br>......<br>38865 records sent, 7771.4 records/sec (7.59 MB/sec), 5232.8 ms avg latency, 8915.0 ms max latency.<br>34230 records sent, 6846.0 records/sec (6.69 MB/sec), 5516.3 ms avg latency, 9635.0 ms max latency.<br>17984 records sent, 3596.1 records/sec (3.51 MB/sec), 7671.3 ms avg latency, 10823.0 ms max latency.<br>1000000 records sent, 6909.942717 records/sec (6.75 MB/sec), 5476.79 ms avg latency, 11644.00 ms max latency, 5038 ms 50th, 9488 ms 95th, 10543 ms 99th, 11564 ms 99.9th.<br></code></pre></td></tr></table></figure>

<p>我们可以从结果上看出，条件为：“batch.size=4096 linger.ms=50 compression.type=gzip”的生产者速度为：</p>
<h4 id="11-1-2-8-第八次压力测试"><a href="#11-1-2-8-第八次压力测试" class="headerlink" title="11.1.2.8 第八次压力测试"></a>11.1.2.8 第八次压力测试</h4><p>条件：batch.size=4096 linger.ms=50 compression.type=lz4</p>
<p>测试的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# /apps/kafka/bin/kafka-producer-perf-test.sh --topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=kafka-node1:9092,kafka-node2:9092,kafka-node3:9092 batch.size=4096 linger.ms=50 compression.type=lz4<br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">12604 records sent, 2520.8 records/sec (2.46 MB/sec), 2295.1 ms avg latency, 3767.0 ms max latency.<br>17049 records sent, 3407.8 records/sec (3.33 MB/sec), 5345.9 ms avg latency, 7095.0 ms max latency.<br>20250 records sent, 4049.2 records/sec (3.95 MB/sec), 6772.6 ms avg latency, 7229.0 ms max latency.<br>......<br>24762 records sent, 4952.4 records/sec (4.84 MB/sec), 5002.2 ms avg latency, 9761.0 ms max latency.<br>14778 records sent, 2955.6 records/sec (2.89 MB/sec), 6546.8 ms avg latency, 10047.0 ms max latency.<br>5535 records sent, 1106.8 records/sec (1.08 MB/sec), 10681.6 ms avg latency, 11903.0 ms max latency.<br>1000000 records sent, 4125.463599 records/sec (4.03 MB/sec), 5716.52 ms avg latency, 12766.00 ms max latency, 5102 ms 50th, 9566 ms 95th, 9870 ms 99th, 12362 ms 99.9th.<br></code></pre></td></tr></table></figure>

<p>我们可以从结果上看出，条件为：“batch.size=4096 linger.ms=50 compression.type=lz4”的生产者速度为：4.03 MB/sec</p>
<h4 id="11-1-2-9-第九次压力测试"><a href="#11-1-2-9-第九次压力测试" class="headerlink" title="11.1.2.9 第九次压力测试"></a>11.1.2.9 第九次压力测试</h4><p>条件：batch.size=4096 linger.ms=50 buffer.memory=67108864</p>
<p>测试的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# /apps/kafka/bin/kafka-producer-perf-test.sh --topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=kafka-node1:9092,kafka-node2:9092,kafka-node3:9092 batch.size=4096 linger.ms=50 buffer.memory=67108864<br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">14443 records sent, 2884.6 records/sec (2.82 MB/sec), 2055.8 ms avg latency, 3606.0 ms max latency.<br>18717 records sent, 3739.7 records/sec (3.65 MB/sec), 5139.0 ms avg latency, 6857.0 ms max latency.<br>20931 records sent, 4185.4 records/sec (4.09 MB/sec), 8270.7 ms avg latency, 9751.0 ms max latency.<br>......<br>18391 records sent, 3676.7 records/sec (3.59 MB/sec), 11856.3 ms avg latency, 16460.0 ms max latency.<br>13095 records sent, 2619.0 records/sec (2.56 MB/sec), 14563.7 ms avg latency, 16987.0 ms max latency.<br>6645 records sent, 1328.5 records/sec (1.30 MB/sec), 16841.8 ms avg latency, 18410.0 ms max latency.<br>1000000 records sent, 4134.264370 records/sec (4.04 MB/sec), 11131.95 ms avg latency, 18802.00 ms max latency, 11878 ms 50th, 15699 ms 95th, 16697 ms 99th, 18567 ms 99.9th.<br></code></pre></td></tr></table></figure>

<p>我们可以从结果上看出，条件为：“batch.size=4096 linger.ms=50 buffer.memory=67108864”的生产者速度为：4.04 MB/sec</p>
<h3 id="11-1-3-消费者进行压力测试"><a href="#11-1-3-消费者进行压力测试" class="headerlink" title="11.1.3 消费者进行压力测试"></a>11.1.3 消费者进行压力测试</h3><h4 id="11-1-3-1-第一次压力测试"><a href="#11-1-3-1-第一次压力测试" class="headerlink" title="11.1.3.1 第一次压力测试"></a>11.1.3.1 第一次压力测试</h4><p>条件：消费者一次处理500条</p>
<p>修改消费者配置文件，主要添加内容为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# vim /apps/kafka/config/consumer.properties<br>......<br>max.poll.records=500<br></code></pre></td></tr></table></figure>

<p>测试的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# /apps/kafka/bin/kafka-consumer-perf-test.sh --bootstrap-server kafka-node1:9092,kafka-node2:9092,kafka-node3:9092 --topic test --messages 1000000 --consumer.config /apps/kafka/config/consumer.properties<br></code></pre></td></tr></table></figure>

<p>参数说明：</p>
<p><code>--bootstrap-server</code>：指定 Kafka 集群地址</p>
<p><code>--topic</code>：指定 topic 的名称</p>
<p><code>--messages</code>：总共要消费的消息个数。本次实验 100 万条。</p>
<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec<br>2023-02-25 17:11:39:099, 2023-02-25 17:11:57:775, 976.6240, 52.2930, 1000063, 53548.0296, 5618, 13058, 74.7912, 76586.2307<br></code></pre></td></tr></table></figure>

<p>我们可以从结果上看出，条件为：“消费者一次处理500条”的消费者速度为：52.2930 m/s</p>
<h4 id="11-1-3-2-第二次压力测试"><a href="#11-1-3-2-第二次压力测试" class="headerlink" title="11.1.3.2 第二次压力测试"></a>11.1.3.2 第二次压力测试</h4><p>条件：消费者一次处理2000条</p>
<p>修改消费者配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# vim /apps/kafka/config/consumer.properties<br>......<br>max.poll.records=2000<br></code></pre></td></tr></table></figure>

<p>测试的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# /apps/kafka/bin/kafka-consumer-perf-test.sh --bootstrap-server kafka-node1:9092,kafka-node2:9092,kafka-node3:9092 --topic test --messages 1000000 --consumer.config /apps/kafka/config/consumer.properties<br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec<br>2023-02-25 17:14:44:227, 2023-02-25 17:14:53:687, 977.3184, 103.3106, 1000774, 105790.0634, 4016, 5444, 179.5221, 183830.6392<br></code></pre></td></tr></table></figure>

<p>我们可以从结果上看出，条件为：“消费者一次处理2000条”的消费者速度为：103.3106 m/s</p>
<h4 id="11-1-3-3-第三次压力测试"><a href="#11-1-3-3-第三次压力测试" class="headerlink" title="11.1.3.3 第三次压力测试"></a>11.1.3.3 第三次压力测试</h4><p>条件：消费者一次处理2000条 fetch.max.bytes=104857600</p>
<p>修改消费者配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# vim /apps/kafka/config/consumer.properties<br>......<br>max.poll.records=2000<br>fetch.max.bytes=104857600<br></code></pre></td></tr></table></figure>

<p>测试的命令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@kafka-test ~]# /apps/kafka/bin/kafka-consumer-perf-test.sh --bootstrap-server kafka-node1:9092,kafka-node2:9092,kafka-node3:9092 --topic test --messages 1000000 --consumer.config /apps/kafka/config/consumer.properties<br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">start.time, end.time, data.consumed.in.MB, MB.sec, data.consumed.in.nMsg, nMsg.sec, rebalance.time.ms, fetch.time.ms, fetch.MB.sec, fetch.nMsg.sec<br>2023-02-25 17:15:58:878, 2023-02-25 17:16:07:415, 977.3691, 114.4863, 1000826, 117233.9229, 3666, 4871, 200.6506, 205466.2287<br></code></pre></td></tr></table></figure>

<p>我们可以从结果上看出，条件为：“消费者一次处理2000条 fetch.max.bytes=104857600”的消费者速度为：114.4863 m/s</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/blog/tags/linux-kafka/">linux kafka</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！</p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/blog/2023/03/07/java%E5%BE%AE%E6%9C%8D%E5%8A%A1/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">java微服务</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/blog/2023/02/18/MySQL%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/">
                        <span class="hidden-mobile">MySQL之数据库备份与恢复</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>

<!-- SCRIPTS -->

  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js" ></script>
<script  src="/blog/js/debouncer.js" ></script>
<script  src="/blog/js/events.js" ></script>
<script  src="/blog/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/blog/js/lazyload.js" ></script>
  



  



  <script  src="https://cdn.jsdelivr.net/npm/tocbot@4.12.0/dist/tocbot.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js" ></script>






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>



  <script  src="/blog/js/local-search.js" ></script>
  <script>
    (function () {
      var path = "/blog/local-search.xml";
      var inputArea = document.querySelector("#local-search-input");
      inputArea.onclick = function () {
        searchFunc(path, 'local-search-input', 'local-search-result');
        this.onclick = null
      }
    })()
  </script>















<!-- 主题的启动项 保持在最底部 -->
<script  src="/blog/js/boot.js" ></script>



</body>
</html>
